{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\" Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, please just say that you don't know the answer, don't try to make up\n",
    "an answer. \n",
    "\n",
    "Context : {context}\n",
    "Question : {question}\n",
    "\n",
    "The answer should consist of at least 1 sentence for short questions or 7 sentences for more detailed qeustions. Only returns the helpful and reasonable answer below and nothing else.\n",
    "No need to return the question. I just want answer. Please don't show unhelpful answers.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt(custom_prompt_template):\n",
    "    prompt = PromptTemplate(template=custom_prompt_template, input_variables=['context',\n",
    "                                                                              'question'])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    n_gpu_layers = 32  # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/home/sira/sira_project/meta-Llama2/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,n_ctx = 4096, temperature = 0.1, max_tokens = 4096\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def load_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = \"thenlper/gte-base\",\n",
    "                                       model_kwargs = {'device': 'cpu'})\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(source_list):\n",
    "    res = []\n",
    "    for i in source_list:\n",
    "        if i not in res:\n",
    "            res.append(i)\n",
    "    return res\n",
    "\n",
    "def convert_to_website_format(urls):\n",
    "    convert_urls = []\n",
    "    for url in urls:\n",
    "        # Remove any '.html' at the end of the URL\n",
    "        url = re.sub(r'\\.html$', '', url)\n",
    "        # Check if the URL starts with 'www.' or 'http://'\n",
    "        if not re.match(r'(www\\.|http://)', url):\n",
    "            url = 'https://' + url\n",
    "        if '/index' in url:\n",
    "            url = url.split('/index')[0]\n",
    "        match = re.match(r'^([^ ]+)', url)\n",
    "        if match:\n",
    "            url = match.group(1)\n",
    "        convert_urls.append(url)\n",
    "    return convert_urls\n",
    "\n",
    "def regex_source(answer):\n",
    "    pattern = r\"'source': '(.*?)'\"\n",
    "    matchs = re.findall(pattern, str(answer))\n",
    "    convert_urls = convert_to_website_format(matchs)\n",
    "    res_urls = check_duplicate(source_list=convert_urls)\n",
    "    #res_urls = filter_similar_url(res_urls)\n",
    "    return res_urls\n",
    "\n",
    "def filter_similar_url(urls):\n",
    "    urls_remove = [\"www.omniscien.com/aboutus/company\",\"www.omniscien.com/lsev6/asr/automatic-speech-recognition-overview\", \"www.omniscien.com/lsev6/features/asr/autonomous-speech-recognition-overview\",\"www.omniscien.com/lsev6/asr\"]\n",
    "    # Remove the URL from the list\n",
    "    filtered_urls = [url for url in urls if url not in  urls_remove]\n",
    "    return filtered_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_search(db_similarity, diff_val):\n",
    "    filter_list = []\n",
    "    top_score = db_similarity[0][1]\n",
    "    for index, score in enumerate(db_similarity) :\n",
    "        if score[1] - top_score <= diff_val:\n",
    "              filter_list.append(score)\n",
    "    return filter_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. At the end of standalone question add this 'Answer the question in German language.' If you do not know the answer reply with 'I am sorry'.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sira/anaconda3/envs/lang-lama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama.cpp: loading model from /home/sira/sira_project/meta-Llama2/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 7354.73 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "DB_FAISS_PATH = \"/home/sira/sira_project/meta-Llama2/vectorstores_clean_doc_gte-base/db_faiss\"\n",
    "embeddings = load_embeddings()\n",
    "db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
    "llm = load_llm()\n",
    "qa_prompt = set_custom_prompt(custom_prompt_template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"query\", output_key=\"result\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = db.as_retriever(search_kwargs = {'k':3}), \n",
    "    return_source_documents = True,\n",
    "    memory = memory,\n",
    "    chain_type_kwargs = {\"prompt\":qa_prompt}) \n",
    "\n",
    "\n",
    "# diff_val = st.slider(label ='Select a diff value',\n",
    "#                    min_value = 0.00, \n",
    "#                    max_value = 1.00, \n",
    "#                    step = 0.01, value = 0.01, format = \"%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As Chief Technology Officer and Co-Founder of Omniscien, he provides guidance on technology solutions that drive business success.Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As Chief Technology Officer and Co-Founder of Omniscien, he provides guidance on technology solutions that drive business success.\n",
      "1: https://omniscien.com/about-us/company\n",
      "2: https://omniscien.com/blog/hype-cycle-for-ai-technologies-in-business\n",
      "Respone time: 47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 135304.14 ms\n",
      "llama_print_timings:      sample time =    50.43 ms /    86 runs   (    0.59 ms per token,  1705.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19456.52 ms /    68 tokens (  286.13 ms per token,     3.49 tokens per second)\n",
      "llama_print_timings:        eval time = 27288.26 ms /    85 runs   (  321.04 ms per token,     3.11 tokens per second)\n",
      "llama_print_timings:       total time = 47046.17 ms\n"
     ]
    }
   ],
   "source": [
    "history_log = []\n",
    "query = \"Who is Dion Wiggins\"\n",
    "start = time.time()\n",
    "#db_similarity = db.similarity_search_with_score(query, k=10)\n",
    "#filter_list = filter_search(db_similarity, diff_val)\n",
    "response = qa_chain({'query': query})\n",
    "print(response[\"result\"])\n",
    "urls = regex_source(response)\n",
    "for count, url in enumerate(urls):\n",
    "    print(str(count+1)+\":\", url)\n",
    "end = time.time()\n",
    "print(\"Respone time:\",int(end-start),\"sec\")\n",
    "history_log.append(memory.load_memory_variables({})[\"chat_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who is Dion Wiggins', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As the Chief Technology Officer and Co-Founder of Omniscien, he has advised literally hundreds of enterprises on their ICT strategy.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As the Chief Technology Officer and Co-Founder of Omniscien, he has advised literally hundreds of enterprises on their ICT strategy.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As the Chief Technology Officer and Co-Founder of Omniscien, he has advised literally hundreds of enterprises on their ICT strategy.Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As the Chief Technology Officer and Co-Founder of Omniscien, he has advised literally hundreds of enterprises on their ICT strategy.\n",
      "1: https://omniscien.com/about-us/company\n",
      "2: https://omniscien.com/blog/hype-cycle-for-ai-technologies-in-business\n",
      "Respone time: 50 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 135304.14 ms\n",
      "llama_print_timings:      sample time =    49.36 ms /    92 runs   (    0.54 ms per token,  1863.86 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19706.92 ms /    71 tokens (  277.56 ms per token,     3.60 tokens per second)\n",
      "llama_print_timings:        eval time = 30011.02 ms /    91 runs   (  329.79 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:       total time = 50073.92 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Dion Wiggins 12\"\n",
    "start = time.time()\n",
    "#db_similarity = db.similarity_search_with_score(query, k=10)\n",
    "#filter_list = filter_search(db_similarity, diff_val)\n",
    "response = qa_chain({'query': query})\n",
    "print(response[\"result\"])\n",
    "urls = regex_source(response)\n",
    "for count, url in enumerate(urls):\n",
    "    print(str(count+1)+\":\", url)\n",
    "end = time.time()\n",
    "print(\"Respone time:\",int(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m memory\u001b[39m.\u001b[39;49mload_memory_variables({})[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(memory.load_memory_variables([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who is Dion Wiggins', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As the Chief Technology Officer and Co-Founder of Omniscien, he has advised literally hundreds of enterprises on their ICT strategy.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Who is Dion Wiggins 12', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As the Chief Technology Officer and Co-Founder of Omniscien, he has advised literally hundreds of enterprises on their ICT strategy.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[HumanMessage(content='Who is Dion Wiggins', additional_kwargs={}, example=False), AIMessage(content='Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As the Chief Technology Officer and Co-Founder of Omniscien, he has advised literally hundreds of enterprises on their ICT strategy.', additional_kwargs={}, example=False), HumanMessage(content='Who is Dion Wiggins 12', additional_kwargs={}, example=False), AIMessage(content='Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. As the Chief Technology Officer and Co-Founder of Omniscien, he has advised literally hundreds of enterprises on their ICT strategy.', additional_kwargs={}, example=False)]\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(memory.load_memory_variables({})[\"chat_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key='result', input_key='query', return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='chat_history')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-lama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
