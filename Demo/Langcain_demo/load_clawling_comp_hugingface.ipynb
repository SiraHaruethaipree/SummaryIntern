{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1 Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/PradipNichite/Youtube-Tutorials/blob/main/Langchain_Semnatic_Serach_Pinecone.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "#for Huggingface\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "#for openai\n",
    "from langchain.llms import OpenAI\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n"
     ]
    }
   ],
   "source": [
    "def load_docs(docs_path):\n",
    "    loader = DirectoryLoader(docs_path, glob=\"**/*.html\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    sp_docs = text_splitter.split_documents(documents)\n",
    "    return sp_docs\n",
    "\n",
    "documents = load_docs('omniscien.com')\n",
    "sp_docs = split_docs(documents)\n",
    "print(len(sp_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract time = 1.41\n",
    "create vector = 2.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# save model \n",
    "# db1 = Chroma.from_documents(sp_docs, embedding_function, persist_directory=\"./chroma_db_hugingface\")\n",
    "# db1.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model \n",
    "db = Chroma(persist_directory=\"./chroma_db_hugingface\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similiar_docs(query,k=2,score=False):\n",
    "  if score:\n",
    "    similar_docs = db.similarity_search_with_score(query,k=k)\n",
    "  else:\n",
    "    similar_docs = db.similarity_search(query,k=k)\n",
    "  return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"declare-lab/flan-alpaca-large\"\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm,  chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': 'SOURCES: - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia - Wikipedia -'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is NMT\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I am wondering who will be absorbing the cost of implementing NMT? Will it be the end client, the LSPs or translators?\\n\\nPutting this question into perspective, an SMT engine that takes about 1 day to train on a multiple CPU system with about 256GB RAM. Training a NMT system such as this with CPU technology is not viable as it would take many months. Training using GPU technology still takes about 3 days or more for a large enigne. As such, the costs of training NMT engines are notably higher.', metadata={'source': 'omniscien.com/faq/conference-and-webinar-nmt-questions-and-answers/index.html'})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': 'Dion is a founder of The ActiveX Factory, a former holder of a US O1 Extraordinary Ability Visa, and a professor at Omniscien. He is a pioneer in the machine translation space, with books on Statistical Machine Translation and Neural Machine Translation. He is also a professor at the University of California, Berkeley.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Dion\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dion is a founder of The ActiveX Factory, a former holder of a US O1 Extraordinary Ability Visa, and a professor at Omniscien. He is a pioneer in the machine translation space, with books on Statistical Machine Translation and Neural Machine Translation. He is also a professor at the University of California, Berkeley.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dion is a pioneer of the Asian Internet Industry, founder of one of Asia's first ISPs (Asia Online in Hong Kong), and was Vice President and Research Director for Gartner. He was the recipient of the Chairman's Commendation Award presented by Microsoft's Bill Gates for the best showcase of software developed in the Philippines, is recognized by the US Government as being in the top 5% of his field worldwide, and is a former holder of a US O1 Extraordinary Ability Visa.\n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r'\\n.*', '', y[\"output_text\"])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"Available as two Platform Editions specifically designed to match different business needs.\\n\\nProduct Overview\\n\\nFeatures\\n\\nBenefits of Media Studio (White Paper)\\n\\nSubtitle Optimized Machine Translation\\n\\nData Security & Privacy\\n\\nSecure by Design\\n\\nProject Management and Editing PlatformProject, People, Resource, Video, and Subtitle Management\\n\\nDetailed Features\\n\\nData Processing PlatformData Creation, Analysis, Cleaning, and Organization\\n\\nDetailed Features\\n\\nRequest a Demo\\n\\nFeature   Overview\\n\\nEach feature is built on a core of Artificial Intelligence, Machine Learning and Natural Language Processing\\n\\nMachine learning enables machines to work more like humans so that humans don't have to work more like machines. Each feature is designed to augment human intelligence, enhance productivity, increase quality, and reduce cost. Artificial intelligence enables processing and organization of data that simply not be cost-effective or feasible with a human only approach.\", metadata={'source': 'omnisciencom/products/media-studio/project-management-and-editing-platform/index.html'})],\n",
       " 'question': 'Who is Dion wiggins',\n",
       " 'output_text': ' Dion Wiggins is not mentioned in the source.\\nSOURCES: omniscien.com/products/media-studio/project-management-and-editing-platform/index.html'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/chains/popular/vector_db_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docsearch = Chroma.from_documents(sp_docs, embedding_function)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_hug, chain_type=\"stuff\", retriever=db.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Dion\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. He is an accomplished speaker and has a high media profile for his perceptive analysis of ICT in Asia/Pacific.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Dion was a founder of The ActiveX Factory, where he was the recipient of the Chairman’s Commendation Award presented by Microsoft’s Bill Gates for the best showcase of software developed in the Philippines. The US Government has recognized Dion as being in the top 5% of his field worldwide and he is a former holder of a US O1 Extraordinary Ability Visa.\\n\\nPhilipp Koehn\\n\\nChief Scientist\\n\\nBehind many of the tools design is Omniscien’s Chief Scientist, Professor Philipp Koehn who leads our team of researchers and developers. Philipp is a pioneer in the machine translation space, his books on Statistical Machine Translation and Neural Machine Translation are the leading academic textbooks globally on machine translation. Both books are available now from Amazon.com or leading book stores.', metadata={'source': 'omniscien.com/about-us/company/index.html'}),\n",
       " Document(page_content='Dion was a founder of The ActiveX Factory, where he was the recipient of the Chairman’s Commendation Award presented by Microsoft’s Bill Gates for the best showcase of software developed in the Philippines. The US Government has recognized Dion as being in the top 5% of his field worldwide and he is a former holder of a US O1 Extraordinary Ability Visa.\\n\\nPhilipp Koehn\\n\\nChief Scientist\\n\\nBehind many of the tools design is Omniscien’s Chief Scientist, Professor Philipp Koehn who leads our team of researchers and developers. Philipp is a pioneer in the machine translation space, his books on Statistical Machine Translation and Neural Machine Translation are the leading academic textbooks globally on machine translation. Both books are available now from Amazon.com or leading book stores.', metadata={'source': 'omniscien.com/aboutus/company/index.html'}),\n",
       " Document(page_content='Dion Wiggins\\n\\nChief Technology Officer, Co-Founder\\n\\nDion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. He is an accomplished speaker and has a high media profile for his perceptive analysis of ICT in Asia/Pacific.\\n\\nPreviously Dion was Vice President and Research Director for Gartner based in Hong Kong, where he was the most senior and highly-respected analyst based in all of Asia. Dion’s research reports on ICT in China helped change the way the world views this market.\\n\\nDion is also a well-known pioneer of the Asian Internet Industry, being the founder of one of Asia’s first ever ISPs (Asia Online in Hong Kong). In his role at Gartner and in various other consulting positions prior to that, Dion advised literally hundreds of enterprises on their ICT strategy.', metadata={'source': 'omniscien.com/about-us/company/index.html'}),\n",
       " Document(page_content='Dion Wiggins\\n\\nChief Technology Officer, Co-Founder\\n\\nDion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He has an impressive knowledge in the fields of software development, architecture, and management, as well as an in-depth understanding of Asian ICT markets. He is an accomplished speaker and has a high media profile for his perceptive analysis of ICT in Asia/Pacific.\\n\\nPreviously Dion was Vice President and Research Director for Gartner based in Hong Kong, where he was the most senior and highly-respected analyst based in all of Asia. Dion’s research reports on ICT in China helped change the way the world views this market.\\n\\nDion is also a well-known pioneer of the Asian Internet Industry, being the founder of one of Asia’s first ever ISPs (Asia Online in Hong Kong). In his role at Gartner and in various other consulting positions prior to that, Dion advised literally hundreds of enterprises on their ICT strategy.', metadata={'source': 'omniscien.com/aboutus/company/index.html'})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/chains/additional/question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docsearch = Chroma.from_documents(sp_docs, embedding_function)\n",
    "query = \"What is NMT\"\n",
    "docs_qa = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm_hug, chain_type=\"stuff\")\n",
    "query = \"What is NMT\"\n",
    "docs_qa = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='I am wondering who will be absorbing the cost of implementing NMT? Will it be the end client, the LSPs or translators?\\n\\nPutting this question into perspective, an SMT engine that takes about 1 day to train on a multiple CPU system with about 256GB RAM. Training a NMT system such as this with CPU technology is not viable as it would take many months. Training using GPU technology still takes about 3 days or more for a large enigne. As such, the costs of training NMT engines are notably higher.', metadata={'source': 'omniscien.com_fix/faq/conference-and-webinar-nmt-questions-and-answers/Neural Machine Translation Questions and Answers - Omniscien Technologies.html'}),\n",
       "  Document(page_content='Search\\n\\nOmniscien » FAQ » What is Neural Machine Translation (NMT)?\\n\\nWhat is Neural Machine Translation (NMT)?\\n\\nNeural Machine Translation (also known\\xa0as Neural MT, NMT, Deep Neural Machine Translation, Deep NMT, or DNMT) is a state-of-the-art machine translation approach that utilizes neural network techniques to predict the likelihood of a set of words in sequence. This can be a text fragment, complete sentence, or with the latest advances an entire document.\\n\\nNMT is a\\xa0radically different approach to solving the problem of language translation and localization that uses deep neural networks and artificial intelligence to train neural models.\\xa0NMT has quickly become the dominant approach to machine translation with a major transition from SMT to NMT in just 3 years. Neural Machine Translation typically produces much higher quality translations that Statistical Machine Translation approaches, with better fluency and adequacy.', metadata={'source': 'omniscien.com_fix/faq/what-is-neural-machine-translation/index.html'}),\n",
       "  Document(page_content='Research has shown that NMT does indeed require higher quality translations as training data when compared to SMT. SMT is helped in part by having a language model in the target language that works as a kind of filter to some of the lower probability choices.\\n\\nOne of the things that’s made SMT commercially feasible is adaptivity: to adjust a system’s behaviour based on small bits of human input, without retraining on the entire data set (too costly). As I understand that also needed technology innovation. Do current NMT models have this capability, or is adaptive NMT still some ways off / or not feasible with current technology at all?\\n\\nThe learning algorithm that is used to train all these parameters is called the online learning algorithm meaning that it learns by continuously processing new training examples. That means in practice you give it new sentence pairs and it learns from them.', metadata={'source': 'omniscien.com_fix/faq/conference-and-webinar-nmt-questions-and-answers/Neural Machine Translation Questions and Answers - Omniscien Technologies.html'}),\n",
       "  Document(page_content='Services\\n\\nSolutions\\n\\nTechnology\\n\\nResourcesAbout UsCompanyCareersInternshipsNewsOffice LocationsLegalContact UsFrequently Asked Questions (FAQ)SupportOmniscien BlogWebinarsEvents and ConferencesAI, MT and Language Processing SymposiumCase StudiesTestimonialsIntegrated Solution PartnersTechnology PartnersLanguage Pairs – Machine TranslationSupported Document FormatsWays to TranslateData Security and Privacy\\n\\nSearch\\n\\nOmniscien » FAQ » What is Neural Machine Translation (NMT)?\\n\\nWhat is Neural Machine Translation (NMT)?\\n\\nNeural Machine Translation (also known\\xa0as Neural MT, NMT, Deep Neural Machine Translation, Deep NMT, or DNMT) is a state-of-the-art machine translation approach that utilizes neural network techniques to predict the likelihood of a set of words in sequence. This can be a text fragment, complete sentence, or with the latest advances an entire document.', metadata={'source': 'omniscien.com_fix/faq/what-is-neural-machine-translation/index.html'})],\n",
       " 'question': 'What is NMT',\n",
       " 'output_text': 'SOURCES: - Omniscien Technologies - FAQ - Neural Machine Translation - Deep Neural Machine Translation - DNMT - Statistical Machine Translation - Machine Translation - Natural Language Processing - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation - Machine Translation '}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": docs_qa, \"question\": query}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Dion\"\n",
    "docs_qa = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='One of the key features of TensorFlow is its ability to support a wide range of machine learning algorithms and models, including deep neural networks, convolutional neural networks, and recurrent neural networks. It also provides a variety of tools and libraries for data pre-processing, model evaluation, and model deployment, making it a comprehensive solution for building end-to-end machine learning applications.\\n\\nTensorFlow has become one of the most widely used machine learning libraries in the world, with a large and active community of developers contributing to its development and evolution. It has been used to power a wide range of applications, including natural language processing, computer vision, speech recognition, and many others.\\n\\nText-To-Speech (TTS)', metadata={'source': 'omniscien.com_fix/blog/speech-recognition-speech-synthesis-glossary-o-u/index.html'}),\n",
       "  Document(page_content='One of the key features of TensorFlow is its ability to support a wide range of machine learning algorithms and models, including deep neural networks, convolutional neural networks, and recurrent neural networks. It also provides a variety of tools and libraries for data pre-processing, model evaluation, and model deployment, making it a comprehensive solution for building end-to-end machine learning applications.\\n\\nTensorFlow has become one of the most widely used machine learning libraries in the world, with a large and active community of developers contributing to its development and evolution. It has been used to power a wide range of applications, including natural language processing, computer vision, speech recognition, and many others.\\n\\nText-To-Speech (TTS)', metadata={'source': 'omniscien.com_fix/blog/speech-recognition-speech-synthesis-glossary-o-u/index.html'}),\n",
       "  Document(page_content='In summary, a Bottleneck MLP is a neural network architecture that is used in speech recognition systems to extract the most informative and compact features from raw spectral features. By forcing the network to extract the most important features, the bottleneck layer can significantly improve the accuracy of speech recognition systems while reducing computational costs.\\n\\nCadence\\n\\nCadence refers to the rhythm, flow, and melody of speech, including the timing, stress, and intonation of individual syllables and words. It is an essential aspect of human communication and plays a crucial role in conveying meaning, emotion, and intent in spoken language.', metadata={'source': 'omniscien.com_fix/blog/speech-recognition-speech-synthesis-glossary-a-g/Speech Recognition & Speech Synthesis Glossary (A-G) - Omniscien Technologies.html'}),\n",
       "  Document(page_content='In summary, a Bottleneck MLP is a neural network architecture that is used in speech recognition systems to extract the most informative and compact features from raw spectral features. By forcing the network to extract the most important features, the bottleneck layer can significantly improve the accuracy of speech recognition systems while reducing computational costs.\\n\\nCadence\\n\\nCadence refers to the rhythm, flow, and melody of speech, including the timing, stress, and intonation of individual syllables and words. It is an essential aspect of human communication and plays a crucial role in conveying meaning, emotion, and intent in spoken language.', metadata={'source': 'omniscien.com_fix/blog/speech-recognition-speech-synthesis-glossary-a-g/Speech Recognition & Speech Synthesis Glossary (A-G) - Omniscien Technologies.html'})],\n",
       " 'question': 'Who is Dion',\n",
       " 'output_text': 'Dion is a software engineer and a professor at the University of California, Berkeley. He is a leading expert in speech recognition and speech synthesis. He is a prolific author and has published over 100 papers and books on the subject. He is also a frequent speaker at conferences and workshops.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": docs_qa, \"question\": query}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db_openai = Chroma.from_documents(sp_docs, embeddings)\n",
    "\n",
    "model_name = \"text-davinci-003\"\n",
    "# model_name = \"gpt-3.5-turbo\"\n",
    "#model_name = \"gpt-4\"\n",
    "llm = OpenAI(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similiar_docs(query,k=2,score=False):\n",
    "  if score:\n",
    "    similar_docs = db_openai.similarity_search_with_score(query,k=k)\n",
    "  else:\n",
    "    similar_docs = db_openai.similarity_search(query,k=k)\n",
    "  return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "def get_answer(query):\n",
    "  similar_docs = get_similiar_docs(query)\n",
    "  # print(similar_docs)\n",
    "  answer =  chain.run(input_documents=similar_docs, question=query)\n",
    "  return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': ' Neural Machine Translation (NMT) is a state-of-the-art machine translation approach that utilizes neural network techniques to predict the likelihood of a set of words in sequence. It uses deep neural networks and artificial intelligence to train neural models, and typically produces much higher quality translations that Statistical Machine Translation approaches. \\nSOURCES: omnisciencom/faq/what-is-neural-machine-translation/index.html'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is NMT\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': \" I don't know.\\nSOURCES: omnisciencom/lsev6/ocr/optical-character-recognition-overview/index.html\"}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Dion wiggins\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': ' Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He is an accomplished speaker and has a high media profile for his perceptive analysis of ICT in Asia/Pacific. He was previously Vice President and Research Director for Gartner based in Hong Kong and is a founder of The ActiveX Factory. He is a former holder of a US O1 Extraordinary Ability Visa.\\n\\nSOURCES: omnisciencom/about-us/company/Index.html'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Dion wiggins\"  \n",
    "similar_docs_openai = db_openai.similarity_search(query)\n",
    "print(chain({\"input_documents\": similar_docs_openai, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Click here...', metadata={'source': 'omnisciencom/category/faq/index.html'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2 (not work yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import(\n",
    "    GPTVectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    LangchainEmbedding,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    )\n",
    "\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import download_loader \n",
    "\n",
    "#scrap website\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# upload model \n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from llama_index.llms import LangChainLLM\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_path):\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm_langchain = LlamaCpp(\n",
    "    model_path= model_path, \n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True, \n",
    "    n_ctx=2048) #define n-ctx for prevent exceed token error\n",
    "    llm = LangChainLLM(llm=llm_langchain)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_to_gpt_vectorstore(folder_path, model_path, model_emb_path):\n",
    "    \n",
    "    documents = SimpleDirectoryReader(folder_path).load_data()\n",
    "    #loader = DirectoryLoader(folder_path, glob=\"**/*.html\")\n",
    "    #documents = loader.load()\n",
    "    parser = SimpleNodeParser()\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    llm = load_llm(model_path)\n",
    "    llm_predictor = LLMPredictor(llm = llm)\n",
    "    embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=model_emb_path))\n",
    "\n",
    "\n",
    "    max_input_size = 4096\n",
    "    num_output = 512\n",
    "    max_chunk_overlap = 0.20\n",
    "    prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model,\n",
    "    prompt_helper=prompt_helper,\n",
    "    )\n",
    "\n",
    "    index = GPTVectorStoreIndex(nodes, service_context=service_context) \n",
    "    #index.save_to_disk(\"./gpt_index_docs_api_remotion_v2.json\") #cant use save_to_disk replace with storage_context\n",
    "    index.storage_context.persist(persist_dir=\"./llama_index_docs_api_v1\") # create json file for index\n",
    "    return index, service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.06 MB\n",
      "llama_model_load_internal: mem required  = 2862.72 MB (+  682.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "url = \"https://anaconda.org/conda-forge/attrs\"\n",
    "model_path = \"orca-mini-3b.ggmlv3.q4_0.bin\"\n",
    "model_emb_path = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "folder_path = 'omnisciencom'\n",
    "\n",
    "index, service_context = load_document_to_gpt_vectorstore(folder_path= folder_path, \n",
    "                                         model_path= model_path,\n",
    "                                         model_emb_path=model_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./llama_index_docs_api_v1\")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=1, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the purpose of this tool? \n",
      "\n",
      "My understanding is that this question cannot be answered without additional context. Can you please provide more information or clarify the question further?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 17716.62 ms\n",
      "llama_print_timings:      sample time =    21.25 ms /    37 runs   (    0.57 ms per token,  1741.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   455.64 ms /     4 tokens (  113.91 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time =  5620.15 ms /    36 runs   (  156.12 ms per token,     6.41 tokens per second)\n",
      "llama_print_timings:       total time =  8675.38 ms\n"
     ]
    }
   ],
   "source": [
    "response_stream = query_engine.query(\"What is Language studio?\")\n",
    "response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, LLMPredictor\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_path):      \n",
    "    llm = HuggingFaceHub(repo_id = model_path, model_kwargs = {\"temperature\":0, \"max_length\":512}) #770M parameters\t\t\t\n",
    "    return llm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents into nodes: 100%|██████████| 169/169 [04:17<00:00,  1.52s/it]\n",
      "Generating embeddings:   4%|▍         | 410/10273 [02:29<53:06,  3.10it/s]  "
     ]
    }
   ],
   "source": [
    "# create client and a new collection\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "# load documents\n",
    "url = \"omnisciencom\"\n",
    "documents = SimpleDirectoryReader(url, recursive = True).load_data()\n",
    "\n",
    "model_path = \"declare-lab/flan-alpaca-large\"\n",
    "llm = load_llm(model_path)\n",
    "llm_predictor = LLMPredictor(llm = llm)\n",
    "\n",
    "# define embedding function\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor = llm_predictor)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context, show_progress=True\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What did the author do growing up?\")\n",
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Omniscien is a company that specializes in language processing, voice recognition, OCR, data mining, and data automation.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"what is Omniscien technology\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"EleutherAI/pythia-1b\"\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm,  chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1635"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(similar_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': ' Neural'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is NMT\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I am wondering who will be absorbing the cost of implementing NMT? Will it be the end client, the LSPs or translators?\\n\\nPutting this question into perspective, an SMT engine that takes about 1 day to train on a multiple CPU system with about 256GB RAM. Training a NMT system such as this with CPU technology is not viable as it would take many months. Training using GPU technology still takes about 3 days or more for a large enigne. As such, the costs of training NMT engines are notably higher.', metadata={'source': 'omniscien.com/faq/conference-and-webinar-nmt-questions-and-answers/index.html'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#declare-lab/flan-alpaca-large\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': 'Dion is a founder of The ActiveX Factory, a former holder of a US O1 Extraordinary Ability Visa, and a professor at Omniscien. He is a pioneer in the machine translation space, with books on Statistical Machine Translation and Neural Machine Translation. He is also a professor at the University of California, Berkeley.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Dion\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
