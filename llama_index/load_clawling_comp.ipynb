{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1 Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/PradipNichite/Youtube-Tutorials/blob/main/Langchain_Semnatic_Serach_Pinecone.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "#for Huggingface\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "#for openai\n",
    "from langchain.llms import OpenAI\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3133\n"
     ]
    }
   ],
   "source": [
    "def load_docs(docs_path):\n",
    "    loader = DirectoryLoader(docs_path, glob=\"**/*.html\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    sp_docs = text_splitter.split_documents(documents)\n",
    "    return sp_docs\n",
    "\n",
    "documents = load_docs('omniscien.com')\n",
    "sp_docs = split_docs(documents)\n",
    "print(len(sp_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# save model \n",
    "# db1 = Chroma.from_documents(sp_docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "# db1.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model \n",
    "db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similiar_docs(query,k=1,score=False):\n",
    "  if score:\n",
    "    similar_docs = db.similarity_search_with_score(query,k=k)\n",
    "  else:\n",
    "    similar_docs = db.similarity_search(query,k=k)\n",
    "  return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"declare-lab/flan-alpaca-large\"\n",
    "llm_hug = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 768})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm_hug,  chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_documents': [Document(page_content='Search\\n\\nOmniscien » FAQ » What is Neural Machine Translation (NMT)?\\n\\nWhat is Neural Machine Translation (NMT)?\\n\\nNeural Machine Translation (also known\\xa0as Neural MT, NMT, Deep Neural Machine Translation, Deep NMT, or DNMT) is a state-of-the-art machine translation approach that utilizes neural network techniques to predict the likelihood of a set of words in sequence. This can be a text fragment, complete sentence, or with the latest advances an entire document.\\n\\nNMT is a\\xa0radically different approach to solving the problem of language translation and localization that uses deep neural networks and artificial intelligence to train neural models.\\xa0NMT has quickly become the dominant approach to machine translation with a major transition from SMT to NMT in just 3 years. Neural Machine Translation typically produces much higher quality translations that Statistical Machine Translation approaches, with better fluency and adequacy.', metadata={'source': 'omnisciencom/faq/what-is-neural-machine-translation/index.html'})], 'question': 'What is NMT', 'output_text': 'Neural Machine Translation (NMT) is a state-of-the-art machine translation approach that utilizes neural network techniques to predict the likelihood of a set of words in sequence. This can be a text fragment, complete sentence, or with the latest advances an entire document. NMT has quickly become the dominant approach to machine translation with a major transition from SMT to NMT in just 3 years.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is NMT\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Search\\n\\nOmniscien » FAQ » What is Neural Machine Translation (NMT)?\\n\\nWhat is Neural Machine Translation (NMT)?\\n\\nNeural Machine Translation (also known\\xa0as Neural MT, NMT, Deep Neural Machine Translation, Deep NMT, or DNMT) is a state-of-the-art machine translation approach that utilizes neural network techniques to predict the likelihood of a set of words in sequence. This can be a text fragment, complete sentence, or with the latest advances an entire document.\\n\\nNMT is a\\xa0radically different approach to solving the problem of language translation and localization that uses deep neural networks and artificial intelligence to train neural models.\\xa0NMT has quickly become the dominant approach to machine translation with a major transition from SMT to NMT in just 3 years. Neural Machine Translation typically produces much higher quality translations that Statistical Machine Translation approaches, with better fluency and adequacy.', metadata={'source': 'omnisciencom/faq/what-is-neural-machine-translation/index.html'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': \" Dion is the Vice President and Research Director for Gartner based in Hong Kong and is a well-known pioneer of the Asian Internet Industry. He is the founder of one of Asia's first ISPs (Asia Online in Hong Kong) and has been recognized by the US Government as being in the top 5% of his field worldwide.\\nSOURCES: omnisciencom/about-us/company/Index.html\"}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Dion\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Dion is a pioneer of the Asian Internet Industry, founder of one of Asia's first ISPs (Asia Online in Hong Kong), and was Vice President and Research Director for Gartner. He was the recipient of the Chairman's Commendation Award presented by Microsoft's Bill Gates for the best showcase of software developed in the Philippines, is recognized by the US Government as being in the top 5% of his field worldwide, and is a former holder of a US O1 Extraordinary Ability Visa.\\nSOURCES: omnisciencom/about-us/company/Index.html\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dion is a pioneer of the Asian Internet Industry, founder of one of Asia's first ISPs (Asia Online in Hong Kong), and was Vice President and Research Director for Gartner. He was the recipient of the Chairman's Commendation Award presented by Microsoft's Bill Gates for the best showcase of software developed in the Philippines, is recognized by the US Government as being in the top 5% of his field worldwide, and is a former holder of a US O1 Extraordinary Ability Visa.\n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r'\\n.*', '', y[\"output_text\"])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"Available as two Platform Editions specifically designed to match different business needs.\\n\\nProduct Overview\\n\\nFeatures\\n\\nBenefits of Media Studio (White Paper)\\n\\nSubtitle Optimized Machine Translation\\n\\nData Security & Privacy\\n\\nSecure by Design\\n\\nProject Management and Editing PlatformProject, People, Resource, Video, and Subtitle Management\\n\\nDetailed Features\\n\\nData Processing PlatformData Creation, Analysis, Cleaning, and Organization\\n\\nDetailed Features\\n\\nRequest a Demo\\n\\nFeature   Overview\\n\\nEach feature is built on a core of Artificial Intelligence, Machine Learning and Natural Language Processing\\n\\nMachine learning enables machines to work more like humans so that humans don't have to work more like machines. Each feature is designed to augment human intelligence, enhance productivity, increase quality, and reduce cost. Artificial intelligence enables processing and organization of data that simply not be cost-effective or feasible with a human only approach.\", metadata={'source': 'omnisciencom/products/media-studio/project-management-and-editing-platform/index.html'})],\n",
       " 'question': 'Who is Dion wiggins',\n",
       " 'output_text': ' Dion Wiggins is not mentioned in the source.\\nSOURCES: omniscien.com/products/media-studio/project-management-and-editing-platform/index.html'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/chains/popular/vector_db_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docsearch = Chroma.from_documents(sp_docs, embedding_function)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_hug, chain_type=\"stuff\", retriever=db.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is NMT\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neural Machine Translation is a powerful tool for language translation and localization. It is able to accurately translate text into different languages, with better fluency and adequacy. It is also able to generate text that is more readable and understandable.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Available as two Platform Editions specifically designed to match different business needs.\\n\\nProduct Overview\\n\\nFeatures\\n\\nBenefits of Media Studio (White Paper)\\n\\nSubtitle Optimized Machine Translation\\n\\nData Security & Privacy\\n\\nSecure by Design\\n\\nProject Management and Editing PlatformProject, People, Resource, Video, and Subtitle Management\\n\\nDetailed Features\\n\\nData Processing PlatformData Creation, Analysis, Cleaning, and Organization\\n\\nDetailed Features\\n\\nRequest a Demo\\n\\nFeature   Overview\\n\\nEach feature is built on a core of Artificial Intelligence, Machine Learning and Natural Language Processing\\n\\nMachine learning enables machines to work more like humans so that humans don't have to work more like machines. Each feature is designed to augment human intelligence, enhance productivity, increase quality, and reduce cost. Artificial intelligence enables processing and organization of data that simply not be cost-effective or feasible with a human only approach.\", metadata={'source': 'omnisciencom/products/media-studio/project-management-and-editing-platform/index.html'}),\n",
       " Document(page_content='Artificial Intelligence Built for   Smart Language Processing\\n\\nArtificial Intelligence Built for   Smart Language Processing\\n\\nData is the Fuel    that Powers Artificial Intelligence\\n\\nData is the Fuel    thatPowers Artificial Intelligence\\n\\nBuilt on the world’s leading translation, language processing, workflow automation, and artificial intelligence technologies.\\n\\nTranslation and language processing technologies have evolved substantially over the last decade. The Omniscien team has been at the forefront of research and development, leading the way with a comprehensive set of integrated tools, features, and technologies that are powered by and drive artificial intelligence and machine learning.', metadata={'source': 'omnisciencom/index.html'}),\n",
       " Document(page_content=\"Available as two Platform Editions specifically designed to match different business needs.\\n\\nProduct Overview\\n\\nFeatures\\n\\nBenefits of Media Studio (White Paper)\\n\\nSubtitle Optimized Machine Translation\\n\\nData Security & Privacy\\n\\nSecure by Design\\n\\nProject Management and Editing PlatformProject, People, Resource, Video, and Subtitle Management\\n\\nDetailed Features\\n\\nData Processing PlatformData Creation, Analysis, Cleaning, and Organization\\n\\nDetailed Features\\n\\nRequest a Demo\\n\\nFeature Detail\\n\\nEach feature is built on a core of Artificial Intelligence, Machine Learning and Natural Language Processing\\n\\nMachine learning enables machines to work more like humans so that humans don't have to work more like machines. Each feature is designed to augment human intelligence, enhance productivity, increase quality, and reduce cost. Artificial intelligence enables processing and organization of data that simply would not be cost-effective or feasible with a human only approach.\", metadata={'source': 'omnisciencom/products/media-studio/project-management-and-editing-platform-features/index.html'}),\n",
       " Document(page_content='One-shot learning has several potential applications, including image recognition, speech recognition, and natural language processing. As the field of artificial intelligence continues to evolve, one-shot learning is expected to play an increasingly important role in enabling machines to learn and recognize new patterns and objects with limited data.\\n\\nIn summary, one-shot learning is a machine learning approach that aims to learn from a single example. This approach is particularly useful in situations where obtaining large amounts of training data is difficult or impossible, and has potential applications in various fields such as image recognition and natural language processing.\\n\\nOOV\\n\\nSee Out Of Vocabulary\\n\\n%OOV\\n\\nSee Out Of Vocabulary Word Rate\\n\\nOpen Caption', metadata={'source': 'omnisciencom/blog/speech-recognition-speech-synthesis-glossary-o-u/index.html'})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/chains/additional/question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docsearch = Chroma.from_documents(sp_docs, embedding_function)\n",
    "query = \"What is NMT\"\n",
    "docs_qa = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'SOURCES: Neural Machine Translation (NMT) is a state-of-the-art machine translation approach that utilizes deep neural networks and artificial intelligence to train neural models. It is a radically different approach to solving the problem of language translation and localization that uses deep neural networks and artificial intelligence to train neural models. It has quickly become the dominant approach to machine translation with a major transition from SMT to NMT in just 3 years.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_with_sources_chain(llm_hug, chain_type=\"stuff\")\n",
    "query = \"What is NMT\"\n",
    "docs_qa = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Search\\n\\nOmniscien » FAQ » What is Neural Machine Translation (NMT)?\\n\\nWhat is Neural Machine Translation (NMT)?\\n\\nNeural Machine Translation (also known\\xa0as Neural MT, NMT, Deep Neural Machine Translation, Deep NMT, or DNMT) is a state-of-the-art machine translation approach that utilizes neural network techniques to predict the likelihood of a set of words in sequence. This can be a text fragment, complete sentence, or with the latest advances an entire document.\\n\\nNMT is a\\xa0radically different approach to solving the problem of language translation and localization that uses deep neural networks and artificial intelligence to train neural models.\\xa0NMT has quickly become the dominant approach to machine translation with a major transition from SMT to NMT in just 3 years. Neural Machine Translation typically produces much higher quality translations that Statistical Machine Translation approaches, with better fluency and adequacy.', metadata={'source': 'omnisciencom/faq/what-is-neural-machine-translation/index.html'}),\n",
       "  Document(page_content='See the Omniscien FAQ – What is Neural Machine Translation (NMT)?\\n\\nNeural Network\\n\\nNeural networks are a subset of machine learning that mimic the way that biological neurons signal to one another. They are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network.\\n\\nSee Wikipedia article on Neural network\\n\\nNMT\\n\\nSee Neural Machine Translation\\n\\nPEMT\\n\\nSee Post-Edited MT\\n\\nPivot Translation / Pivot Language Pairs', metadata={'source': 'omnisciencom/blog/localization-glossary-terminology-that-you-should-know/index.html'}),\n",
       "  Document(page_content='While NMT certainly is better overall (in most cases) than all the previous technologies, it has a number of weaknesses. There is no one-technology that will always perform well under any circumstance thrown at it. For this reason a hybrid confidence-based approach makes a lot more sense. This approach is user configurable and enables multiple technologies to “compete” with each other to produce the highest-possible quality machine translation output. SMT will typically produce a higher quality translation than NMT in 8-15 percent of the cases.', metadata={'source': 'omnisciencom/faq/different-types-of-machine-translation/index.html'}),\n",
       "  Document(page_content='wave of hype is going to be deep learning AI and in the case of machine translation that is known as Deep NMT. Today there are a handful of MT vendors, such as Omniscien Technologies, who are delivering Deep NMT commercially as compared to those offering NMT. Expect there to be some confusion around the difference between NMT and Deep NMT for some time.', metadata={'source': 'omnisciencom/blog/riding-machine-translation-hype-cycle/index.html'})],\n",
       " 'question': 'What is NMT',\n",
       " 'output_text': 'SOURCES: Neural Machine Translation (NMT) is a state-of-the-art machine translation approach that utilizes deep neural networks and artificial intelligence to train neural models. It is a radically different approach to solving the problem of language translation and localization that uses deep neural networks and artificial intelligence to train neural models. It has quickly become the dominant approach to machine translation with a major transition from SMT to NMT in just 3 years.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": docs_qa, \"question\": query}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Dion\"\n",
    "docs_qa = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"Available as two Platform Editions specifically designed to match different business needs.\\n\\nProduct Overview\\n\\nFeatures\\n\\nBenefits of Media Studio (White Paper)\\n\\nSubtitle Optimized Machine Translation\\n\\nData Security & Privacy\\n\\nSecure by Design\\n\\nProject Management and Editing PlatformProject, People, Resource, Video, and Subtitle Management\\n\\nDetailed Features\\n\\nData Processing PlatformData Creation, Analysis, Cleaning, and Organization\\n\\nDetailed Features\\n\\nRequest a Demo\\n\\nFeature   Overview\\n\\nEach feature is built on a core of Artificial Intelligence, Machine Learning and Natural Language Processing\\n\\nMachine learning enables machines to work more like humans so that humans don't have to work more like machines. Each feature is designed to augment human intelligence, enhance productivity, increase quality, and reduce cost. Artificial intelligence enables processing and organization of data that simply not be cost-effective or feasible with a human only approach.\", metadata={'source': 'omnisciencom/products/media-studio/project-management-and-editing-platform/index.html'}),\n",
       "  Document(page_content='Artificial Intelligence Built for   Smart Language Processing\\n\\nArtificial Intelligence Built for   Smart Language Processing\\n\\nData is the Fuel    that Powers Artificial Intelligence\\n\\nData is the Fuel    thatPowers Artificial Intelligence\\n\\nBuilt on the world’s leading translation, language processing, workflow automation, and artificial intelligence technologies.\\n\\nTranslation and language processing technologies have evolved substantially over the last decade. The Omniscien team has been at the forefront of research and development, leading the way with a comprehensive set of integrated tools, features, and technologies that are powered by and drive artificial intelligence and machine learning.', metadata={'source': 'omnisciencom/index.html'}),\n",
       "  Document(page_content=\"Available as two Platform Editions specifically designed to match different business needs.\\n\\nProduct Overview\\n\\nFeatures\\n\\nBenefits of Media Studio (White Paper)\\n\\nSubtitle Optimized Machine Translation\\n\\nData Security & Privacy\\n\\nSecure by Design\\n\\nProject Management and Editing PlatformProject, People, Resource, Video, and Subtitle Management\\n\\nDetailed Features\\n\\nData Processing PlatformData Creation, Analysis, Cleaning, and Organization\\n\\nDetailed Features\\n\\nRequest a Demo\\n\\nFeature Detail\\n\\nEach feature is built on a core of Artificial Intelligence, Machine Learning and Natural Language Processing\\n\\nMachine learning enables machines to work more like humans so that humans don't have to work more like machines. Each feature is designed to augment human intelligence, enhance productivity, increase quality, and reduce cost. Artificial intelligence enables processing and organization of data that simply would not be cost-effective or feasible with a human only approach.\", metadata={'source': 'omnisciencom/products/media-studio/project-management-and-editing-platform-features/index.html'}),\n",
       "  Document(page_content='One-shot learning has several potential applications, including image recognition, speech recognition, and natural language processing. As the field of artificial intelligence continues to evolve, one-shot learning is expected to play an increasingly important role in enabling machines to learn and recognize new patterns and objects with limited data.\\n\\nIn summary, one-shot learning is a machine learning approach that aims to learn from a single example. This approach is particularly useful in situations where obtaining large amounts of training data is difficult or impossible, and has potential applications in various fields such as image recognition and natural language processing.\\n\\nOOV\\n\\nSee Out Of Vocabulary\\n\\n%OOV\\n\\nSee Out Of Vocabulary Word Rate\\n\\nOpen Caption', metadata={'source': 'omnisciencom/blog/speech-recognition-speech-synthesis-glossary-o-u/index.html'})],\n",
       " 'question': 'Who is Dion',\n",
       " 'output_text': 'SOURCES: 1. Dion is a software engineer and a professor at the University of California, Berkeley. 2. He is the founder and CEO of Omniscien, a company that specializes in machine learning and natural language processing. 3. He is a frequent speaker at conferences and workshops, and is a frequent contributor to the AI and Machine Learning communities. 4. He is a frequent contributor to the AI and Machine Learning Journal. 5. He is a frequent contributor to the AI and Machine Learning Journal. 6. He is a frequent contributor to the AI and Machine Learning Journal. 7. He is a frequent contributor to the AI and Machine Learning Journal. 8. He is a frequent contributor to the AI and Machine Learning Journal. 9. He is a frequent contributor to the AI and Machine Learning Journal. 10. He is a frequent contributor to the AI and Machine Learning Journal.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input_documents\": docs_qa, \"question\": query}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db_openai = Chroma.from_documents(sp_docs, embeddings)\n",
    "\n",
    "model_name = \"text-davinci-003\"\n",
    "# model_name = \"gpt-3.5-turbo\"\n",
    "#model_name = \"gpt-4\"\n",
    "llm = OpenAI(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similiar_docs(query,k=1,score=False):\n",
    "  if score:\n",
    "    similar_docs = db_openai.similarity_search_with_score(query,k=k)\n",
    "  else:\n",
    "    similar_docs = db_openai.similarity_search(query,k=k)\n",
    "  return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "def get_answer(query):\n",
    "  similar_docs = get_similiar_docs(query)\n",
    "  # print(similar_docs)\n",
    "  answer =  chain.run(input_documents=similar_docs, question=query)\n",
    "  return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': ' Neural Machine Translation (NMT) is a state-of-the-art machine translation approach that utilizes neural network techniques to predict the likelihood of a set of words in sequence. It uses deep neural networks and artificial intelligence to train neural models, and typically produces much higher quality translations that Statistical Machine Translation approaches. \\nSOURCES: omnisciencom/faq/what-is-neural-machine-translation/index.html'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is NMT\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': \" I don't know.\\nSOURCES: omnisciencom/lsev6/ocr/optical-character-recognition-overview/index.html\"}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Dion wiggins\"  \n",
    "similar_docs = get_similiar_docs(query)\n",
    "print(chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': ' Dion Wiggins is a highly experienced ICT industry visionary, entrepreneur, analyst, and consultant. He is an accomplished speaker and has a high media profile for his perceptive analysis of ICT in Asia/Pacific. He was previously Vice President and Research Director for Gartner based in Hong Kong and is a founder of The ActiveX Factory. He is a former holder of a US O1 Extraordinary Ability Visa.\\n\\nSOURCES: omnisciencom/about-us/company/Index.html'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Dion wiggins\"  \n",
    "similar_docs_openai = db_openai.similarity_search(query)\n",
    "print(chain({\"input_documents\": similar_docs_openai, \"question\": query}, return_only_outputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Click here...', metadata={'source': 'omnisciencom/category/faq/index.html'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2 (not work yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import(\n",
    "    GPTVectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    LangchainEmbedding,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    )\n",
    "\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import download_loader \n",
    "\n",
    "#scrap website\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# upload model \n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from llama_index.llms import LangChainLLM\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_path):\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm_langchain = LlamaCpp(\n",
    "    model_path= model_path, \n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True, \n",
    "    n_ctx=2048) #define n-ctx for prevent exceed token error\n",
    "    llm = LangChainLLM(llm=llm_langchain)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_to_gpt_vectorstore(folder_path, model_path, model_emb_path):\n",
    "    \n",
    "    documents = SimpleDirectoryReader(folder_path).load_data()\n",
    "    #loader = DirectoryLoader(folder_path, glob=\"**/*.html\")\n",
    "    #documents = loader.load()\n",
    "    parser = SimpleNodeParser()\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    llm = load_llm(model_path)\n",
    "    llm_predictor = LLMPredictor(llm = llm)\n",
    "    embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=model_emb_path))\n",
    "\n",
    "\n",
    "    max_input_size = 4096\n",
    "    num_output = 512\n",
    "    max_chunk_overlap = 0.20\n",
    "    prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model,\n",
    "    prompt_helper=prompt_helper,\n",
    "    )\n",
    "\n",
    "    index = GPTVectorStoreIndex(nodes, service_context=service_context) \n",
    "    #index.save_to_disk(\"./gpt_index_docs_api_remotion_v2.json\") #cant use save_to_disk replace with storage_context\n",
    "    index.storage_context.persist(persist_dir=\"./llama_index_docs_api_v1\") # create json file for index\n",
    "    return index, service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.06 MB\n",
      "llama_model_load_internal: mem required  = 2862.72 MB (+  682.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "url = \"https://anaconda.org/conda-forge/attrs\"\n",
    "model_path = \"orca-mini-3b.ggmlv3.q4_0.bin\"\n",
    "model_emb_path = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "folder_path = 'omnisciencom'\n",
    "\n",
    "index, service_context = load_document_to_gpt_vectorstore(folder_path= folder_path, \n",
    "                                         model_path= model_path,\n",
    "                                         model_emb_path=model_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./llama_index_docs_api_v1\")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=1, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the purpose of this tool? \n",
      "\n",
      "My understanding is that this question cannot be answered without additional context. Can you please provide more information or clarify the question further?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 17716.62 ms\n",
      "llama_print_timings:      sample time =    21.25 ms /    37 runs   (    0.57 ms per token,  1741.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   455.64 ms /     4 tokens (  113.91 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:        eval time =  5620.15 ms /    36 runs   (  156.12 ms per token,     6.41 tokens per second)\n",
      "llama_print_timings:       total time =  8675.38 ms\n"
     ]
    }
   ],
   "source": [
    "response_stream = query_engine.query(\"What is Language studio?\")\n",
    "response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, LLMPredictor\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_path):      \n",
    "    llm = HuggingFaceHub(repo_id = model_path, model_kwargs = {\"temperature\":0, \"max_length\":512}) #770M parameters\t\t\t\n",
    "    return llm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents into nodes: 100%|██████████| 169/169 [04:17<00:00,  1.52s/it]\n",
      "Generating embeddings:   4%|▍         | 410/10273 [02:29<53:06,  3.10it/s]  "
     ]
    }
   ],
   "source": [
    "# create client and a new collection\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "# load documents\n",
    "url = \"omnisciencom\"\n",
    "documents = SimpleDirectoryReader(url, recursive = True).load_data()\n",
    "\n",
    "model_path = \"declare-lab/flan-alpaca-large\"\n",
    "llm = load_llm(model_path)\n",
    "llm_predictor = LLMPredictor(llm = llm)\n",
    "\n",
    "# define embedding function\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor = llm_predictor)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context, show_progress=True\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What did the author do growing up?\")\n",
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Omniscien is a company that specializes in language processing, voice recognition, OCR, data mining, and data automation.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"what is Omniscien technology\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "index = VectorstoreIndexCreator(\n",
    "    text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0),\n",
    "    embedding = embedding_function, vectorstore_cls = Chroma,\n",
    "    vectorstore_kwargs={\"persist_directory\":\"./vector_index_save_dir_path\"}).from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Chroma(persist_directory=\"./vector_index_save_dir_path\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"declare-lab/flan-alpaca-large\"\n",
    "llm_hug = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 768})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_25617/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">808267207.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_25617/808267207.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Chroma'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_25617/\u001b[0m\u001b[1;33m808267207.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_25617/808267207.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'Chroma'\u001b[0m object has no attribute \u001b[32m'query'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Who is Dion\"\n",
    "index.query(llm=llm_hug, question=query, chain_type = \"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm_hug, chain_type=\"stuff\", retriever=index.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is NMT\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NMT is a type of machine learning algorithm that uses natural language processing to generate text. It is used to generate text that is grammatically correct and is able to understand and respond to human language. It is useful for tasks such as text summarization, text summarization, and text summarization.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is dion\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dion is an excellent helper.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Philipp Koehn is a Chief Scientist.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is Philipp\"\n",
    "index.query(llm=llm_hug, question=query, chain_type = \"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Studio is helpful because it provides a powerful and flexible translation platform that can be used to automate tasks, provide data privacy and security, and provide a wide range of features that are usually found only on large web-based public providers.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is language studio\"\n",
    "index.query(llm=llm_hug, question=query, chain_type = \"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is media studio\"\n",
    "index.query(llm=llm_hug, question=query, chain_type = \"stuff\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
