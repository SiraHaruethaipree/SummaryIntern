{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import(\n",
    "    GPTVectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    LangchainEmbedding,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex\n",
    "    )\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# upload model \n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import HuggingFaceHub\n",
    "from llama_index import download_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_path):      \n",
    "    llm = HuggingFaceHub(repo_id = model_path, model_kwargs = {\"temperature\":0, \"max_length\":512}) #770M parameters\t\t\t\n",
    "    return llm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"omnisciencom\"\n",
    "model_path = \"declare-lab/flan-alpaca-large\"\n",
    "model_emb_path = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "def load_service_context(url, model_path, model_emb_path):\n",
    "    from llama_index import download_loader \n",
    "\n",
    "    documents = SimpleDirectoryReader(url, recursive = True).load_data()\n",
    "    parser = SimpleNodeParser()\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    llm = load_llm(model_path)\n",
    "    llm_predictor = LLMPredictor(llm = llm)\n",
    "    embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=model_emb_path))\n",
    "\n",
    "\n",
    "    max_input_size = 4096\n",
    "    num_output = 512\n",
    "    max_chunk_overlap = 0.20\n",
    "    prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model,\n",
    "    prompt_helper=prompt_helper,\n",
    "    )\n",
    "\n",
    "    return service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = load_service_context(url, model_path, model_emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTVectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex(nodes, service_context=service_context) \n",
    "index.storage_context.persist(persist_dir=\"./llama_index_docs_api_v1\") #take long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./llama_index_docs_api_v1\")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=False, similarity_top_k=1, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Dion is a computer scientist who is a pioneer in the field of statistical machine translation and neural machine translation research. He is a former holder of a US O1 Extraordinary Ability Visa.', source_nodes=[NodeWithScore(node=TextNode(id_='1412b9ab-0003-44ee-8c0a-7144f2791444', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d028cdd3-1f14-4aca-b1e7-6ffb998ed996', node_type=None, metadata={}, hash='1af4e54898b7d653f386d686dc99e4c4f125679ec4d0e953fc6448b51fc22cfb'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='22a46e29-5e10-4a7b-9a78-2b41298490f0', node_type=None, metadata={}, hash='0bedd1ca0c44e784d921551ad1280cc4e34f1220156b6554496169ce6110d3d6'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d3ce9409-74c8-4ce5-98d2-7a71db2aac4e', node_type=None, metadata={}, hash='376ca3b4ea669c603a95b54192590cb1b3290bef2a96a6e1f8c3ad32e74987bf')}, hash='eceb6690bd6bbf24dccfe8d9532426202c5b83c11a6d382265a49d5e62e3bfcb', text='Bill Gates for the best \\nshowcase of software developed in the Philippines. The US Government has\\n recognized Dion as being in the top 5% of his field worldwide and he is\\n a former holder of a US O1 Extraordinary Ability Visa.</p></div></div></div><div class=\"et_pb_module et_pb_team_member et_pb_team_member_2 clearfix  et_pb_text_align_justified et_pb_bg_layout_light\"><div class=\"et_pb_team_member_image et-waypoint et_pb_animation_off et-animated\"><img decoding=\"async\" src=\"Index_files/Koehn-1721.webp\" alt=\"Philipp Koehn\" class=\"wp-image-366\" width=\"171\" height=\"216\"></div><div class=\"et_pb_team_member_description\"><h3 class=\"et_pb_module_header\">Philipp Koehn</h3><p class=\"et_pb_member_position\">Chief Scientist</p><div><p>Behind\\n many of the tools design is Omniscien’s Chief Scientist, Professor \\nPhilipp Koehn who leads our team of researchers and developers. Philipp \\nis a pioneer in the machine translation space, his books on <a title=\"Statistical Machine Translation\" href=\"https://www.amazon.com/Statistical-Machine-Translation-Philipp-Koehn/dp/0521874157\" target=\"_blank\" rel=\"noopener noreferrer\">Statistical Machine Translation</a> and <a title=\"Neural Machine Translation\" href=\"https://www.amazon.com/Neural-Machine-Translation-Philipp-Koehn/dp/1108497322\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation</a>\\n are the leading academic textbooks globally on machine translation. \\nBoth books are available now from Amazon.com or leading book stores.</p><p>Philipp Koehn is a Professor of Computer Science at <a href=\"https://www.jhu.edu/\" target=\"_blank\" rel=\"noopener\">Johns Hopkins University</a> and also holds the Chair for Machine Translation in the School of Informatics at the <a href=\"https://www.ed.ac.uk/\" target=\"_blank\" rel=\"noopener\">University of Edinburgh</a>.\\n Philipp Koehn is a leader in the field of statistical machine \\ntranslation and neural machine translation research with over 100 \\npublications. He is the author of several textbooks and online training \\ncourses in the field.</p><p>Under his leadership, the <a href=\"https://www2.statmt.org/moses/\" target=\"_blank\" rel=\"noopener\">open source Moses system</a>\\n that, prior to NMT, became the de-facto standard toolkit for \\nstatistical machine translation in research and commercial deployment. \\nKoehn led international research projects such as <a href=\"https://www.euromatrixplus.net/\" target=\"_blank\" rel=\"noopener\">Euromatrix</a> and <a href=\"https://www.casmacat.eu/\" target=\"_blank\" rel=\"noopener\">CASMACAT</a>. Koehn’s research has been funded by the <a href=\"https://european-union.europa.eu/\" target=\"_blank\" rel=\"noopener\">European Union</a>, <a href=\"https://www.darpa.mil/\" target=\"_blank\" rel=\"noopener\">DARPA</a>, <a href=\"https://google.com/\" target=\"_blank\" rel=\"noopener\">Google</a>, <a href=\"https://www.meta.com/\" target=\"_blank\" rel=\"noopener\">Facebook</a>, <a href=\"https://amazon.com/\" target=\"_blank\" rel=\"noopener\">Amazon</a>, <a href=\"https://bloomberg.com/\" target=\"_blank\" rel=\"noopener\">Bloomberg</a>, and several other funding agencies. Koehn received his PhD in 2003 from the <a href=\"https://www.usc.edu/\" target=\"_blank\" rel=\"noopener\">University of Southern California</a> and was a postdoctoral research associate at <a href=\"https://www.mit.edu/\" target=\"_blank\" rel=\"noopener\">MIT</a>. He', start_char_idx=185586, end_char_idx=188926, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.24395908838455813)], metadata={'1412b9ab-0003-44ee-8c0a-7144f2791444': {}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation technology that has been developed to improve accuracy and translation quality?</p><p>&nbsp;</p></div></div><div class=\"et_pb_column et_pb_text et_pb_text_46 2nd-last-child\"><div class=\"et_pb_text_inner\"><h2>Deep Neural Machine Translation Technology</h2><p><a href=\"../../products/language-studio/index.html\">Language Studio</a> utilizes the latest in state-of-the-art translation technologies. Our custom machine translation engines utilize by <a href=\"https://github.com/Microsoft/BERT\" target=\"_blank\" rel=\"noopener noreferrer\">Google's BERT model and its pre-trained task-specific models</a> to provide high quality translations.</p><h3>BERT model  (<a href=\"https://github.com/Microsoft/BERT\" target=\"_blank\" rel=\"noopener noreferrer\">Russakovsky et al. 2017</a>) </h3><p><span style=\"font-weight: 400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   641.80 ms\n",
      "llama_print_timings:      sample time =   139.07 ms /   256 runs   (    0.54 ms per token,  1840.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 41530.51 ms /   256 runs   (  162.23 ms per token,     6.16 tokens per second)\n",
      "llama_print_timings:       total time = 42532.16 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='Translation technology that has been developed to improve accuracy and translation quality?</p><p>&nbsp;</p></div></div><div class=\"et_pb_column et_pb_text et_pb_text_46 2nd-last-child\"><div class=\"et_pb_text_inner\"><h2>Deep Neural Machine Translation Technology</h2><p><a href=\"../../products/language-studio/index.html\">Language Studio</a> utilizes the latest in state-of-the-art translation technologies. Our custom machine translation engines utilize by <a href=\"https://github.com/Microsoft/BERT\" target=\"_blank\" rel=\"noopener noreferrer\">Google\\'s BERT model and its pre-trained task-specific models</a> to provide high quality translations.</p><h3>BERT model \\xa0(<a href=\"https://github.com/Microsoft/BERT\" target=\"_blank\" rel=\"noopener noreferrer\">Russakovsky et al. 2017</a>)\\xa0</h3><p><span style=\"font-weight: 400', source_nodes=[NodeWithScore(node=TextNode(id_='0a9571f1-d5b6-436c-8342-a6427c151285', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='152f2eff-4233-4713-a1b6-9e594b7bfdb1', node_type=None, metadata={}, hash='b049f2654c4bf008c3ec792117edc02b1e1b94244fb63848c5e3790988377cc0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='32599fcd-4a81-45e9-90d3-d1f02b3d16ea', node_type=None, metadata={}, hash='bee582989ff8165f304bc639bf0868993b21c770b007a638bae7134ad0b8fbcf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='460786ed-8b5b-48b5-8725-d64eb06ef9d1', node_type=None, metadata={}, hash='a0098080248bcca462e9f3ab607b9d5808df0f9554f0bff9b04460d629f21bc4')}, hash='1be44f9c4025c0c2470ad51992434cc7952d0a2927e9d0356143ea54dafe6e99', text='NMT systems were based on Shallow NMT, with fewer layers. As the technology advanced it became possible to process with more layers and further improve accuracy and translation quality.</p><p>&nbsp;</p></div></div><div class=\"et_pb_module et_pb_image et_pb_image_16\"> <span class=\"et_pb_image_wrap \"><img decoding=\"async\" width=\"800\" height=\"293\" src=\"https://omniscien.com/wp-content/uploads/2020/10/ShallowNMTDeepNMT.png\" alt=\"Shallow NMT vs Deep NMT\" title=\"Shallow NMT vs Deep NMT\" srcset=\"https://omniscien.com/wp-content/uploads/2020/10/ShallowNMTDeepNMT.png 800w, https://omniscien.com/wp-content/uploads/2020/10/ShallowNMTDeepNMT-480x176.png 480w\" sizes=\"(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 800px, 100vw\" class=\"wp-image-11752\" /></span></div></div></div><div class=\"et_pb_row et_pb_row_15\"><div class=\"et_pb_column et_pb_column_4_4 et_pb_column_46  et_pb_css_mix_blend_mode_passthrough et-last-child\"><div class=\"et_pb_module et_pb_text et_pb_text_46  et_pb_text_align_justified et_pb_bg_layout_light\"><div class=\"et_pb_text_inner\"><h2>Neural Machine Translation Technology</h2><p><a href=\"../../products/language-studio/index.html\">Language Studio</a> utilizes the latest in state-of-the-art translation technologies. Our <a href=\"../../machine-translation/custom-mt-engines/index.html\">custom machine translation engines</a> utilize by the Recurrent Neural Network technology and Google&#8217;s Transformer model technology.</p><h3>Recurrent Neural Network \\xa0(<a href=\"http://aclweb.org/anthology/W17-4710\" target=\"_blank\" rel=\"noopener noreferrer\">Miceli Barone et al. 2017</a>)\\xa0</h3><p><span style=\"font-weight: 400;\">Recurrent Neural Networks (RNNs) are a class of neural networks that allow previous outputs to be used as inputs while having hidden states. Deep Recurrent Neural Networks (RNNs) with Deep Transition Cells are one of the more advanced form of RNN and used by Omniscien as the basis for many of our </span><a href=\"../../machine-translation/custom-mt-engines/index.html\"><span style=\"font-weight: 400;\">custom NMT engines</span></a><span style=\"font-weight: 400;\">.</span></p><p><span style=\"font-size: 14px;\">RNNs are a network of\\xa0 </span><a href=\"https://en.wikipedia.org/wiki/Artificial_neuron\" target=\"_blank\" title=\"Artificial neuron\" rel=\"noopener noreferrer\" style=\"font-size: 14px;\">neuron-like</a><span style=\"font-size: 14px;\"> \\xa0nodes organized into successive layers. Each node in a given layer is connected with a\\xa0 </span><a href=\"https://en.wikipedia.org/wiki/Directed_graph\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"font-size: 14px;\">directed (one-way) connection</a><span style=\"font-size: 14px;\"> \\xa0to every other node in the next successive layer.\\xa0 Each node (neuron) has a time-varying real-valued activation. Each connection (synapse) has a modifiable real-valued\\xa0 </span><a href=\"https://en.wikipedia.org/wiki/Weighting\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"font-size: 14px;\">weight</a><span style=\"font-size: 14px;\"> . Nodes are either input nodes (receiving data from outside of the', start_char_idx=186832, end_char_idx=189914, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5429282597519356)], metadata={'0a9571f1-d5b6-436c-8342-a6427c151285': {}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is NMT\"\n",
    "response_stream = query_engine.query(query)\n",
    "response_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NMT is a type of machine translation technology that uses neural networks to process text. It is based on Shallow NMT, with fewer layers, and is used for text classification tasks such as text summarization and text summarization. Deep NMT is a type of RNN that uses a network of connected nodes to process text. It is used for text classification tasks such as text summarization and text summarization.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_stream.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Dion is a computer scientist who is a pioneer in the field of statistical machine translation and neural machine translation research. He is a former holder of a US O1 Extraordinary Ability Visa.', source_nodes=[NodeWithScore(node=TextNode(id_='1412b9ab-0003-44ee-8c0a-7144f2791444', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d028cdd3-1f14-4aca-b1e7-6ffb998ed996', node_type=None, metadata={}, hash='1af4e54898b7d653f386d686dc99e4c4f125679ec4d0e953fc6448b51fc22cfb'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='22a46e29-5e10-4a7b-9a78-2b41298490f0', node_type=None, metadata={}, hash='0bedd1ca0c44e784d921551ad1280cc4e34f1220156b6554496169ce6110d3d6'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d3ce9409-74c8-4ce5-98d2-7a71db2aac4e', node_type=None, metadata={}, hash='376ca3b4ea669c603a95b54192590cb1b3290bef2a96a6e1f8c3ad32e74987bf')}, hash='eceb6690bd6bbf24dccfe8d9532426202c5b83c11a6d382265a49d5e62e3bfcb', text='Bill Gates for the best \\nshowcase of software developed in the Philippines. The US Government has\\n recognized Dion as being in the top 5% of his field worldwide and he is\\n a former holder of a US O1 Extraordinary Ability Visa.</p></div></div></div><div class=\"et_pb_module et_pb_team_member et_pb_team_member_2 clearfix  et_pb_text_align_justified et_pb_bg_layout_light\"><div class=\"et_pb_team_member_image et-waypoint et_pb_animation_off et-animated\"><img decoding=\"async\" src=\"Index_files/Koehn-1721.webp\" alt=\"Philipp Koehn\" class=\"wp-image-366\" width=\"171\" height=\"216\"></div><div class=\"et_pb_team_member_description\"><h3 class=\"et_pb_module_header\">Philipp Koehn</h3><p class=\"et_pb_member_position\">Chief Scientist</p><div><p>Behind\\n many of the tools design is Omniscien’s Chief Scientist, Professor \\nPhilipp Koehn who leads our team of researchers and developers. Philipp \\nis a pioneer in the machine translation space, his books on <a title=\"Statistical Machine Translation\" href=\"https://www.amazon.com/Statistical-Machine-Translation-Philipp-Koehn/dp/0521874157\" target=\"_blank\" rel=\"noopener noreferrer\">Statistical Machine Translation</a> and <a title=\"Neural Machine Translation\" href=\"https://www.amazon.com/Neural-Machine-Translation-Philipp-Koehn/dp/1108497322\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation</a>\\n are the leading academic textbooks globally on machine translation. \\nBoth books are available now from Amazon.com or leading book stores.</p><p>Philipp Koehn is a Professor of Computer Science at <a href=\"https://www.jhu.edu/\" target=\"_blank\" rel=\"noopener\">Johns Hopkins University</a> and also holds the Chair for Machine Translation in the School of Informatics at the <a href=\"https://www.ed.ac.uk/\" target=\"_blank\" rel=\"noopener\">University of Edinburgh</a>.\\n Philipp Koehn is a leader in the field of statistical machine \\ntranslation and neural machine translation research with over 100 \\npublications. He is the author of several textbooks and online training \\ncourses in the field.</p><p>Under his leadership, the <a href=\"https://www2.statmt.org/moses/\" target=\"_blank\" rel=\"noopener\">open source Moses system</a>\\n that, prior to NMT, became the de-facto standard toolkit for \\nstatistical machine translation in research and commercial deployment. \\nKoehn led international research projects such as <a href=\"https://www.euromatrixplus.net/\" target=\"_blank\" rel=\"noopener\">Euromatrix</a> and <a href=\"https://www.casmacat.eu/\" target=\"_blank\" rel=\"noopener\">CASMACAT</a>. Koehn’s research has been funded by the <a href=\"https://european-union.europa.eu/\" target=\"_blank\" rel=\"noopener\">European Union</a>, <a href=\"https://www.darpa.mil/\" target=\"_blank\" rel=\"noopener\">DARPA</a>, <a href=\"https://google.com/\" target=\"_blank\" rel=\"noopener\">Google</a>, <a href=\"https://www.meta.com/\" target=\"_blank\" rel=\"noopener\">Facebook</a>, <a href=\"https://amazon.com/\" target=\"_blank\" rel=\"noopener\">Amazon</a>, <a href=\"https://bloomberg.com/\" target=\"_blank\" rel=\"noopener\">Bloomberg</a>, and several other funding agencies. Koehn received his PhD in 2003 from the <a href=\"https://www.usc.edu/\" target=\"_blank\" rel=\"noopener\">University of Southern California</a> and was a postdoctoral research associate at <a href=\"https://www.mit.edu/\" target=\"_blank\" rel=\"noopener\">MIT</a>. He', start_char_idx=185586, end_char_idx=188926, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.24395908838455813)], metadata={'1412b9ab-0003-44ee-8c0a-7144f2791444': {}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Who is dion\"\n",
    "response_stream = query_engine.query(query)\n",
    "response_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dion is a computer scientist who is a pioneer in the field of statistical machine translation and neural machine translation research. He is a former holder of a US O1 Extraordinary Ability Visa.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_stream.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStoreIndex define node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(url, recursive = True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "index = VectorStoreIndex(nodes, service_context=service_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, LLMPredictor\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_path):      \n",
    "    llm = HuggingFaceHub(repo_id = model_path, model_kwargs = {\"temperature\":0, \"max_length\":512}) #770M parameters\t\t\t\n",
    "    return llm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents into nodes: 100%|██████████| 169/169 [04:17<00:00,  1.52s/it]\n",
      "Generating embeddings:   4%|▍         | 410/10273 [02:29<53:06,  3.10it/s]  "
     ]
    }
   ],
   "source": [
    "# create client and a new collection\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "# load documents\n",
    "url = \"omnisciencom\"\n",
    "documents = SimpleDirectoryReader(url, recursive = True).load_data()\n",
    "\n",
    "model_path = \"declare-lab/flan-alpaca-large\"\n",
    "llm = load_llm(model_path)\n",
    "llm_predictor = LLMPredictor(llm = llm)\n",
    "\n",
    "# define embedding function\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor = llm_predictor)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context, show_progress=True\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What did the author do growing up?\")\n",
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Omniscien is a company that specializes in language processing, voice recognition, OCR, data mining, and data automation.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"what is Omniscien technology\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStoreIndex Not Use define node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents into nodes: 100%|██████████| 169/169 [04:29<00:00,  1.60s/it]\n",
      "Generating embeddings: 100%|██████████| 20546/20546 [2:07:19<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorStore Index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocstoreDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores import NebulaGraphStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "url = \"omnisciencom\"\n",
    "documents = SimpleDirectoryReader(url, recursive = True).load_data()\n",
    "nodes = SimpleNodeParser().get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_path = \"declare-lab/flan-alpaca-large\"\n",
    "model_emb_path = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "def load_llm(model_path):      \n",
    "    llm = HuggingFaceHub(repo_id = model_path, model_kwargs = {\"temperature\":0, \"max_length\":512}) #770M parameters\t\t\t\n",
    "    return llm   \n",
    "llm = load_llm(model_path)\n",
    "llm_predictor = LLMPredictor(llm = llm)\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=model_emb_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ListIndex, SimpleKeywordTableIndex\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "list_index = ListIndex(nodes, storage_context=storage_context)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor = llm_predictor)\n",
    "#vector_index = VectorStoreIndex(nodes, storage_context=storage_context, service_context = service_context, show_progress=True)\n",
    "keyword_table_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context_llama = ServiceContext.from_defaults(llm=llm, chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_6887/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">752556012.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_6887/752556012.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/query/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">query</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_manager.as_trace(<span style=\"color: #808000; text-decoration-color: #808000\">\"query\"</span>):                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(str_or_query_bundle, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   │   │   │   </span>str_or_query_bundle = QueryBundle(str_or_query_bundle)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._query(str_or_query_bundle)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> response                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">async</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">aquery</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, str_or_query_bundle: QueryType) -&gt; RESPONSE_TYPE:                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/query_engine/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">retri</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ever_query_engine.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">154</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_query</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 │   │   │   </span>event_id=retrieve_id,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">153 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>154 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._response_synthesizer.synthesize(                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155 │   │   │   </span>query=query_bundle,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156 │   │   │   </span>nodes=nodes,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">157 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/response_synthesiz</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">124</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">synthesize</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(query, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   │   │   </span>query = QueryBundle(query_str=query)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>124 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response_str = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_response(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 │   │   │   </span>query_str=query.query_str,                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   │   </span>text_chunks=[                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 │   │   │   │   </span>n.node.get_content(metadata_mode=MetadataMode.LLM) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> n <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> nodes          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/response_synthesiz</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">compact_and_refine.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">34</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_response</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 │   │   # TODO: This is a temporary fix - reason it's temporary is that</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 │   │   # the refine template does not account for size of previous answer.</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 │   │   </span>new_texts = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._make_compact_text_chunks(query_str, text_chunks)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>34 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().get_response(                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 │   │   │   </span>query_str=query_str, text_chunks=new_texts, **response_kwargs                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 │   │   </span>)                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> response                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/response_synthesiz</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">refine.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">49</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_response</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 46 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> prev_response_obj <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 47 │   │   │   │   # if this is the first chunk, and text chunk already</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 48 │   │   │   │   # is an answer, then return it</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 49 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._give_response_single(                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 50 │   │   │   │   │   </span>query_str,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 51 │   │   │   │   │   </span>text_chunk,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 52 │   │   │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/response_synthesiz</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">refine.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">80</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_give_response_single</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 77 │   │   # TODO: consolidate with loop in get_response_default</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 78 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> cur_text_chunk <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> text_chunks:                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 79 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> response <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._streaming:                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 80 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._service_context.llm_predictor.predict(                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 81 │   │   │   │   │   </span>text_qa_template,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 82 │   │   │   │   │   </span>context_str=cur_text_chunk,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 83 │   │   │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llm_predictor/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">123</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 │   │   │   </span>formatted_prompt = messages_to_prompt(messages)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   │   │   </span>formatted_prompt = prompt.format(llm=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._llm, **prompt_args)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>123 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._llm.complete(formatted_prompt)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 │   │   │   </span>output = response.text                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   </span>logger.debug(output)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">langchain.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">46</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">complete</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 43 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ChatResponse(message=message)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 44 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 45 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">complete</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, prompt: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, **kwargs: Any) -&gt; CompletionResponse:                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 46 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output_str = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._llm.predict(prompt, **kwargs)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 47 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> CompletionResponse(text=output_str)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 48 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 49 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">stream_chat</span>(                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">467</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">464 │   │   │   </span>_stop = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">465 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">466 │   │   │   </span>_stop = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(stop)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>467 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(text, stop=_stop, **kwargs)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">468 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">469 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict_messages</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">470 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">427</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">424 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"`generate` instead.\"</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">425 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">426 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> (                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>427 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.generate(                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">428 │   │   │   │   </span>[prompt],                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">429 │   │   │   │   </span>stop=stop,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">430 │   │   │   │   </span>callbacks=callbacks,                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">279</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">276 │   │   │   </span>run_managers = callback_manager.on_llm_start(                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 │   │   │   │   </span>dumpd(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>), prompts, invocation_params=params, options=options            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>279 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._generate_helper(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">280 │   │   │   │   </span>prompts, stop, run_managers, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>(new_arg_supported), **kwargs             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> output                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">223</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate_helper</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> run_manager <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> run_managers:                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">222 │   │   │   │   </span>run_manager.on_llm_error(e)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>223 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> e                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">224 │   │   </span>flattened_outputs = output.flatten()                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">225 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> manager, flattened_output <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(run_managers, flattened_outputs):             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">226 │   │   │   </span>manager.on_llm_end(flattened_output)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">210</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate_helper</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">207 │   </span>) -&gt; LLMResult:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">208 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">209 │   │   │   </span>output = (                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>210 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._generate(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">211 │   │   │   │   │   </span>prompts,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">212 │   │   │   │   │   </span>stop=stop,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213 │   │   │   │   │   # TODO: support multiple run managers</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">602</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">599 │   │   </span>new_arg_supported = inspect.signature(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call).parameters.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"run_manager\"</span>)    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">600 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> prompt <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> prompts:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">601 │   │   │   </span>text = (                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>602 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(prompt, stop=stop, run_manager=run_manager, **kwargs)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">603 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> new_arg_supported                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">604 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(prompt, stop=stop, **kwargs)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">605 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">huggingface_hub</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   </span>params = {**_model_kwargs, **kwargs}                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.client(inputs=prompt, params=params)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"error\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> response:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Error raised by inference API: {</span>response[<span style=\"color: #808000; text-decoration-color: #808000\">'error'</span>]<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.client.task == <span style=\"color: #808000; text-decoration-color: #808000\">\"text-generation\"</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   │   │   # Text generation return includes the starter text.</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   │   │   </span>text = response[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #808000; text-decoration-color: #808000\">\"generated_text\"</span>][<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(prompt) :]                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Error raised by inference API: Internal Server Error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_6887/\u001b[0m\u001b[1;33m752556012.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_6887/752556012.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/query/\u001b[0m\u001b[1;33mbase\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92mquery\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.callback_manager.as_trace(\u001b[33m\"\u001b[0m\u001b[33mquery\u001b[0m\u001b[33m\"\u001b[0m):                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(str_or_query_bundle, \u001b[96mstr\u001b[0m):                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mstr_or_query_bundle = QueryBundle(str_or_query_bundle)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 \u001b[2m│   │   │   \u001b[0mresponse = \u001b[96mself\u001b[0m._query(str_or_query_bundle)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m response                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m \u001b[92maquery\u001b[0m(\u001b[96mself\u001b[0m, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/query_engine/\u001b[0m\u001b[1;33mretri\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mever_query_engine.py\u001b[0m:\u001b[94m154\u001b[0m in \u001b[92m_query\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   │   \u001b[0mevent_id=retrieve_id,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m154 \u001b[2m│   │   \u001b[0mresponse = \u001b[96mself\u001b[0m._response_synthesizer.synthesize(                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   │   │   \u001b[0mquery=query_bundle,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   │   \u001b[0mnodes=nodes,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/response_synthesiz\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mers/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m124\u001b[0m in \u001b[92msynthesize\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(query, \u001b[96mstr\u001b[0m):                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   \u001b[0mquery = QueryBundle(query_str=query)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m124 \u001b[2m│   │   \u001b[0mresponse_str = \u001b[96mself\u001b[0m.get_response(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   │   \u001b[0mquery_str=query.query_str,                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext_chunks=[                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m127 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mn.node.get_content(metadata_mode=MetadataMode.LLM) \u001b[94mfor\u001b[0m n \u001b[95min\u001b[0m nodes          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/response_synthesiz\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mers/\u001b[0m\u001b[1;33mcompact_and_refine.py\u001b[0m:\u001b[94m34\u001b[0m in \u001b[92mget_response\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# TODO: This is a temporary fix - reason it's temporary is that\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# the refine template does not account for size of previous answer.\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   │   \u001b[0mnew_texts = \u001b[96mself\u001b[0m._make_compact_text_chunks(query_str, text_chunks)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m34 \u001b[2m│   │   \u001b[0mresponse = \u001b[96msuper\u001b[0m().get_response(                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m│   │   │   \u001b[0mquery_str=query_str, text_chunks=new_texts, **response_kwargs                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m response                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/response_synthesiz\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mers/\u001b[0m\u001b[1;33mrefine.py\u001b[0m:\u001b[94m49\u001b[0m in \u001b[92mget_response\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 46 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m prev_response_obj \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 47 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# if this is the first chunk, and text chunk already\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 48 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# is an answer, then return it\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 49 \u001b[2m│   │   │   │   \u001b[0mresponse = \u001b[96mself\u001b[0m._give_response_single(                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 50 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mquery_str,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 51 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mtext_chunk,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 52 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/response_synthesiz\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mers/\u001b[0m\u001b[1;33mrefine.py\u001b[0m:\u001b[94m80\u001b[0m in \u001b[92m_give_response_single\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# TODO: consolidate with loop in get_response_default\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 78 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m cur_text_chunk \u001b[95min\u001b[0m text_chunks:                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m response \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._streaming:                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 80 \u001b[2m│   │   │   │   \u001b[0mresponse = \u001b[96mself\u001b[0m._service_context.llm_predictor.predict(                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mtext_qa_template,                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 82 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcontext_str=cur_text_chunk,                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 83 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llm_predictor/\u001b[0m\u001b[1;33mbase\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m123\u001b[0m in \u001b[92mpredict\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0mformatted_prompt = messages_to_prompt(messages)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   \u001b[0mformatted_prompt = prompt.format(llm=\u001b[96mself\u001b[0m._llm, **prompt_args)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m123 \u001b[2m│   │   │   \u001b[0mresponse = \u001b[96mself\u001b[0m._llm.complete(formatted_prompt)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput = response.text                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.debug(output)                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/\u001b[0m\u001b[1;33mlangchain.py\u001b[0m: \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m46\u001b[0m in \u001b[92mcomplete\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 43 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m ChatResponse(message=message)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 44 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 45 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcomplete\u001b[0m(\u001b[96mself\u001b[0m, prompt: \u001b[96mstr\u001b[0m, **kwargs: Any) -> CompletionResponse:                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 46 \u001b[2m│   │   \u001b[0moutput_str = \u001b[96mself\u001b[0m._llm.predict(prompt, **kwargs)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 47 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m CompletionResponse(text=output_str)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 48 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 49 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mstream_chat\u001b[0m(                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m467\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mpredict\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m464 \u001b[0m\u001b[2m│   │   │   \u001b[0m_stop = \u001b[94mNone\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m465 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m466 \u001b[0m\u001b[2m│   │   │   \u001b[0m_stop = \u001b[96mlist\u001b[0m(stop)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m467 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m(text, stop=_stop, **kwargs)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m468 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m469 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mpredict_messages\u001b[0m(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m470 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m427\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__call__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m424 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m`generate` instead.\u001b[0m\u001b[33m\"\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m425 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m426 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m (                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m427 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.generate(                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m428 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m[prompt],                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m429 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mstop=stop,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m430 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcallbacks=callbacks,                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m279\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mgenerate\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m276 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_managers = callback_manager.on_llm_start(                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdumpd(\u001b[96mself\u001b[0m), prompts, invocation_params=params, options=options            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m279 \u001b[2m│   │   │   \u001b[0moutput = \u001b[96mself\u001b[0m._generate_helper(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m280 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mprompts, stop, run_managers, \u001b[96mbool\u001b[0m(new_arg_supported), **kwargs             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m output                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m223\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate_helper\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m (\u001b[96mKeyboardInterrupt\u001b[0m, \u001b[96mException\u001b[0m) \u001b[94mas\u001b[0m e:                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m run_manager \u001b[95min\u001b[0m run_managers:                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mrun_manager.on_llm_error(e)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m223 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m e                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m│   │   \u001b[0mflattened_outputs = output.flatten()                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m manager, flattened_output \u001b[95min\u001b[0m \u001b[96mzip\u001b[0m(run_managers, flattened_outputs):             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m│   │   │   \u001b[0mmanager.on_llm_end(flattened_output)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m210\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate_helper\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m207 \u001b[0m\u001b[2m│   \u001b[0m) -> LLMResult:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m208 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m209 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput = (                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m210 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._generate(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m211 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mprompts,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m212 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mstop=stop,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m213 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# TODO: support multiple run managers\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m602\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m599 \u001b[0m\u001b[2m│   │   \u001b[0mnew_arg_supported = inspect.signature(\u001b[96mself\u001b[0m._call).parameters.get(\u001b[33m\"\u001b[0m\u001b[33mrun_manager\u001b[0m\u001b[33m\"\u001b[0m)    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m600 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m prompt \u001b[95min\u001b[0m prompts:                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m601 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext = (                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m602 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._call(prompt, stop=stop, run_manager=run_manager, **kwargs)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m603 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m new_arg_supported                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m604 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[96mself\u001b[0m._call(prompt, stop=stop, **kwargs)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m605 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mhuggingface_hub\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92m_call\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0mparams = {**_model_kwargs, **kwargs}                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   \u001b[0mresponse = \u001b[96mself\u001b[0m.client(inputs=prompt, params=params)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33merror\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m response:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mError raised by inference API: \u001b[0m\u001b[33m{\u001b[0mresponse[\u001b[33m'\u001b[0m\u001b[33merror\u001b[0m\u001b[33m'\u001b[0m]\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.client.task == \u001b[33m\"\u001b[0m\u001b[33mtext-generation\u001b[0m\u001b[33m\"\u001b[0m:                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Text generation return includes the starter text.\u001b[0m                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext = response[\u001b[94m0\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mgenerated_text\u001b[0m\u001b[33m\"\u001b[0m][\u001b[96mlen\u001b[0m(prompt) :]                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mError raised by inference API: Internal Server Error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = list_index.as_query_engine(service_context = service_context)\n",
    "response = query_engine.query(\"What is a summary of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_6887/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2966680582.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_6887/2966680582.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_6887/\u001b[0m\u001b[1;33m2966680582.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_6887/2966680582.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'response'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_6887/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2184414091.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_6887/2184414091.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/query/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">query</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_manager.as_trace(<span style=\"color: #808000; text-decoration-color: #808000\">\"query\"</span>):                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(str_or_query_bundle, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   │   │   │   </span>str_or_query_bundle = QueryBundle(str_or_query_bundle)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._query(str_or_query_bundle)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> response                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">async</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">aquery</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, str_or_query_bundle: QueryType) -&gt; RESPONSE_TYPE:                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/query_engine/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">retri</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ever_query_engine.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">147</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_query</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">144 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">145 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 │   │   </span>retrieve_id = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_manager.on_event_start(CBEventType.RETRIEVE)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>147 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>nodes = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.retrieve(query_bundle)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_manager.on_event_end(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">149 │   │   │   </span>CBEventType.RETRIEVE,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 │   │   │   </span>payload={EventPayload.NODES: nodes},                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/query_engine/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">retri</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ever_query_engine.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">107</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>107 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>nodes = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._retriever.retrieve(query_bundle)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> node_postprocessor <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._node_postprocessors:                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 │   │   │   </span>nodes = node_postprocessor.postprocess_nodes(                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base_retri</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ever.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">21</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(str_or_query_bundle, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   │   │   </span>str_or_query_bundle = QueryBundle(str_or_query_bundle)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>21 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._retrieve(str_or_query_bundle)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@abstractmethod</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_retrieve</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/keyword_ta</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ble/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">retrievers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">81</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_retrieve</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 78 │   </span>) -&gt; List[NodeWithScore]:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 79 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Get nodes for response.\"\"\"</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 80 │   │   </span>logger.info(<span style=\"color: #808000; text-decoration-color: #808000\">f\"&gt; Starting query: {</span>query_bundle.query_str<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 81 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>keywords = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._get_keywords(query_bundle.query_str)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 82 │   │   </span>logger.info(<span style=\"color: #808000; text-decoration-color: #808000\">f\"query keywords: {</span>keywords<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 83 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 84 │   │   # go through text chunks in order of most matching keywords</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/keyword_ta</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ble/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">retrievers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">121</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_keywords</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_keywords</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, query_str: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) -&gt; List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>]:                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Extract keywords.\"\"\"</span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>121 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._service_context.llm_predictor.predict(                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.query_keyword_extract_template,                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 │   │   │   </span>max_keywords=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.max_keywords_per_query,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 │   │   │   </span>question=query_str,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llm_predictor/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">123</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 │   │   │   </span>formatted_prompt = messages_to_prompt(messages)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   │   │   </span>formatted_prompt = prompt.format(llm=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._llm, **prompt_args)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>123 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._llm.complete(formatted_prompt)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 │   │   │   </span>output = response.text                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   </span>logger.debug(output)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">openai.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">72</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">complete</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 69 │   │   │   </span>complete_fn = chat_to_completion_decorator(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._chat)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 70 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 71 │   │   │   </span>complete_fn = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._complete                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 72 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> complete_fn(prompt, **kwargs)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 73 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 74 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">stream_complete</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, prompt: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, **kwargs: Any) -&gt; CompletionResponseGen:        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 75 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._is_chat_model:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">openai.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">183</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_complete</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180 │   │   │   </span>max_tokens = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._get_max_token_for_prompt(prompt)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">181 │   │   │   </span>all_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"max_tokens\"</span>] = max_tokens                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">182 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>183 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = completion_with_retry(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 │   │   │   </span>is_chat_model=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._is_chat_model,                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">185 │   │   │   </span>max_retries=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.max_retries,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">186 │   │   │   </span>prompt=prompt,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">openai_utils.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">123</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">completion_with_retry</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 │   │   </span>client = get_completion_endpoint(is_chat_model)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> client.create(**kwargs)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>123 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _completion_with_retry(**kwargs)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">async</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">acompletion_with_retry</span>(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tenacity/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">289</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapped_f</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(f)                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">288 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapped_f</span>(*args: t.Any, **kw: t.Any) -&gt; t.Any:                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>289 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(f, *args, **kw)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">retry_with</span>(*args: t.Any, **kwargs: t.Any) -&gt; WrappedFn:                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.copy(*args, **kwargs).wraps(f)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tenacity/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">379</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">376 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">377 │   │   </span>retry_state = RetryCallState(retry_object=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, fn=fn, args=args, kwargs=kwargs)   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">378 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>379 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>do = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.iter(retry_state=retry_state)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">380 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(do, DoAttempt):                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">381 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">382 │   │   │   │   │   </span>result = fn(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tenacity/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">314</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">iter</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">311 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">312 │   │   </span>is_explicit_retry = fut.failed <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(fut.exception(), TryAgain)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">313 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (is_explicit_retry <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.retry(retry_state)):                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>314 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fut.result()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">316 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.after <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.after(retry_state)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/concurrent/futures/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">439</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">result</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">436 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._state <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> [CANCELLED, CANCELLED_AND_NOTIFIED]:                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">437 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> CancelledError()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">438 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._state == FINISHED:                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>439 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.__get_result()                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">440 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">441 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._condition.wait(timeout)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">442 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/concurrent/futures/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">391</span> in           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__get_result</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">388 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__get_result</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">389 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._exception:                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">390 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>391 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._exception                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">392 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">393 │   │   │   │   # Break a reference cycle with the exception in self._exception</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">394 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span> = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tenacity/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">382</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">379 │   │   │   </span>do = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.iter(retry_state=retry_state)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">380 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(do, DoAttempt):                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">381 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>382 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>result = fn(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">383 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">BaseException</span>:  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># noqa: B902</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">384 │   │   │   │   │   </span>retry_state.set_exception(sys.exc_info())  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[arg-type]</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">385 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">openai_utils.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">121</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_completion_with_retry</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@retry_decorator</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_completion_with_retry</span>(**kwargs: Any) -&gt; Any:                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 │   │   </span>client = get_completion_endpoint(is_chat_model)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>121 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> client.create(**kwargs)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _completion_with_retry(**kwargs)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/api_resources/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">completio</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">n.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">25</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">create</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>25 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().create(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> TryAgain <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> timeout <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> time.time() &gt; start + timeout:                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/api_resources/abstract/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">engine_api_resource.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">149</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">create</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 │   │   │   </span>requestor,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147 │   │   │   </span>url,                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148 │   │   │   </span>params,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>149 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>) = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.__prepare_create_request(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 │   │   │   </span>api_key, api_base, api_type, api_version, organization, **params               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/api_resources/abstract/</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">engine_api_resource.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">106</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__prepare_create_request</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">103 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> timeout == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 │   │   │   </span>params[<span style=\"color: #808000; text-decoration-color: #808000\">\"timeout\"</span>] = MAX_TIMEOUT                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>106 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>requestor = api_requestor.APIRequestor(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107 │   │   │   </span>api_key,                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 │   │   │   </span>api_base=api_base,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109 │   │   │   </span>api_type=api_type,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">api_requestor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">138</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">135 │   │   </span>organization=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">136 │   </span>):                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.api_base = api_base <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> openai.api_base                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>138 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.api_key = key <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> util.default_api_key()                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.api_type = (                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">140 │   │   │   </span>ApiType.from_str(api_type)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> api_type                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">util.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">186</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">default_api_key</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> openai.api_key <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> openai.api_key                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">185 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>186 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> openai.error.AuthenticationError(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"No API key provided. You can set your API key in code using 'openai.api_key</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AuthenticationError: </span>No API key provided. You can set your API key in code using <span style=\"color: #008000; text-decoration-color: #008000\">'openai.api_key = &lt;API-KEY&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, or </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you can set the environment variable </span><span style=\"color: #808000; text-decoration-color: #808000\">OPENAI_API_KEY</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;API-KEY&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. If your API key is stored in a file, you can point </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the openai module at it with </span><span style=\"color: #008000; text-decoration-color: #008000\">'openai.api_key_path = &lt;PATH&gt;'</span>. You can generate API keys in the OpenAI web interface.\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://platform.openai.com/account/api-keys</span> for details.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_6887/\u001b[0m\u001b[1;33m2184414091.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_6887/2184414091.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/query/\u001b[0m\u001b[1;33mbase\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92mquery\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.callback_manager.as_trace(\u001b[33m\"\u001b[0m\u001b[33mquery\u001b[0m\u001b[33m\"\u001b[0m):                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(str_or_query_bundle, \u001b[96mstr\u001b[0m):                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mstr_or_query_bundle = QueryBundle(str_or_query_bundle)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 \u001b[2m│   │   │   \u001b[0mresponse = \u001b[96mself\u001b[0m._query(str_or_query_bundle)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m response                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m \u001b[92maquery\u001b[0m(\u001b[96mself\u001b[0m, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/query_engine/\u001b[0m\u001b[1;33mretri\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mever_query_engine.py\u001b[0m:\u001b[94m147\u001b[0m in \u001b[92m_query\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m144 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m145 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2m│   │   \u001b[0mretrieve_id = \u001b[96mself\u001b[0m.callback_manager.on_event_start(CBEventType.RETRIEVE)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m147 \u001b[2m│   │   \u001b[0mnodes = \u001b[96mself\u001b[0m.retrieve(query_bundle)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m148 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.callback_manager.on_event_end(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m149 \u001b[0m\u001b[2m│   │   │   \u001b[0mCBEventType.RETRIEVE,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   │   \u001b[0mpayload={EventPayload.NODES: nodes},                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/query_engine/\u001b[0m\u001b[1;33mretri\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mever_query_engine.py\u001b[0m:\u001b[94m107\u001b[0m in \u001b[92mretrieve\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mretrieve\u001b[0m(\u001b[96mself\u001b[0m, query_bundle: QueryBundle) -> List[NodeWithScore]:                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m107 \u001b[2m│   │   \u001b[0mnodes = \u001b[96mself\u001b[0m._retriever.retrieve(query_bundle)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m node_postprocessor \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._node_postprocessors:                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   │   │   \u001b[0mnodes = node_postprocessor.postprocess_nodes(                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/\u001b[0m\u001b[1;33mbase_retri\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mever.py\u001b[0m:\u001b[94m21\u001b[0m in \u001b[92mretrieve\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(str_or_query_bundle, \u001b[96mstr\u001b[0m):                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   │   \u001b[0mstr_or_query_bundle = QueryBundle(str_or_query_bundle)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m21 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._retrieve(str_or_query_bundle)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@abstractmethod\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_retrieve\u001b[0m(\u001b[96mself\u001b[0m, query_bundle: QueryBundle) -> List[NodeWithScore]:                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/keyword_ta\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mble/\u001b[0m\u001b[1;33mretrievers.py\u001b[0m:\u001b[94m81\u001b[0m in \u001b[92m_retrieve\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 78 \u001b[0m\u001b[2m│   \u001b[0m) -> List[NodeWithScore]:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Get nodes for response.\"\"\"\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.info(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m> Starting query: \u001b[0m\u001b[33m{\u001b[0mquery_bundle.query_str\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 81 \u001b[2m│   │   \u001b[0mkeywords = \u001b[96mself\u001b[0m._get_keywords(query_bundle.query_str)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 82 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.info(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mquery keywords: \u001b[0m\u001b[33m{\u001b[0mkeywords\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 83 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 84 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# go through text chunks in order of most matching keywords\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/indices/keyword_ta\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mble/\u001b[0m\u001b[1;33mretrievers.py\u001b[0m:\u001b[94m121\u001b[0m in \u001b[92m_get_keywords\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_get_keywords\u001b[0m(\u001b[96mself\u001b[0m, query_str: \u001b[96mstr\u001b[0m) -> List[\u001b[96mstr\u001b[0m]:                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Extract keywords.\"\"\"\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m121 \u001b[2m│   │   \u001b[0mresponse = \u001b[96mself\u001b[0m._service_context.llm_predictor.predict(                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.query_keyword_extract_template,                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m│   │   │   \u001b[0mmax_keywords=\u001b[96mself\u001b[0m.max_keywords_per_query,                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   │   │   \u001b[0mquestion=query_str,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llm_predictor/\u001b[0m\u001b[1;33mbase\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m123\u001b[0m in \u001b[92mpredict\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0mformatted_prompt = messages_to_prompt(messages)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   \u001b[0mformatted_prompt = prompt.format(llm=\u001b[96mself\u001b[0m._llm, **prompt_args)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m123 \u001b[2m│   │   │   \u001b[0mresponse = \u001b[96mself\u001b[0m._llm.complete(formatted_prompt)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput = response.text                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.debug(output)                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/\u001b[0m\u001b[1;33mopenai.py\u001b[0m:\u001b[94m72\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mcomplete\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 69 \u001b[0m\u001b[2m│   │   │   \u001b[0mcomplete_fn = chat_to_completion_decorator(\u001b[96mself\u001b[0m._chat)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2m│   │   │   \u001b[0mcomplete_fn = \u001b[96mself\u001b[0m._complete                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 72 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m complete_fn(prompt, **kwargs)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 73 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 74 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mstream_complete\u001b[0m(\u001b[96mself\u001b[0m, prompt: \u001b[96mstr\u001b[0m, **kwargs: Any) -> CompletionResponseGen:        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 75 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._is_chat_model:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/\u001b[0m\u001b[1;33mopenai.py\u001b[0m:\u001b[94m183\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_complete\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   │   │   \u001b[0mmax_tokens = \u001b[96mself\u001b[0m._get_max_token_for_prompt(prompt)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m│   │   │   \u001b[0mall_kwargs[\u001b[33m\"\u001b[0m\u001b[33mmax_tokens\u001b[0m\u001b[33m\"\u001b[0m] = max_tokens                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m182 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m183 \u001b[2m│   │   \u001b[0mresponse = completion_with_retry(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   │   \u001b[0mis_chat_model=\u001b[96mself\u001b[0m._is_chat_model,                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   │   \u001b[0mmax_retries=\u001b[96mself\u001b[0m.max_retries,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m186 \u001b[0m\u001b[2m│   │   │   \u001b[0mprompt=prompt,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/\u001b[0m\u001b[1;33mopenai_utils.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpy\u001b[0m:\u001b[94m123\u001b[0m in \u001b[92mcompletion_with_retry\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   \u001b[0mclient = get_completion_endpoint(is_chat_model)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m client.create(**kwargs)                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m123 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _completion_with_retry(**kwargs)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m \u001b[92macompletion_with_retry\u001b[0m(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tenacity/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m289\u001b[0m in    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mwrapped_f\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m287 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(f)                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mwrapped_f\u001b[0m(*args: t.Any, **kw: t.Any) -> t.Any:                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m289 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m(f, *args, **kw)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mretry_with\u001b[0m(*args: t.Any, **kwargs: t.Any) -> WrappedFn:                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.copy(*args, **kwargs).wraps(f)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tenacity/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m379\u001b[0m in    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__call__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m376 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m377 \u001b[0m\u001b[2m│   │   \u001b[0mretry_state = RetryCallState(retry_object=\u001b[96mself\u001b[0m, fn=fn, args=args, kwargs=kwargs)   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m378 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m379 \u001b[2m│   │   │   \u001b[0mdo = \u001b[96mself\u001b[0m.iter(retry_state=retry_state)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m380 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(do, DoAttempt):                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m381 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m382 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mresult = fn(*args, **kwargs)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tenacity/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m314\u001b[0m in    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92miter\u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m311 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m312 \u001b[0m\u001b[2m│   │   \u001b[0mis_explicit_retry = fut.failed \u001b[95mand\u001b[0m \u001b[96misinstance\u001b[0m(fut.exception(), TryAgain)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (is_explicit_retry \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.retry(retry_state)):                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m314 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m fut.result()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.after \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.after(retry_state)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/concurrent/futures/\u001b[0m\u001b[1;33m_base.py\u001b[0m:\u001b[94m439\u001b[0m in \u001b[92mresult\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m436 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._state \u001b[95min\u001b[0m [CANCELLED, CANCELLED_AND_NOTIFIED]:                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m437 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m CancelledError()                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m438 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m._state == FINISHED:                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m439 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.__get_result()                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m440 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m441 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._condition.wait(timeout)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m442 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/concurrent/futures/\u001b[0m\u001b[1;33m_base.py\u001b[0m:\u001b[94m391\u001b[0m in           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__get_result\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m388 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__get_result\u001b[0m(\u001b[96mself\u001b[0m):                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m389 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._exception:                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m390 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m391 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mself\u001b[0m._exception                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m392 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m393 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Break a reference cycle with the exception in self._exception\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m394 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m = \u001b[94mNone\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/tenacity/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m382\u001b[0m in    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__call__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m379 \u001b[0m\u001b[2m│   │   │   \u001b[0mdo = \u001b[96mself\u001b[0m.iter(retry_state=retry_state)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m380 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(do, DoAttempt):                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m381 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m382 \u001b[2m│   │   │   │   │   \u001b[0mresult = fn(*args, **kwargs)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m383 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mBaseException\u001b[0m:  \u001b[2m# noqa: B902\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m384 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mretry_state.set_exception(sys.exc_info())  \u001b[2m# type: ignore[arg-type]\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m385 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_index/llms/\u001b[0m\u001b[1;33mopenai_utils.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpy\u001b[0m:\u001b[94m121\u001b[0m in \u001b[92m_completion_with_retry\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@retry_decorator\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_completion_with_retry\u001b[0m(**kwargs: Any) -> Any:                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   \u001b[0mclient = get_completion_endpoint(is_chat_model)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m121 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m client.create(**kwargs)                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _completion_with_retry(**kwargs)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/api_resources/\u001b[0m\u001b[1;33mcompletio\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mn.py\u001b[0m:\u001b[94m25\u001b[0m in \u001b[92mcreate\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m25 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().create(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m TryAgain \u001b[94mas\u001b[0m e:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m timeout \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m time.time() > start + timeout:                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/api_resources/abstract/\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mengine_api_resource.py\u001b[0m:\u001b[94m149\u001b[0m in \u001b[92mcreate\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2m│   │   │   \u001b[0mrequestor,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m147 \u001b[0m\u001b[2m│   │   │   \u001b[0murl,                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m148 \u001b[0m\u001b[2m│   │   │   \u001b[0mparams,                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m149 \u001b[2m│   │   \u001b[0m) = \u001b[96mcls\u001b[0m.__prepare_create_request(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   │   \u001b[0mapi_key, api_base, api_type, api_version, organization, **params               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/api_resources/abstract/\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mengine_api_resource.py\u001b[0m:\u001b[94m106\u001b[0m in \u001b[92m__prepare_create_request\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m timeout == \u001b[94m0\u001b[0m:                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   │   \u001b[0mparams[\u001b[33m\"\u001b[0m\u001b[33mtimeout\u001b[0m\u001b[33m\"\u001b[0m] = MAX_TIMEOUT                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m106 \u001b[2m│   │   \u001b[0mrequestor = api_requestor.APIRequestor(                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   │   \u001b[0mapi_key,                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   │   \u001b[0mapi_base=api_base,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   │   │   \u001b[0mapi_type=api_type,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/\u001b[0m\u001b[1;33mapi_requestor.py\u001b[0m:\u001b[94m138\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__init__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   │   \u001b[0morganization=\u001b[94mNone\u001b[0m,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[2m│   \u001b[0m):                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.api_base = api_base \u001b[95mor\u001b[0m openai.api_base                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m138 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.api_key = key \u001b[95mor\u001b[0m util.default_api_key()                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.api_type = (                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m│   │   │   \u001b[0mApiType.from_str(api_type)                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m api_type                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/openai/\u001b[0m\u001b[1;33mutil.py\u001b[0m:\u001b[94m186\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mdefault_api_key\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m openai.api_key \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m openai.api_key                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m186 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m openai.error.AuthenticationError(                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mNo API key provided. You can set your API key in code using \u001b[0m\u001b[33m'\u001b[0m\u001b[33mopenai.api_key\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAuthenticationError: \u001b[0mNo API key provided. You can set your API key in code using \u001b[32m'openai.api_key = \u001b[0m\u001b[32m<\u001b[0m\u001b[32mAPI-KEY\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, or \u001b[0m\n",
       "\u001b[39myou can set the environment variable \u001b[0m\u001b[33mOPENAI_API_KEY\u001b[0m\u001b[39m=<API-KEY>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. If your API key is stored in a file, you can point \u001b[0m\n",
       "\u001b[39mthe openai module at it with \u001b[0m\u001b[32m'openai.api_key_path = <PATH\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m. You can generate API keys in the OpenAI web interface.\n",
       "See \u001b[4;94mhttps://platform.openai.com/account/api-keys\u001b[0m for details.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = keyword_table_index.as_query_engine(service_context = service_context_llama)\n",
    "response = query_engine.query(\"What is a summary of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import GraphIndexCreator\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state_of_the_union.txt\") as f:\n",
    "    all_text = f.read()\n",
    "text = \"\\n\".join(all_text.split(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(docs_path):\n",
    "    loader = DirectoryLoader(docs_path, glob=\"**/*.html\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_docs('omnisciencom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_10772/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2159886000.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_10772/2159886000.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>object of type <span style=\"color: #008000; text-decoration-color: #008000\">'Document'</span> has no <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">len</span><span style=\"font-weight: bold\">()</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_10772/\u001b[0m\u001b[1;33m2159886000.py\u001b[0m:\u001b[94m8\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_10772/2159886000.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mobject of type \u001b[32m'Document'\u001b[0m has no \u001b[1;35mlen\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating a new empty list for storing result\n",
    "resultList = []\n",
    "\n",
    "# Traversing in till the length of the input list of lists\n",
    "for m in range(len(documents)):\n",
    "\n",
    "   # using nested for loop, traversing the inner lists\n",
    "   for n in range (len(documents[m])):\n",
    "\n",
    "      # Add each element to the result list\n",
    "      resultList.append(documents[m][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(join_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"google/flan-t5-xxl\" \n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\":1024}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_creator = GraphIndexCreator(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38181"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_10772/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">857842783.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_10772/857842783.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/indexes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">graph.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">29</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_text</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"llm should not be None\"</span>)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 │   │   </span>graph = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.graph_type()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 │   │   </span>chain = LLMChain(llm=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.llm, prompt=prompt)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>29 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = chain.predict(text=text)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 │   │   </span>knowledge = parse_triples(output)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> triple <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> knowledge:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 │   │   │   </span>graph.add_triple(triple)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">252</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">249 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">250 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">completion = llm.predict(adjective=\"funny\")</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">251 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>252 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(kwargs, callbacks=callbacks)[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.output_key]                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">253 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">254 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">async</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apredict</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, callbacks: Callbacks = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, **kwargs: Any) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">255 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Format prompt with kwargs and pass to LLM.</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">243</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">241 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">242 │   │   │   </span>run_manager.on_chain_error(e)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>243 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> e                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">244 │   │   </span>run_manager.on_chain_end(outputs)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">245 │   │   </span>final_outputs: Dict[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, Any] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prep_outputs(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">246 │   │   │   </span>inputs, outputs, return_only_outputs                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">237</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">234 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">235 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">236 │   │   │   </span>outputs = (                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>237 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(inputs, run_manager=run_manager)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">238 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> new_arg_supported                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">239 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(inputs)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">92</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call</span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 89 │   │   </span>inputs: Dict[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, Any],                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 │   │   </span>run_manager: Optional[CallbackManagerForChainRun] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 │   </span>) -&gt; Dict[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>]:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 92 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.generate([inputs], run_manager=run_manager)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.create_outputs(response)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">102</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 │   </span>) -&gt; LLMResult:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Generate LLM result from inputs.\"\"\"</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">101 │   │   </span>prompts, stop = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prep_prompts(input_list, run_manager=run_manager)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.llm.generate_prompt(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">103 │   │   │   </span>prompts,                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 │   │   │   </span>stop,                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 │   │   │   </span>callbacks=run_manager.get_child() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> run_manager <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">186</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_prompt</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183 │   │   </span>**kwargs: Any,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 │   </span>) -&gt; LLMResult:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">185 │   │   </span>prompt_strings = [p.to_string() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> p <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> prompts]                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>186 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">async</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">agenerate_prompt</span>(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">279</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">276 │   │   │   </span>run_managers = callback_manager.on_llm_start(                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 │   │   │   │   </span>dumpd(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>), prompts, invocation_params=params, options=options            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>279 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._generate_helper(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">280 │   │   │   │   </span>prompts, stop, run_managers, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>(new_arg_supported), **kwargs             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> output                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">223</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate_helper</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> run_manager <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> run_managers:                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">222 │   │   │   │   </span>run_manager.on_llm_error(e)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>223 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> e                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">224 │   │   </span>flattened_outputs = output.flatten()                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">225 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> manager, flattened_output <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(run_managers, flattened_outputs):             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">226 │   │   │   </span>manager.on_llm_end(flattened_output)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">210</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate_helper</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">207 │   </span>) -&gt; LLMResult:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">208 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">209 │   │   │   </span>output = (                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>210 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._generate(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">211 │   │   │   │   │   </span>prompts,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">212 │   │   │   │   │   </span>stop=stop,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213 │   │   │   │   │   # TODO: support multiple run managers</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">602</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">599 │   │   </span>new_arg_supported = inspect.signature(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call).parameters.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"run_manager\"</span>)    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">600 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> prompt <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> prompts:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">601 │   │   │   </span>text = (                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>602 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(prompt, stop=stop, run_manager=run_manager, **kwargs)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">603 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> new_arg_supported                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">604 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(prompt, stop=stop, **kwargs)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">605 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">huggingface_hub</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   </span>params = {**_model_kwargs, **kwargs}                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.client(inputs=prompt, params=params)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"error\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> response:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Error raised by inference API: {</span>response[<span style=\"color: #808000; text-decoration-color: #808000\">'error'</span>]<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.client.task == <span style=\"color: #808000; text-decoration-color: #808000\">\"text-generation\"</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   │   │   # Text generation return includes the starter text.</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   │   │   </span>text = response[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #808000; text-decoration-color: #808000\">\"generated_text\"</span>][<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(prompt) :]                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Error raised by inference API: Input validation error: `inputs` tokens + `max_new_tokens` must be &lt;= \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1512</span>. Given: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8850</span> `inputs` tokens and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> `max_new_tokens`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_10772/\u001b[0m\u001b[1;33m857842783.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_10772/857842783.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/indexes/\u001b[0m\u001b[1;33mgraph.py\u001b[0m:\u001b[94m29\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mfrom_text\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mllm should not be None\u001b[0m\u001b[33m\"\u001b[0m)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   │   \u001b[0mgraph = \u001b[96mself\u001b[0m.graph_type()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m│   │   \u001b[0mchain = LLMChain(llm=\u001b[96mself\u001b[0m.llm, prompt=prompt)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m29 \u001b[2m│   │   \u001b[0moutput = chain.predict(text=text)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   │   \u001b[0mknowledge = parse_triples(output)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m triple \u001b[95min\u001b[0m knowledge:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m│   │   │   \u001b[0mgraph.add_triple(triple)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mllm.py\u001b[0m:\u001b[94m252\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mpredict\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m250 \u001b[0m\u001b[2;33m│   │   │   │   \u001b[0m\u001b[33mcompletion = llm.predict(adjective=\"funny\")\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m251 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m252 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m(kwargs, callbacks=callbacks)[\u001b[96mself\u001b[0m.output_key]                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m \u001b[92mapredict\u001b[0m(\u001b[96mself\u001b[0m, callbacks: Callbacks = \u001b[94mNone\u001b[0m, **kwargs: Any) -> \u001b[96mstr\u001b[0m:           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m255 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m243\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__call__\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m240 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m241 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m (\u001b[96mKeyboardInterrupt\u001b[0m, \u001b[96mException\u001b[0m) \u001b[94mas\u001b[0m e:                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m242 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_manager.on_chain_error(e)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m243 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m e                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m244 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager.on_chain_end(outputs)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m245 \u001b[0m\u001b[2m│   │   \u001b[0mfinal_outputs: Dict[\u001b[96mstr\u001b[0m, Any] = \u001b[96mself\u001b[0m.prep_outputs(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs, outputs, return_only_outputs                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m237\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__call__\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m234 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m235 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m236 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = (                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m237 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._call(inputs, run_manager=run_manager)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m238 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m new_arg_supported                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m239 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[96mself\u001b[0m._call(inputs)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m240 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mllm.py\u001b[0m:\u001b[94m92\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call\u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   │   \u001b[0minputs: Dict[\u001b[96mstr\u001b[0m, Any],                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager: Optional[CallbackManagerForChainRun] = \u001b[94mNone\u001b[0m,                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   \u001b[0m) -> Dict[\u001b[96mstr\u001b[0m, \u001b[96mstr\u001b[0m]:                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 92 \u001b[2m│   │   \u001b[0mresponse = \u001b[96mself\u001b[0m.generate([inputs], run_manager=run_manager)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.create_outputs(response)[\u001b[94m0\u001b[0m]                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mgenerate\u001b[0m(                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mllm.py\u001b[0m:\u001b[94m102\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mgenerate\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   \u001b[0m) -> LLMResult:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   \u001b[0mprompts, stop = \u001b[96mself\u001b[0m.prep_prompts(input_list, run_manager=run_manager)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m102 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.llm.generate_prompt(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   │   │   \u001b[0mprompts,                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   │   \u001b[0mstop,                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   │   \u001b[0mcallbacks=run_manager.get_child() \u001b[94mif\u001b[0m run_manager \u001b[94melse\u001b[0m \u001b[94mNone\u001b[0m,                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m186\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mgenerate_prompt\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs: Any,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   \u001b[0m) -> LLMResult:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   \u001b[0mprompt_strings = [p.to_string() \u001b[94mfor\u001b[0m p \u001b[95min\u001b[0m prompts]                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m186 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m \u001b[92magenerate_prompt\u001b[0m(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m279\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mgenerate\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m276 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_managers = callback_manager.on_llm_start(                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdumpd(\u001b[96mself\u001b[0m), prompts, invocation_params=params, options=options            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m279 \u001b[2m│   │   │   \u001b[0moutput = \u001b[96mself\u001b[0m._generate_helper(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m280 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mprompts, stop, run_managers, \u001b[96mbool\u001b[0m(new_arg_supported), **kwargs             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m output                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m223\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate_helper\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m (\u001b[96mKeyboardInterrupt\u001b[0m, \u001b[96mException\u001b[0m) \u001b[94mas\u001b[0m e:                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m run_manager \u001b[95min\u001b[0m run_managers:                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mrun_manager.on_llm_error(e)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m223 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m e                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m│   │   \u001b[0mflattened_outputs = output.flatten()                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m manager, flattened_output \u001b[95min\u001b[0m \u001b[96mzip\u001b[0m(run_managers, flattened_outputs):             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m│   │   │   \u001b[0mmanager.on_llm_end(flattened_output)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m210\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate_helper\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m207 \u001b[0m\u001b[2m│   \u001b[0m) -> LLMResult:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m208 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m209 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput = (                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m210 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._generate(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m211 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mprompts,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m212 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mstop=stop,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m213 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# TODO: support multiple run managers\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m602\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m599 \u001b[0m\u001b[2m│   │   \u001b[0mnew_arg_supported = inspect.signature(\u001b[96mself\u001b[0m._call).parameters.get(\u001b[33m\"\u001b[0m\u001b[33mrun_manager\u001b[0m\u001b[33m\"\u001b[0m)    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m600 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m prompt \u001b[95min\u001b[0m prompts:                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m601 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext = (                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m602 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._call(prompt, stop=stop, run_manager=run_manager, **kwargs)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m603 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m new_arg_supported                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m604 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[96mself\u001b[0m._call(prompt, stop=stop, **kwargs)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m605 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mhuggingface_hub\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92m_call\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0mparams = {**_model_kwargs, **kwargs}                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   \u001b[0mresponse = \u001b[96mself\u001b[0m.client(inputs=prompt, params=params)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33merror\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m response:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mError raised by inference API: \u001b[0m\u001b[33m{\u001b[0mresponse[\u001b[33m'\u001b[0m\u001b[33merror\u001b[0m\u001b[33m'\u001b[0m]\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.client.task == \u001b[33m\"\u001b[0m\u001b[33mtext-generation\u001b[0m\u001b[33m\"\u001b[0m:                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Text generation return includes the starter text.\u001b[0m                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext = response[\u001b[94m0\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mgenerated_text\u001b[0m\u001b[33m\"\u001b[0m][\u001b[96mlen\u001b[0m(prompt) :]                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mError raised by inference API: Input validation error: `inputs` tokens + `max_new_tokens` must be <= \n",
       "\u001b[1;36m1512\u001b[0m. Given: \u001b[1;36m8850\u001b[0m `inputs` tokens and \u001b[1;36m20\u001b[0m `max_new_tokens`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = index_creator.from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/sira/sira_project/llama_index/orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.06 MB\n",
      "llama_model_load_internal: mem required  = 2862.72 MB (+  682.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  162.50 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import (\n",
    "    StreamingStdOutCallbackHandler,\n",
    ") \n",
    "model_path = \"/home/sira/sira_project/llama_index/orca-mini-3b.ggmlv3.q4_0.bin\" #check model path \n",
    "def create_model(model_path):\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(model_path=model_path, callback_manager=callback_manager, verbose=True)\n",
    "    return llm \n",
    "llm = create_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_tokenize: too many tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_10772/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1052672732.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_10772/1052672732.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/indexes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">graph.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">29</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_text</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"llm should not be None\"</span>)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 │   │   </span>graph = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.graph_type()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 │   │   </span>chain = LLMChain(llm=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.llm, prompt=prompt)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>29 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = chain.predict(text=text)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 │   │   </span>knowledge = parse_triples(output)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> triple <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> knowledge:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 │   │   │   </span>graph.add_triple(triple)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">252</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">249 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">250 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">completion = llm.predict(adjective=\"funny\")</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">251 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>252 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(kwargs, callbacks=callbacks)[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.output_key]                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">253 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">254 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">async</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apredict</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, callbacks: Callbacks = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, **kwargs: Any) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">255 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Format prompt with kwargs and pass to LLM.</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">243</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">241 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">242 │   │   │   </span>run_manager.on_chain_error(e)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>243 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> e                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">244 │   │   </span>run_manager.on_chain_end(outputs)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">245 │   │   </span>final_outputs: Dict[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, Any] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prep_outputs(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">246 │   │   │   </span>inputs, outputs, return_only_outputs                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">237</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">234 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">235 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">236 │   │   │   </span>outputs = (                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>237 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(inputs, run_manager=run_manager)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">238 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> new_arg_supported                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">239 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(inputs)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">92</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call</span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 89 │   │   </span>inputs: Dict[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, Any],                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 │   │   </span>run_manager: Optional[CallbackManagerForChainRun] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 │   </span>) -&gt; Dict[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>]:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 92 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.generate([inputs], run_manager=run_manager)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.create_outputs(response)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">102</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 │   </span>) -&gt; LLMResult:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Generate LLM result from inputs.\"\"\"</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">101 │   │   </span>prompts, stop = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prep_prompts(input_list, run_manager=run_manager)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.llm.generate_prompt(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">103 │   │   │   </span>prompts,                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 │   │   │   </span>stop,                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 │   │   │   </span>callbacks=run_manager.get_child() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> run_manager <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">186</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_prompt</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183 │   │   </span>**kwargs: Any,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 │   </span>) -&gt; LLMResult:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">185 │   │   </span>prompt_strings = [p.to_string() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> p <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> prompts]                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>186 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">async</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">agenerate_prompt</span>(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">279</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">276 │   │   │   </span>run_managers = callback_manager.on_llm_start(                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 │   │   │   │   </span>dumpd(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>), prompts, invocation_params=params, options=options            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>279 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._generate_helper(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">280 │   │   │   │   </span>prompts, stop, run_managers, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>(new_arg_supported), **kwargs             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> output                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">223</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate_helper</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> run_manager <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> run_managers:                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">222 │   │   │   │   </span>run_manager.on_llm_error(e)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>223 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> e                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">224 │   │   </span>flattened_outputs = output.flatten()                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">225 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> manager, flattened_output <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(run_managers, flattened_outputs):             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">226 │   │   │   </span>manager.on_llm_end(flattened_output)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">210</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate_helper</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">207 │   </span>) -&gt; LLMResult:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">208 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">209 │   │   │   </span>output = (                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>210 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._generate(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">211 │   │   │   │   │   </span>prompts,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">212 │   │   │   │   │   </span>stop=stop,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213 │   │   │   │   │   # TODO: support multiple run managers</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">602</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_generate</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">599 │   │   </span>new_arg_supported = inspect.signature(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call).parameters.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"run_manager\"</span>)    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">600 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> prompt <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> prompts:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">601 │   │   │   </span>text = (                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>602 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(prompt, stop=stop, run_manager=run_manager, **kwargs)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">603 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> new_arg_supported                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">604 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call(prompt, stop=stop, **kwargs)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">605 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llamacpp.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">230</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">227 │   │   │   # method that yields as they are generated</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">228 │   │   │   # and return the combined strings from the first choices's text:</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229 │   │   │   </span>combined_text_output = <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>230 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> token <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.stream(prompt=prompt, stop=stop, run_manager=run_manager):   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">231 │   │   │   │   </span>combined_text_output += token[<span style=\"color: #808000; text-decoration-color: #808000\">\"choices\"</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span>]                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">232 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> combined_text_output                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">233 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llamacpp.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">280</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">stream</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 │   │   </span>params = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._get_parameters(stop)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279 │   │   </span>result = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.client(prompt=prompt, stream=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, **params)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>280 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> chunk <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> result:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 │   │   │   </span>token = chunk[<span style=\"color: #808000; text-decoration-color: #808000\">\"choices\"</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #808000; text-decoration-color: #808000\">\"text\"</span>]                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 │   │   │   </span>log_probs = chunk[<span style=\"color: #808000; text-decoration-color: #808000\">\"choices\"</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>].get(<span style=\"color: #808000; text-decoration-color: #808000\">\"logprobs\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">283 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> run_manager:                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_cpp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">828</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_create_completion</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 825 │   │   │   </span>llama_cpp.llama_reset_timings(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ctx)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 826 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 827 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(prompt_tokens) &gt;= llama_cpp.llama_n_ctx(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ctx):                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 828 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 829 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Requested tokens exceed context window of {</span>llama_cpp.llama_n_ctx(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 830 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 831 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Requested tokens exceed context window of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_10772/\u001b[0m\u001b[1;33m1052672732.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_10772/1052672732.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/indexes/\u001b[0m\u001b[1;33mgraph.py\u001b[0m:\u001b[94m29\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mfrom_text\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mllm should not be None\u001b[0m\u001b[33m\"\u001b[0m)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   │   \u001b[0mgraph = \u001b[96mself\u001b[0m.graph_type()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m│   │   \u001b[0mchain = LLMChain(llm=\u001b[96mself\u001b[0m.llm, prompt=prompt)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m29 \u001b[2m│   │   \u001b[0moutput = chain.predict(text=text)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   │   \u001b[0mknowledge = parse_triples(output)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m triple \u001b[95min\u001b[0m knowledge:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m│   │   │   \u001b[0mgraph.add_triple(triple)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mllm.py\u001b[0m:\u001b[94m252\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mpredict\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m250 \u001b[0m\u001b[2;33m│   │   │   │   \u001b[0m\u001b[33mcompletion = llm.predict(adjective=\"funny\")\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m251 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m252 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m(kwargs, callbacks=callbacks)[\u001b[96mself\u001b[0m.output_key]                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m \u001b[92mapredict\u001b[0m(\u001b[96mself\u001b[0m, callbacks: Callbacks = \u001b[94mNone\u001b[0m, **kwargs: Any) -> \u001b[96mstr\u001b[0m:           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m255 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m243\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__call__\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m240 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m241 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m (\u001b[96mKeyboardInterrupt\u001b[0m, \u001b[96mException\u001b[0m) \u001b[94mas\u001b[0m e:                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m242 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_manager.on_chain_error(e)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m243 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m e                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m244 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager.on_chain_end(outputs)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m245 \u001b[0m\u001b[2m│   │   \u001b[0mfinal_outputs: Dict[\u001b[96mstr\u001b[0m, Any] = \u001b[96mself\u001b[0m.prep_outputs(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs, outputs, return_only_outputs                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m237\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__call__\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m234 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m235 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m236 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = (                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m237 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._call(inputs, run_manager=run_manager)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m238 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m new_arg_supported                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m239 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[96mself\u001b[0m._call(inputs)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m240 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mllm.py\u001b[0m:\u001b[94m92\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call\u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   │   \u001b[0minputs: Dict[\u001b[96mstr\u001b[0m, Any],                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager: Optional[CallbackManagerForChainRun] = \u001b[94mNone\u001b[0m,                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   \u001b[0m) -> Dict[\u001b[96mstr\u001b[0m, \u001b[96mstr\u001b[0m]:                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 92 \u001b[2m│   │   \u001b[0mresponse = \u001b[96mself\u001b[0m.generate([inputs], run_manager=run_manager)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.create_outputs(response)[\u001b[94m0\u001b[0m]                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mgenerate\u001b[0m(                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/chains/\u001b[0m\u001b[1;33mllm.py\u001b[0m:\u001b[94m102\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mgenerate\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   \u001b[0m) -> LLMResult:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   \u001b[0mprompts, stop = \u001b[96mself\u001b[0m.prep_prompts(input_list, run_manager=run_manager)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m102 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.llm.generate_prompt(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   │   │   \u001b[0mprompts,                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   │   \u001b[0mstop,                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   │   \u001b[0mcallbacks=run_manager.get_child() \u001b[94mif\u001b[0m run_manager \u001b[94melse\u001b[0m \u001b[94mNone\u001b[0m,                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m186\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mgenerate_prompt\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs: Any,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   \u001b[0m) -> LLMResult:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   \u001b[0mprompt_strings = [p.to_string() \u001b[94mfor\u001b[0m p \u001b[95min\u001b[0m prompts]                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m186 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m \u001b[92magenerate_prompt\u001b[0m(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m279\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mgenerate\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m276 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_managers = callback_manager.on_llm_start(                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdumpd(\u001b[96mself\u001b[0m), prompts, invocation_params=params, options=options            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m279 \u001b[2m│   │   │   \u001b[0moutput = \u001b[96mself\u001b[0m._generate_helper(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m280 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mprompts, stop, run_managers, \u001b[96mbool\u001b[0m(new_arg_supported), **kwargs             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m output                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m223\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate_helper\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m (\u001b[96mKeyboardInterrupt\u001b[0m, \u001b[96mException\u001b[0m) \u001b[94mas\u001b[0m e:                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m run_manager \u001b[95min\u001b[0m run_managers:                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mrun_manager.on_llm_error(e)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m223 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m e                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m│   │   \u001b[0mflattened_outputs = output.flatten()                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m manager, flattened_output \u001b[95min\u001b[0m \u001b[96mzip\u001b[0m(run_managers, flattened_outputs):             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m│   │   │   \u001b[0mmanager.on_llm_end(flattened_output)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m210\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate_helper\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m207 \u001b[0m\u001b[2m│   \u001b[0m) -> LLMResult:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m208 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m209 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput = (                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m210 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._generate(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m211 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mprompts,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m212 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mstop=stop,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m213 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# TODO: support multiple run managers\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m602\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_generate\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m599 \u001b[0m\u001b[2m│   │   \u001b[0mnew_arg_supported = inspect.signature(\u001b[96mself\u001b[0m._call).parameters.get(\u001b[33m\"\u001b[0m\u001b[33mrun_manager\u001b[0m\u001b[33m\"\u001b[0m)    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m600 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m prompt \u001b[95min\u001b[0m prompts:                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m601 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext = (                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m602 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._call(prompt, stop=stop, run_manager=run_manager, **kwargs)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m603 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m new_arg_supported                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m604 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[96mself\u001b[0m._call(prompt, stop=stop, **kwargs)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m605 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mllamacpp.py\u001b[0m:\u001b[94m230\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m227 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# method that yields as they are generated\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m228 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# and return the combined strings from the first choices's text:\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m229 \u001b[0m\u001b[2m│   │   │   \u001b[0mcombined_text_output = \u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m230 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m token \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.stream(prompt=prompt, stop=stop, run_manager=run_manager):   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m231 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcombined_text_output += token[\u001b[33m\"\u001b[0m\u001b[33mchoices\u001b[0m\u001b[33m\"\u001b[0m][\u001b[94m0\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mtext\u001b[0m\u001b[33m\"\u001b[0m]                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m232 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m combined_text_output                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m233 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/langchain/llms/\u001b[0m\u001b[1;33mllamacpp.py\u001b[0m:\u001b[94m280\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mstream\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   \u001b[0mparams = \u001b[96mself\u001b[0m._get_parameters(stop)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m279 \u001b[0m\u001b[2m│   │   \u001b[0mresult = \u001b[96mself\u001b[0m.client(prompt=prompt, stream=\u001b[94mTrue\u001b[0m, **params)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m280 \u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m chunk \u001b[95min\u001b[0m result:                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m│   │   │   \u001b[0mtoken = chunk[\u001b[33m\"\u001b[0m\u001b[33mchoices\u001b[0m\u001b[33m\"\u001b[0m][\u001b[94m0\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mtext\u001b[0m\u001b[33m\"\u001b[0m]                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   │   │   \u001b[0mlog_probs = chunk[\u001b[33m\"\u001b[0m\u001b[33mchoices\u001b[0m\u001b[33m\"\u001b[0m][\u001b[94m0\u001b[0m].get(\u001b[33m\"\u001b[0m\u001b[33mlogprobs\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m283 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m run_manager:                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sira/anaconda3/envs/llama_index/lib/python3.9/site-packages/llama_cpp/\u001b[0m\u001b[1;33mllama.py\u001b[0m:\u001b[94m828\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_create_completion\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 825 \u001b[0m\u001b[2m│   │   │   \u001b[0mllama_cpp.llama_reset_timings(\u001b[96mself\u001b[0m.ctx)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 826 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 827 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(prompt_tokens) >= llama_cpp.llama_n_ctx(\u001b[96mself\u001b[0m.ctx):                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 828 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 829 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mRequested tokens exceed context window of \u001b[0m\u001b[33m{\u001b[0mllama_cpp.llama_n_ctx(\u001b[96mself\u001b[0m.  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 830 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 831 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mRequested tokens exceed context window of \u001b[1;36m512\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_creator = GraphIndexCreator(llm= llm)\n",
    "graph = index_creator.from_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_10772/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">922985037.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_10772/922985037.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Document'</span> object is not subscriptable\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_10772/\u001b[0m\u001b[1;33m922985037.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_10772/922985037.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[32m'Document'\u001b[0m object is not subscriptable\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents[0][\"page_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import (\n",
    "    StreamingStdOutCallbackHandler,\n",
    ")  # for streaming resposne\n",
    "from langchain.indexes import GraphIndexCreator\n",
    "from langchain.chains import GraphQAChain\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "model_path = \"/home/sira/sira_project/llama_index/orca-mini-3b.ggmlv3.q4_0.bin\" #check model path \n",
    "text = 'Microsoft Invests $10 Billion in ChatGPT Maker OpenAI.' #text or some document\n",
    "\n",
    "def from_url (urls): # if use URL\n",
    "    loader = UnstructuredURLLoader(urls=urls)\n",
    "    text = loader.load()\n",
    "    return text\n",
    "\n",
    "def create_model(model_path):\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(model_path=model_path, callback_manager=callback_manager, verbose=True)\n",
    "    return llm \n",
    "\n",
    "def create_graph(text, llm):\n",
    "    index_creator = GraphIndexCreator(llm= llm)\n",
    "    graph = index_creator.from_text(text)\n",
    "    return graph\n",
    "\n",
    "def chain_run(llm, graph):\n",
    "    chain = GraphQAChain.from_llm(llm, graph=graph, verbose=True)\n",
    "    return chain \n",
    "\n",
    "\n",
    "llm = create_model(model_path)\n",
    "graph = create_graph(text, llm)\n",
    "chain = chain_run(llm,graph)\n",
    "chain.run(\"Does Microsoft invest $100 Billion to OpenAI?\") # your answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
