{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "import time\n",
    "\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS \n",
    "from langchain.docstore import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_document(web_data):\n",
    "    docs = []\n",
    "    for data in web_data:\n",
    "        text = data[\"snippet\"]\n",
    "        metadata = {\"title\" : data[\"title\"], \"source\" : data[\"link\"]}\n",
    "        docs.append(Document(page_content = text, metadata = metadata))\n",
    "    return docs\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    sp_docs = text_splitter.split_documents(documents)\n",
    "    return sp_docs\n",
    "\n",
    "custom_prompt_template = \"\"\" Use the following pieces that data from google search of information to answer the user's question.\n",
    "If you don't know the answer, please just say that you don't know the answer, don't try to make up\n",
    "an answer. \n",
    "\n",
    "Context : {context}\n",
    "Question : {question}\n",
    "\n",
    "Only returns the helpful and reasonable answer below and nothing else.\n",
    "No need to return the question and don't return duplicate answer. Please don't show unhelpful answers.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt(custom_prompt_template):\n",
    "    prompt = PromptTemplate(template=custom_prompt_template, \n",
    "                            input_variables=['context','question'])\n",
    "    return prompt\n",
    "\n",
    "def load_llm():\n",
    "    n_gpu_layers = 32  # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,n_ctx = 4096, temperature = 0.1, max_tokens = 4096, \n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "def load_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = \"thenlper/gte-base\",\n",
    "                                       model_kwargs = {'device': 'cpu'})\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def check_duplicate(source_list):\n",
    "    res = []\n",
    "    for i in source_list:\n",
    "        if i not in res:\n",
    "            res.append(i)\n",
    "    return res\n",
    "\n",
    "def convert_to_website_format(urls):\n",
    "    convert_urls = []\n",
    "    for url in urls:\n",
    "        # Remove any '.html' at the end of the URL\n",
    "        url = re.sub(r'\\.html$', '', url)\n",
    "        # Check if the URL starts with 'www.' or 'http://'\n",
    "        if url.startswith(\"www.\"):\n",
    "            #url = 'https://' + url\n",
    "            url = \"https://\" + url[4:]\n",
    "        if '/index' in url:\n",
    "            url = url.split('/index')[0]\n",
    "        match = re.match(r'^([^ ]+)', url)\n",
    "        if match:\n",
    "            url = match.group(1)\n",
    "        convert_urls.append(url)\n",
    "    return convert_urls\n",
    "\n",
    "def regex_source(answer):\n",
    "    pattern = r\"'source': '(.*?)'\"\n",
    "    matchs = re.findall(pattern, str(answer))\n",
    "    convert_urls = convert_to_website_format(matchs)\n",
    "    res_urls = check_duplicate(source_list=convert_urls)\n",
    "    #res_urls = filter_similar_url(res_urls)\n",
    "    return res_urls\n",
    "\n",
    "def filter_search(db_similarity, diff_val):\n",
    "    filter_list = []\n",
    "    top_score = db_similarity[0][1]\n",
    "    for index, score in enumerate(db_similarity) :\n",
    "        if score[1] - top_score <= diff_val:\n",
    "              filter_list.append(score)\n",
    "    return filter_list  \n",
    "\n",
    "def qa_retrival(llm, db, qa_prompt):\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"query\", output_key=\"result\")\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm = llm,\n",
    "        chain_type = \"stuff\",\n",
    "        retriever = db.as_retriever(search_kwargs = {'k':5}), \n",
    "        return_source_documents = True,\n",
    "        chain_type_kwargs = {\"prompt\":qa_prompt},\n",
    "        memory = memory) \n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintRetrievalHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container):\n",
    "        self.container = container.expander(\"Context Retrieval\")\n",
    "\n",
    "    def on_retriever_start(self, query: str, **kwargs):\n",
    "        self.container.write(f\"**Question:** {query}\")\n",
    "\n",
    "    def on_retriever_end(self, documents, **kwargs):\n",
    "        # self.container.write(documents)\n",
    "        for idx, doc in enumerate(documents):\n",
    "            source = doc.metadata[\"source\"]\n",
    "            self.container.write(f\"**Results from {source}**\")\n",
    "            self.container.text(doc.page_content)\n",
    "\n",
    "\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container, initial_text=\"\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        self.container.info(self.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 7354.73 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAwuzcj5uOZHwPZUMDai2KjL1vKcJAe5Cs\"\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"b6aeed6a754c34354\"\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "llm = load_llm()\n",
    "embeddings_model = load_embeddings()  \n",
    "dimension = 768\n",
    "index = faiss.IndexFlatL2(dimension)  \n",
    "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n",
    "\n",
    "web_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm, \n",
    "    search=search,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, here are three Google search queries that are similar to the given question:\n",
      "1. Who is the current prime minister of Japan?\n",
      "2. Which country has the longest-serving prime minister in history?\n",
      "3. Who was the first female prime minister of Australia?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 33779.98 ms\n",
      "llama_print_timings:      sample time =    37.83 ms /    59 runs   (    0.64 ms per token,  1559.57 tokens per second)\n",
      "llama_print_timings: prompt eval time = 33779.92 ms /    92 tokens (  367.17 ms per token,     2.72 tokens per second)\n",
      "llama_print_timings:        eval time = 22321.21 ms /    58 runs   (  384.85 ms per token,     2.60 tokens per second)\n",
      "llama_print_timings:       total time = 56384.51 ms\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWho is the prime minister of Thailand\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m document \u001b[39m=\u001b[39m web_retriever\u001b[39m.\u001b[39;49mget_relevant_documents(question)\n\u001b[1;32m      4\u001b[0m \u001b[39m# sp_docs = split_docs(document)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# db = FAISS.from_documents(sp_docs, embeddings_model)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# qa_prompt = set_custom_prompt(custom_prompt_template)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# end = time.time()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# print(\"Respone time:\",int(end-start),\"sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/schema/retriever.py:181\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_end(\n\u001b[1;32m    184\u001b[0m         result,\n\u001b[1;32m    185\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    186\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/schema/retriever.py:174\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_other_args \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 174\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_relevant_documents(\n\u001b[1;32m    175\u001b[0m         query, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_relevant_documents(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/retrievers/web_research.py:202\u001b[0m, in \u001b[0;36mWebResearchRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    200\u001b[0m html2text \u001b[39m=\u001b[39m Html2TextTransformer()\n\u001b[1;32m    201\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mIndexing new urls...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 202\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    203\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(html2text\u001b[39m.\u001b[39mtransform_documents(docs))\n\u001b[1;32m    204\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_splitter\u001b[39m.\u001b[39msplit_documents(docs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/document_loaders/async_html.py:136\u001b[0m, in \u001b[0;36mAsyncHtmlLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load text from the url(s) in web_path.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     results \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetch_all(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweb_paths))\n\u001b[1;32m    137\u001b[0m     docs \u001b[39m=\u001b[39m []\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m i, text \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(results):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m coroutines\u001b[39m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39ma coroutine was expected, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "question = \"Who is the prime minister of Thailand\"\n",
    "document = web_retriever.get_relevant_documents(question)\n",
    "\n",
    "# sp_docs = split_docs(document)\n",
    "# db = FAISS.from_documents(sp_docs, embeddings_model)\n",
    "# qa_prompt = set_custom_prompt(custom_prompt_template)\n",
    "# qa_chain = qa_retrival(llm, db, qa_prompt)\n",
    "# start = time.time()\n",
    "\n",
    "# # response = qa_chain({'query': question})\n",
    "# # st.write(response[\"result\"])\n",
    "\n",
    "# # answer = st.empty()\n",
    "# # answer.info('`Answer:`\\n' + response[\"result\"])\n",
    "\n",
    "# # retrieval_streamer_cb = PrintRetrievalHandler(st.container())\n",
    "# stream_handler = StreamHandler(answer, initial_text=\"`Answer:`\\n\\n\")\n",
    "# # response = qa_chain({\"query\": question},callbacks=[retrieval_streamer_cb, stream_handler])\n",
    "# response = qa_chain({\"query\": question},callbacks=[stream_handler])\n",
    "# answer.info('`Answer:`\\n\\n' + response['result'])\n",
    "\n",
    "\n",
    "# urls = regex_source(response)\n",
    "# for count, url in enumerate(urls):\n",
    "#     print(str(count+1)+\":\", url)\n",
    "# end = time.time()\n",
    "# print(\"Respone time:\",int(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "import os\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore  \n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAwuzcj5uOZHwPZUMDai2KjL1vKcJAe5Cs\"\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"b6aeed6a754c34354\"\n",
    "\n",
    "\n",
    "def load_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = \"thenlper/gte-base\",\n",
    "                                       model_kwargs = {'device': 'cpu'})\n",
    "    return embeddings\n",
    "\n",
    "def load_llm():\n",
    "    n_gpu_layers = 32  # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,n_ctx = 4096, temperature = 0.1, max_tokens = 4096, \n",
    "    )\n",
    "    return llm\n",
    "\n",
    "embeddings_model = load_embeddings() \n",
    "vectorstore_public = FAISS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-lama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
