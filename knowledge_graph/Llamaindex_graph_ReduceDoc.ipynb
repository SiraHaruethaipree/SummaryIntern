{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import(\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader,\n",
    "    LangchainEmbedding,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    "    load_graph_from_storage,\n",
    "    LLMPredictor,\n",
    "    PromptHelper\n",
    "    )\n",
    "\n",
    "# upload model\n",
    "from llama_index.llms import LangChainLLM\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index import (KnowledgeGraphIndex)\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Generator, List, Optional, Type\n",
    "from pathlib import Path\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "class DirectorySearchSource():\n",
    "    def __init__(\n",
    "    self,\n",
    "    num_files_limit: Optional[int] = None,\n",
    "    exclude_hidden: bool = True,\n",
    "    required_exts: Optional[List[str]]  = None,\n",
    "    recursive : bool = True,):\n",
    "    \n",
    "        super().__init__()\n",
    "        if not input_dir :\n",
    "            raise ValueError(\"Must provide either `input_dir` or `input_files`.\")\n",
    "\n",
    "        self.recursive = recursive\n",
    "        self.exclude_hidden = exclude_hidden\n",
    "        self.required_exts = required_exts\n",
    "        self.num_files_limit = num_files_limit\n",
    "\n",
    "    def add_files(self, input_dir):\n",
    "        all_files = set()\n",
    "        rejected_files = set()\n",
    "        list_files = []\n",
    "\n",
    "        file_refs: Generator[Path, None, None]\n",
    "        if self.recursive:\n",
    "            file_refs = Path(input_dir).rglob(\"*\")\n",
    "        else:\n",
    "            file_refs = Path(input_dir).glob(\"*\")\n",
    "        for ref in file_refs:\n",
    "            # Manually check if file is hidden or directory instead of\n",
    "            # in glob for backwards compatibility.\n",
    "            is_dir = ref.is_dir()\n",
    "            skip_because_hidden = self.exclude_hidden and ref.name.startswith(\".\")\n",
    "            skip_because_bad_ext = (\n",
    "                self.required_exts is not None and ref.suffix not in required_exts\n",
    "            )\n",
    "            skip_because_excluded = ref in rejected_files\n",
    "\n",
    "            if (\n",
    "                is_dir\n",
    "                or skip_because_hidden\n",
    "                or skip_because_bad_ext\n",
    "                or skip_because_excluded\n",
    "            ):\n",
    "                continue\n",
    "            else:\n",
    "                all_files.add(ref)\n",
    "        new_input_files = sorted(list(all_files))\n",
    "\n",
    "        if len(new_input_files) == 0:\n",
    "            raise ValueError(f\"No files found in {input_dir}.\")\n",
    "\n",
    "        if self.num_files_limit is not None and self.num_files_limit > 0:\n",
    "            new_input_files = new_input_files[0 : num_files_limit]\n",
    "\n",
    "        # print total number of files added\n",
    "        logger.debug(\n",
    "            f\"> [SimpleDirectoryReader] Total files added: {len(new_input_files)}\")\n",
    "\n",
    "        for f in new_input_files:\n",
    "            list_files.append(str(f))\n",
    "        return list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.base import BaseReader\n",
    "from llama_index.schema import Document\n",
    "\n",
    "class HtmlFilesReader(BaseReader):\n",
    "    \"\"\"Simple web page reader.\n",
    "\n",
    "    Reads pages from the web.\n",
    "\n",
    "    Args:\n",
    "        html_to_text (bool): Whether to convert HTML to text.\n",
    "            Requires `html2text` package.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, html_to_text: bool = False):\n",
    "        \"\"\"Initialize with parameters.\"\"\"\n",
    "        try:\n",
    "            import html2text  # noqa: F401\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"`html2text` package not found, please run `pip install html2text`\"\n",
    "            )\n",
    "        self._html_to_text = html_to_text\n",
    "\n",
    "    def load_data(self, input_files):\n",
    "        \"\"\"Load data from the input directory.\n",
    "\n",
    "        Args:\n",
    "            urls (List[str]): List of URLs to scrape.\n",
    "\n",
    "        Returns:\n",
    "            List[Document]: List of documents.\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(input_files, list):\n",
    "            raise ValueError(\"input_files must be a list of strings.\")\n",
    "        documents = []\n",
    "        for input_file in input_files:\n",
    "            #response = requests.get(url, headers=None).text\n",
    "            with open(input_file, \"r\", errors = \"ignore\", encoding='utf-8') as f:\n",
    "                response = f.read()\n",
    "            if self._html_to_text:\n",
    "                import html2text\n",
    "\n",
    "                response = html2text.html2text(response)\n",
    "\n",
    "            doc = Document(text=response)\n",
    "            doc.id_ = str(input_file)\n",
    "\n",
    "            documents.append(doc)\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"./omniscienSmall.com/about-us/company\"\n",
    "lists_files = DirectorySearchSource().add_files(input_dir)\n",
    "documents = HtmlFilesReader(html_to_text=True).load_data(input_files = lists_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/home/sira/sira_project/DQA_demo/orca-mini-3b.ggmlv3.q4_0.bin\",\n",
    "        n_ctx=2048,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sira/anaconda3/envs/lang-lama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2 \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/sira/sira_project/DQA_demo/orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 2194.73 MB (+  650.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LangChainLLM(load_llm())\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1000, embed_model= embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "# NOTE: can take a while!\n",
    "index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    max_triplets_per_chunk=2,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.set_index_id(\"vector_index_graph\")\n",
    "# index.storage_context.persist(\"./llama_vector_graph_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./llama_vector_graph_small\")\n",
    "index = load_index_from_storage(storage_context, root_id=\"<root_id>\", service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   806.40 ms\n",
      "llama_print_timings:      sample time =     2.60 ms /     5 runs   (    0.52 ms per token,  1920.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8409.02 ms /    85 tokens (   98.93 ms per token,    10.11 tokens per second)\n",
      "llama_print_timings:        eval time =   447.19 ms /     4 runs   (  111.80 ms per token,     8.94 tokens per second)\n",
      "llama_print_timings:       total time =  8892.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   806.40 ms\n",
      "llama_print_timings:      sample time =     0.50 ms /     1 runs   (    0.50 ms per token,  2016.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7081.04 ms /    72 tokens (   98.35 ms per token,    10.17 tokens per second)\n",
      "llama_print_timings:        eval time =   110.18 ms /     1 runs   (  110.18 ms per token,     9.08 tokens per second)\n",
      "llama_print_timings:       total time =  7211.72 ms\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(include_text=False, response_mode=\"tree_summarize\",\n",
    "                                     retriever_mode=\"keyword\")\n",
    "response = query_engine.query(\n",
    "    \"Tell me more about Philipp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example: 'KEYWORDS: \"Philpp\", \"writing\"'."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   806.40 ms\n",
      "llama_print_timings:      sample time =    12.25 ms /    18 runs   (    0.68 ms per token,  1469.99 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10657.37 ms /    87 tokens (  122.50 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:        eval time =  2469.15 ms /    17 runs   (  145.24 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:       total time = 13247.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next action(s) may include:\n",
      " - Ask for more context information\n",
      " - Check if any of the existing books have been written by Philpp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   806.40 ms\n",
      "llama_print_timings:      sample time =    20.13 ms /    33 runs   (    0.61 ms per token,  1639.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9136.66 ms /    75 tokens (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_print_timings:        eval time =  4223.60 ms /    32 runs   (  131.99 ms per token,     7.58 tokens per second)\n",
      "llama_print_timings:       total time = 13535.81 ms\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"Which book are writing by Philpp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "g = index.get_networkx_graph()\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "net.show(\"example.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-lama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
