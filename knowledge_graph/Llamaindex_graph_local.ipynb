{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import time\n",
    "\n",
    "from llama_index import(\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader,\n",
    "    LangchainEmbedding,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    "    load_graph_from_storage,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    get_response_synthesizer,\n",
    "    QueryBundle,\n",
    "    Prompt\n",
    "    )\n",
    "\n",
    "# upload model\n",
    "from llama_index.llms import LangChainLLM\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index import (KnowledgeGraphIndex)\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from typing import List\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/home/sira/sira_project/meta-Llama2/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,\n",
    "        n_ctx = 4096, \n",
    "        temperature = 0.1, \n",
    "        max_tokens = 4096\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/sira/sira_project/meta-Llama2/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 7354.73 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "/home/sira/anaconda3/envs/lang-lama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=LangChainLLM(llm = load_llm()))\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs = {'device': 'cpu'}))\n",
    "service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor, \n",
    "        chunk_size=1000, \n",
    "        embed_model = embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt():\n",
    "    TEMPLATE_STR = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"\"\"Given this information, please return only useful answer.\n",
    "    Each response should consist of at least two sentences, with a minimum length requirement. \n",
    "    Avoid using redundant or repetitive phrases in your response.\n",
    "    If you don't know the answer, please just say that you don't know the answer, \n",
    "    Don't try to make up an answer and we encourage you to present diverse sentence structures and formats in your answers, \n",
    "    rather than relying on the same patterns repeatedly for each sentence. : {query_str}\\n\"\"\")\n",
    "    QA_TEMPLATE = Prompt(TEMPLATE_STR)\n",
    "    return QA_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph Vector\n",
    "storage_context_graph = StorageContext.from_defaults(persist_dir=\"./llama7b_graph_index_removeHTML\")\n",
    "kg_index = load_index_from_storage(storage_context = storage_context_graph, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_keyword_query_engine = kg_index.as_query_engine(\n",
    "    # setting to false uses the raw triplets instead of adding the text from the corresponding nodes\n",
    "    include_text=True,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    "    streaming=False,\n",
    "    service_context=service_context,\n",
    "    similarity_top_k=5,\n",
    "    text_qa_template=custom_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, if you were asked \"What is the capital of France?\", you might provide the following answer:\n",
      "'KEYWORDS: writing, book, Philipp, Koehn'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2495.75 ms\n",
      "llama_print_timings:      sample time =    21.72 ms /    40 runs   (    0.54 ms per token,  1841.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25151.24 ms /    87 tokens (  289.09 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:        eval time = 12177.51 ms /    39 runs   (  312.24 ms per token,     3.20 tokens per second)\n",
      "llama_print_timings:       total time = 37503.22 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide a number or a range of numbers if possible, or indicate that you don't know the answer."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2495.75 ms\n",
      "llama_print_timings:      sample time =    14.15 ms /    24 runs   (    0.59 ms per token,  1696.59 tokens per second)\n",
      "llama_print_timings: prompt eval time = 58180.59 ms /   197 tokens (  295.33 ms per token,     3.39 tokens per second)\n",
      "llama_print_timings:        eval time =  7692.12 ms /    23 runs   (  334.44 ms per token,     2.99 tokens per second)\n",
      "llama_print_timings:       total time = 66010.91 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"How many book are writing by Philipp Koehn.\"\n",
    "response_stream = kg_keyword_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"Please provide a number or a range of numbers if possible, or indicate that you don't know the answer.\", source_nodes=[NodeWithScore(node=TextNode(id_='335d8e78-1521-4b13-a294-47ab7ab2b55d', embedding=None, metadata={'kg_rel_texts': [\"Philipp ['is', 'Chief Scientist of Omniscien']\"], 'kg_rel_map': {'France': [], 'What': [], 'might': [], 'example': [], 'capital': [], 'Philipp': [['is', 'Chief Scientist of Omniscien']], 'book': [], 'For example': [], 'For': [], 'if you were asked \"What is the capital of France?\"': [], 'asked': [], 'answer': [], 'KEYWORDS': [], 'provide': [], 'writing': [], \"you might provide the following answer:\\n'KEYWORDS: writing\": [], \"Koehn'\": [], 'following': []}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_texts'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_texts'], relationships={}, hash='20058839efae79517bc935d0978f124d823e86755ef4c220e5aa7e03a7730144', text=\"The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\\nPhilipp ['is', 'Chief Scientist of Omniscien']\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0)], metadata={'335d8e78-1521-4b13-a294-47ab7ab2b55d': {'kg_rel_texts': [\"Philipp ['is', 'Chief Scientist of Omniscien']\"], 'kg_rel_map': {'France': [], 'What': [], 'might': [], 'example': [], 'capital': [], 'Philipp': [['is', 'Chief Scientist of Omniscien']], 'book': [], 'For example': [], 'For': [], 'if you were asked \"What is the capital of France?\"': [], 'asked': [], 'answer': [], 'KEYWORDS': [], 'provide': [], 'writing': [], \"you might provide the following answer:\\n'KEYWORDS: writing\": [], \"Koehn'\": [], 'following': []}}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, if you were asked \"What is the capital of France?\", you might provide the following answer:\n",
      "'KEYWORDS: writing, book, Philipp, Koehn'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2545.96 ms\n",
      "llama_print_timings:      sample time =    21.59 ms /    40 runs   (    0.54 ms per token,  1852.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 25530.28 ms /    87 tokens (  293.45 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:        eval time = 12240.46 ms /    39 runs   (  313.86 ms per token,     3.19 tokens per second)\n",
      "llama_print_timings:       total time = 37938.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the answer in a number."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2545.96 ms\n",
      "llama_print_timings:      sample time =     5.98 ms /     9 runs   (    0.66 ms per token,  1506.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 672916.15 ms /  2166 tokens (  310.67 ms per token,     3.22 tokens per second)\n",
      "llama_print_timings:        eval time =  3110.69 ms /     8 runs   (  388.84 ms per token,     2.57 tokens per second)\n",
      "llama_print_timings:       total time = 676826.78 ms\n"
     ]
    }
   ],
   "source": [
    "query_engine = kg_index.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"How many book are writing by Philipp Koehn.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Please provide the answer in a number.', source_nodes=[NodeWithScore(node=TextNode(id_='39d6b1d1-2f5f-4afb-a5a6-b3220b525ab2', embedding=None, metadata={'file_name': 'omniscien.com/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='85450672-cd49-423f-8294-94d7449bd3d0', node_type=None, metadata={'file_name': 'omniscien.com/index.html'}, hash='d9d9d676f9d7a153338d7d2d982a4b4d3ff1b74f973b7ad6812746c57ea796f2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='843a3245-2bd3-4495-a238-ca28a0a89886', node_type=None, metadata={'file_name': 'omniscien.com/index.html'}, hash='e709a5dc166133926053fdffedeead7332162206de435c95c26f94f678b62838'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b7e487ea-dea1-4d1d-b33e-30471c46e3c5', node_type=None, metadata={'file_name': 'omniscien.com/index.html'}, hash='6ff92361b39cdb717da3411b58bc4e8a9d7d9b6c48e3327f7ef969293b26cb87')}, hash='196c431f43caeae04b0caa24b667e29811cc99d5e520a335f334674c989d1360', text='MT](http://omniscien.com/wp-content/uploads/2020/08/TrainMT92.png)\\n\\n![](http://omniscien.com/wp-content/uploads/slider9/guage_128.png)\\n\\n![Translate](http://omniscien.com/wp-content/uploads/slider9/languge128.png)\\n\\n![](http://omniscien.com/wp-content/uploads/slider9/lscloud_128-1.png)\\n\\n![](http://omniscien.com/wp-content/uploads/slider9/avatar.png)\\n\\nHey! Did you know that you can more than double your productivity with machine\\ntranslation when it is optimized for purpose? üëçüëçüëç\\n\\n##  Artificial Intelligence Built for  Smart Language Processing\\n\\n##  Artificial Intelligence Built for  Smart Language Processing\\n\\n![Artificial Intelligence, Machine Learning, Natural Language Processing, and\\nMachine Translation](https://omniscien.com/wp-\\ncontent/uploads/2020/07/TrainMT2-300x300.png)\\n\\n##  Data is the Fuel that Powers Artificial Intelligence\\n\\n##  Data is the Fuel that  \\nPowers Artificial Intelligence\\n\\nBuilt on the world\\'s leading translation, language processing, workflow\\nautomation, and artificial intelligence technologies.\\n\\nTranslation and language processing technologies have evolved substantially\\nover the last decade. The Omniscien team has been at the forefront of research\\nand development, leading the way with a [comprehensive set of integrated\\ntools, features, and technologies](technology/index.html) that are powered by\\nand drive artificial intelligence and machine learning.\\n\\nNaturally, Omniscien tools and technologies are heavily reliant on high-\\nquality specialized data to power our platform and technologies. Because we\\nunderstand the importance of high-quality data, the Omniscien team is\\ndedicated to breaking new ground with novel research and approaches to\\ncreating, mining, harvesting, synthesizing, and manufacturing data. Omniscien\\nhas built a variety of [powerful tools for data creation, preparation, and\\nanalysis](machine-translation/powerful-tools-for-data-creation-preparation-\\nand-analysis/index.html).\\n\\nBehind many of the tools design is Omniscien‚Äôs Chief Scientist, Professor\\nPhilipp Koehn who leads our team of researchers and developers. Philipp is a\\npioneer in the machine translation space, his books on [Statistical Machine\\nTranslation](https://www.amazon.com/Statistical-Machine-Translation-Philipp-\\nKoehn/dp/0521874157 \"Statistical Machine Translation\") and [Neural Machine\\nTranslation](https://www.amazon.com/Neural-Machine-Translation-Philipp-\\nKoehn/dp/1108497322 \"Neural Machine Translation\") are the leading academic\\ntextbooks globally on machine translation. Both books are available now from\\nAmazon.com or leading book stores.\\n\\n![Philipp Koehn](https://omniscien.com/wp-\\ncontent/uploads/2020/07/Koehn-1721.jpg)\\n\\nProfessor Philipp Koehn,  \\nChief Scientist,  \\nOmniscien Technologies.\\n\\n[![Neural Machine Translation - Philipp Koehn - Book\\nCover](https://omniscien.com/wp-\\ncontent/uploads/2020/07/51dmGDrqsML1-210x300.jpg)](https://www.amazon.com/Neural-\\nMachine-Translation-Philipp-Koehn/dp/1108497322)\\n\\n[![Statistical Machine Translation - Philipp Koehn -', start_char_idx=15509, end_char_idx=18512, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='41dadf2f-e752-45cc-8e46-526f27b892a0', embedding=None, metadata={'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ae7417e4-44e1-4e48-aa9d-756c97c3d92e', node_type=None, metadata={'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, hash='58f249ffb3a4dede3cbf92137cc787d6c227b44a54db311e11d3de5d814985b5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='873f1a95-ae74-4e9e-a00a-47266baa9435', node_type=None, metadata={'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, hash='e7d5a964e26913cfedc08d865198e05583c832ccfe159af9e1f62483e2bab777'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0a5b75cc-cad7-4483-86fe-237be4fc17bd', node_type=None, metadata={'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, hash='4de333fc459dfa7ace9a23db94f02a96620f1ffcc37e80d1939fcc35841116c3')}, hash='8dfbd50b32cb94259b1110e46cf1c8408fd4a02ff084b9c100dd44141c3e058a', text='   * [Data Security and Privacy](../../machine-translation/data-security-and-privacy/index.html)\\n\\n__ Search\\n\\n# The Omniscien Advantage - We wrote the leading academic machine translation\\ntextbooks!!\\n\\nNov 11, 2022\\n\\nBuilt on the world\\'s leading translation, language processing, workflow\\nautomation, and artificial intelligence technologies.\\n\\nTranslation and language processing technologies have evolved substantially\\nover the last decade. The Omniscien team has been at the forefront of research\\nand development, leading the way with a [comprehensive set of integrated\\ntools, features, and technologies](../../technology/index.html) that are\\npowered by and drive artificial intelligence and machine learning.\\n\\nNaturally, Omniscien tools and technologies are heavily reliant on high-\\nquality specialized data to power our platform and technologies. Because we\\nunderstand the importance of high-quality data, the Omniscien team is\\ndedicated to breaking new ground with novel research and approaches to\\ncreating, mining, harvesting, synthesizing, and manufacturing data. Omniscien\\nhas built a variety of [powerful tools for data creation, preparation, and\\nanalysis](../../machine-translation/powerful-tools-for-data-creation-\\npreparation-and-analysis/index.html).\\n\\nBehind many of the tools design is Omniscien‚Äôs Chief Scientist, Professor\\nPhilipp Koehn who leads our team of researchers and developers. Philipp is a\\npioneer in the machine translation space, his books on [Statistical Machine\\nTranslation](https://www.amazon.com/Statistical-Machine-Translation-Philipp-\\nKoehn/dp/0521874157 \"Statistical Machine Translation\") and [Neural Machine\\nTranslation](https://www.amazon.com/Neural-Machine-Translation-Philipp-\\nKoehn/dp/1108497322 \"Neural Machine Translation\") are the leading academic\\ntextbooks globally on machine translation. Both books are available now from\\nAmazon.com or leading book stores.\\n\\n![Philipp Koehn](https://omniscien.com/wp-\\ncontent/uploads/2020/07/Koehn-1721.jpg)\\n\\nProfessor Philipp Koehn,  \\nChief Scientist,  \\nOmniscien Technologies.\\n\\n[![Neural Machine Translation - Philipp Koehn - Book\\nCover](https://omniscien.com/wp-\\ncontent/uploads/2020/07/51dmGDrqsML1-210x300.jpg)](https://www.amazon.com/Neural-\\nMachine-Translation-Philipp-Koehn/dp/1108497322)\\n\\n[![Statistical Machine Translation - Philipp Koehn - Book\\nCover](https://omniscien.com/wp-\\ncontent/uploads/2020/07/61AHDbe8ymL._SX354_BO1204203200_1-214x300.jpg)](https://www.amazon.com/Statistical-\\nMachine-Translation-Philipp-Koehn/dp/0521874157)\\n\\n## Related Links\\n\\n **FAQ**\\n\\n  * [What is Rules-Based Machine Translation?](../../faq/what-is-rules-based-machine-translation/index.html)\\n  * [What is Syntax-Based Machine Translation?](../../faq/what-is-syntax-based-machine-translation/index.html)\\n  * [What is Neural Machine Translation?](../../faq/what-is-neural-machine-translation/index.html)\\n\\n **Pages**\\n\\n  * [Introduction to Machine Translation at Omniscien](../../machine-translation/index.html)\\n  * [Hybrid NMT/SMT Translation](../../machine-translation/hybrid-machine-translation/index.html)\\n\\nSee how Omniscien can help  \\n **solve your unique  \\nlanguage and document processing challenges**\\n\\n[Request a Demo](../../resources/request-a-demo/index.html)\\n\\n[Contact', start_char_idx=7914, end_char_idx=11157, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='fe88aa9b-805f-4f30-84cd-4141aa37a6fd', embedding=None, metadata={'kg_rel_texts': [\"Philipp ['is', 'Chief Scientist of Omniscien']\"], 'kg_rel_map': {'France': [], 'What': [], 'might': [], 'example': [], 'capital': [], 'Philipp': [['is', 'Chief Scientist of Omniscien']], 'book': [], 'For example': [], 'For': [], 'if you were asked \"What is the capital of France?\"': [], 'asked': [], 'answer': [], 'KEYWORDS': [], 'provide': [], 'writing': [], \"you might provide the following answer:\\n'KEYWORDS: writing\": [], \"Koehn'\": [], 'following': []}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_texts'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_texts'], relationships={}, hash='20058839efae79517bc935d0978f124d823e86755ef4c220e5aa7e03a7730144', text=\"The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\\nPhilipp ['is', 'Chief Scientist of Omniscien']\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0)], metadata={'39d6b1d1-2f5f-4afb-a5a6-b3220b525ab2': {'file_name': 'omniscien.com/index.html'}, '41dadf2f-e752-45cc-8e46-526f27b892a0': {'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, 'fe88aa9b-805f-4f30-84cd-4141aa37a6fd': {'kg_rel_texts': [\"Philipp ['is', 'Chief Scientist of Omniscien']\"], 'kg_rel_map': {'France': [], 'What': [], 'might': [], 'example': [], 'capital': [], 'Philipp': [['is', 'Chief Scientist of Omniscien']], 'book': [], 'For example': [], 'For': [], 'if you were asked \"What is the capital of France?\"': [], 'asked': [], 'answer': [], 'KEYWORDS': [], 'provide': [], 'writing': [], \"you might provide the following answer:\\n'KEYWORDS: writing\": [], \"Koehn'\": [], 'following': []}}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Please provide the answer in a number.', source_nodes=[NodeWithScore(node=TextNode(id_='39d6b1d1-2f5f-4afb-a5a6-b3220b525ab2', embedding=None, metadata={'file_name': 'omniscien.com/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='85450672-cd49-423f-8294-94d7449bd3d0', node_type=None, metadata={'file_name': 'omniscien.com/index.html'}, hash='d9d9d676f9d7a153338d7d2d982a4b4d3ff1b74f973b7ad6812746c57ea796f2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='843a3245-2bd3-4495-a238-ca28a0a89886', node_type=None, metadata={'file_name': 'omniscien.com/index.html'}, hash='e709a5dc166133926053fdffedeead7332162206de435c95c26f94f678b62838'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b7e487ea-dea1-4d1d-b33e-30471c46e3c5', node_type=None, metadata={'file_name': 'omniscien.com/index.html'}, hash='6ff92361b39cdb717da3411b58bc4e8a9d7d9b6c48e3327f7ef969293b26cb87')}, hash='196c431f43caeae04b0caa24b667e29811cc99d5e520a335f334674c989d1360', text='MT](http://omniscien.com/wp-content/uploads/2020/08/TrainMT92.png)\\n\\n![](http://omniscien.com/wp-content/uploads/slider9/guage_128.png)\\n\\n![Translate](http://omniscien.com/wp-content/uploads/slider9/languge128.png)\\n\\n![](http://omniscien.com/wp-content/uploads/slider9/lscloud_128-1.png)\\n\\n![](http://omniscien.com/wp-content/uploads/slider9/avatar.png)\\n\\nHey! Did you know that you can more than double your productivity with machine\\ntranslation when it is optimized for purpose? üëçüëçüëç\\n\\n##  Artificial Intelligence Built for  Smart Language Processing\\n\\n##  Artificial Intelligence Built for  Smart Language Processing\\n\\n![Artificial Intelligence, Machine Learning, Natural Language Processing, and\\nMachine Translation](https://omniscien.com/wp-\\ncontent/uploads/2020/07/TrainMT2-300x300.png)\\n\\n##  Data is the Fuel that Powers Artificial Intelligence\\n\\n##  Data is the Fuel that  \\nPowers Artificial Intelligence\\n\\nBuilt on the world\\'s leading translation, language processing, workflow\\nautomation, and artificial intelligence technologies.\\n\\nTranslation and language processing technologies have evolved substantially\\nover the last decade. The Omniscien team has been at the forefront of research\\nand development, leading the way with a [comprehensive set of integrated\\ntools, features, and technologies](technology/index.html) that are powered by\\nand drive artificial intelligence and machine learning.\\n\\nNaturally, Omniscien tools and technologies are heavily reliant on high-\\nquality specialized data to power our platform and technologies. Because we\\nunderstand the importance of high-quality data, the Omniscien team is\\ndedicated to breaking new ground with novel research and approaches to\\ncreating, mining, harvesting, synthesizing, and manufacturing data. Omniscien\\nhas built a variety of [powerful tools for data creation, preparation, and\\nanalysis](machine-translation/powerful-tools-for-data-creation-preparation-\\nand-analysis/index.html).\\n\\nBehind many of the tools design is Omniscien‚Äôs Chief Scientist, Professor\\nPhilipp Koehn who leads our team of researchers and developers. Philipp is a\\npioneer in the machine translation space, his books on [Statistical Machine\\nTranslation](https://www.amazon.com/Statistical-Machine-Translation-Philipp-\\nKoehn/dp/0521874157 \"Statistical Machine Translation\") and [Neural Machine\\nTranslation](https://www.amazon.com/Neural-Machine-Translation-Philipp-\\nKoehn/dp/1108497322 \"Neural Machine Translation\") are the leading academic\\ntextbooks globally on machine translation. Both books are available now from\\nAmazon.com or leading book stores.\\n\\n![Philipp Koehn](https://omniscien.com/wp-\\ncontent/uploads/2020/07/Koehn-1721.jpg)\\n\\nProfessor Philipp Koehn,  \\nChief Scientist,  \\nOmniscien Technologies.\\n\\n[![Neural Machine Translation - Philipp Koehn - Book\\nCover](https://omniscien.com/wp-\\ncontent/uploads/2020/07/51dmGDrqsML1-210x300.jpg)](https://www.amazon.com/Neural-\\nMachine-Translation-Philipp-Koehn/dp/1108497322)\\n\\n[![Statistical Machine Translation - Philipp Koehn -', start_char_idx=15509, end_char_idx=18512, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='41dadf2f-e752-45cc-8e46-526f27b892a0', embedding=None, metadata={'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ae7417e4-44e1-4e48-aa9d-756c97c3d92e', node_type=None, metadata={'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, hash='58f249ffb3a4dede3cbf92137cc787d6c227b44a54db311e11d3de5d814985b5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='873f1a95-ae74-4e9e-a00a-47266baa9435', node_type=None, metadata={'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, hash='e7d5a964e26913cfedc08d865198e05583c832ccfe159af9e1f62483e2bab777'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0a5b75cc-cad7-4483-86fe-237be4fc17bd', node_type=None, metadata={'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, hash='4de333fc459dfa7ace9a23db94f02a96620f1ffcc37e80d1939fcc35841116c3')}, hash='8dfbd50b32cb94259b1110e46cf1c8408fd4a02ff084b9c100dd44141c3e058a', text='   * [Data Security and Privacy](../../machine-translation/data-security-and-privacy/index.html)\\n\\n__ Search\\n\\n# The Omniscien Advantage - We wrote the leading academic machine translation\\ntextbooks!!\\n\\nNov 11, 2022\\n\\nBuilt on the world\\'s leading translation, language processing, workflow\\nautomation, and artificial intelligence technologies.\\n\\nTranslation and language processing technologies have evolved substantially\\nover the last decade. The Omniscien team has been at the forefront of research\\nand development, leading the way with a [comprehensive set of integrated\\ntools, features, and technologies](../../technology/index.html) that are\\npowered by and drive artificial intelligence and machine learning.\\n\\nNaturally, Omniscien tools and technologies are heavily reliant on high-\\nquality specialized data to power our platform and technologies. Because we\\nunderstand the importance of high-quality data, the Omniscien team is\\ndedicated to breaking new ground with novel research and approaches to\\ncreating, mining, harvesting, synthesizing, and manufacturing data. Omniscien\\nhas built a variety of [powerful tools for data creation, preparation, and\\nanalysis](../../machine-translation/powerful-tools-for-data-creation-\\npreparation-and-analysis/index.html).\\n\\nBehind many of the tools design is Omniscien‚Äôs Chief Scientist, Professor\\nPhilipp Koehn who leads our team of researchers and developers. Philipp is a\\npioneer in the machine translation space, his books on [Statistical Machine\\nTranslation](https://www.amazon.com/Statistical-Machine-Translation-Philipp-\\nKoehn/dp/0521874157 \"Statistical Machine Translation\") and [Neural Machine\\nTranslation](https://www.amazon.com/Neural-Machine-Translation-Philipp-\\nKoehn/dp/1108497322 \"Neural Machine Translation\") are the leading academic\\ntextbooks globally on machine translation. Both books are available now from\\nAmazon.com or leading book stores.\\n\\n![Philipp Koehn](https://omniscien.com/wp-\\ncontent/uploads/2020/07/Koehn-1721.jpg)\\n\\nProfessor Philipp Koehn,  \\nChief Scientist,  \\nOmniscien Technologies.\\n\\n[![Neural Machine Translation - Philipp Koehn - Book\\nCover](https://omniscien.com/wp-\\ncontent/uploads/2020/07/51dmGDrqsML1-210x300.jpg)](https://www.amazon.com/Neural-\\nMachine-Translation-Philipp-Koehn/dp/1108497322)\\n\\n[![Statistical Machine Translation - Philipp Koehn - Book\\nCover](https://omniscien.com/wp-\\ncontent/uploads/2020/07/61AHDbe8ymL._SX354_BO1204203200_1-214x300.jpg)](https://www.amazon.com/Statistical-\\nMachine-Translation-Philipp-Koehn/dp/0521874157)\\n\\n## Related Links\\n\\n **FAQ**\\n\\n  * [What is Rules-Based Machine Translation?](../../faq/what-is-rules-based-machine-translation/index.html)\\n  * [What is Syntax-Based Machine Translation?](../../faq/what-is-syntax-based-machine-translation/index.html)\\n  * [What is Neural Machine Translation?](../../faq/what-is-neural-machine-translation/index.html)\\n\\n **Pages**\\n\\n  * [Introduction to Machine Translation at Omniscien](../../machine-translation/index.html)\\n  * [Hybrid NMT/SMT Translation](../../machine-translation/hybrid-machine-translation/index.html)\\n\\nSee how Omniscien can help  \\n **solve your unique  \\nlanguage and document processing challenges**\\n\\n[Request a Demo](../../resources/request-a-demo/index.html)\\n\\n[Contact', start_char_idx=7914, end_char_idx=11157, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='beabbc21-e642-40c2-b834-3c2c98a9784a', embedding=None, metadata={'kg_rel_texts': [\"Philipp ['is', 'Chief Scientist of Omniscien']\"], 'kg_rel_map': {'France': [], 'What': [], 'might': [], 'example': [], 'capital': [], 'Philipp': [['is', 'Chief Scientist of Omniscien']], 'book': [], 'For example': [], 'For': [], 'if you were asked \"What is the capital of France?\"': [], 'asked': [], 'answer': [], 'KEYWORDS': [], 'provide': [], 'writing': [], \"you might provide the following answer:\\n'KEYWORDS: writing\": [], \"Koehn'\": [], 'following': []}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_texts'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_texts'], relationships={}, hash='20058839efae79517bc935d0978f124d823e86755ef4c220e5aa7e03a7730144', text=\"The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\\nPhilipp ['is', 'Chief Scientist of Omniscien']\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0)], metadata={'39d6b1d1-2f5f-4afb-a5a6-b3220b525ab2': {'file_name': 'omniscien.com/index.html'}, '41dadf2f-e752-45cc-8e46-526f27b892a0': {'file_name': 'omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book/index.html'}, 'beabbc21-e642-40c2-b834-3c2c98a9784a': {'kg_rel_texts': [\"Philipp ['is', 'Chief Scientist of Omniscien']\"], 'kg_rel_map': {'France': [], 'What': [], 'might': [], 'example': [], 'capital': [], 'Philipp': [['is', 'Chief Scientist of Omniscien']], 'book': [], 'For example': [], 'For': [], 'if you were asked \"What is the capital of France?\"': [], 'asked': [], 'answer': [], 'KEYWORDS': [], 'provide': [], 'writing': [], \"you might provide the following answer:\\n'KEYWORDS: writing\": [], \"Koehn'\": [], 'following': []}}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Please provide the answer as a number.', source_nodes=[NodeWithScore(node=TextNode(id_='69b05072-ff45-491c-8dd7-fe9670bb3a32', embedding=None, metadata={'kg_rel_texts': [], 'kg_rel_map': {'if you were asked how many books are written by J.K. Rowling': [], 'books': [], 'For': [], 'written': [], \"J.K. Rowling'\": [], 'For example': [], 'Rowling': [], 'many': [], \"you might provide the following answer:\\n'KEYWORDS: writing\": [], 'might': [], 'provide': [], 'KEYWORDS': [], 'J': [], 'asked': [], 'answer': [], 'writing': [], 'example': [], 'following': [], 'K': []}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_texts'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_texts'], relationships={}, hash='d96d75a30e5603057ccb932f3796a944b6fcf9f72defa0af3bf621266c762d70', text='The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0)], metadata={'69b05072-ff45-491c-8dd7-fe9670bb3a32': {'kg_rel_texts': [], 'kg_rel_map': {'if you were asked how many books are written by J.K. Rowling': [], 'books': [], 'For': [], 'written': [], \"J.K. Rowling'\": [], 'For example': [], 'Rowling': [], 'many': [], \"you might provide the following answer:\\n'KEYWORDS: writing\": [], 'might': [], 'provide': [], 'KEYWORDS': [], 'J': [], 'asked': [], 'answer': [], 'writing': [], 'example': [], 'following': [], 'K': []}}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(source_list):\n",
    "    res = []\n",
    "    for i in source_list:\n",
    "        if i not in res:\n",
    "            res.append(i)\n",
    "    return res\n",
    "\n",
    "def convert_to_website_format(urls):\n",
    "    convert_urls = []\n",
    "    for url in urls:\n",
    "        # Remove any '.html' at the end of the URL\n",
    "        url = re.sub(r'\\.html$', '', url)\n",
    "        # Check if the URL starts with 'www.' or 'http://'\n",
    "        if not re.match(r'(www\\.|http://)', url):\n",
    "            url = 'www.' + url\n",
    "        if '/index' in url:\n",
    "            url = url.split('/index')[0] \n",
    "        convert_urls.append(url)\n",
    "    return convert_urls\n",
    "\n",
    "def regex_source(answer):\n",
    "    pattern = r\"'file_name': '(.*?)'\"\n",
    "    matchs = re.findall(pattern, str(answer))\n",
    "    convert_urls = convert_to_website_format(matchs)\n",
    "    res_urls = check_duplicate(source_list=convert_urls)\n",
    "    return res_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = regex_source(response.get_formatted_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.omniscien.com',\n",
       " 'www.omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = regex_source(response.get_formatted_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.omniscien.com',\n",
       " 'www.omniscien.com/blog/the-omniscien-advantage-we-wrote-the-book']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check with max-triple = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('./llama7b_graph_index_removeHTML_MaxTriplets10/graph_store.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is', 'Chief Technology Officer'],\n",
       " ['CTO', 'Omniscien Technologies'],\n",
       " ['is', 'Co-Founder'],\n",
       " ['is', 'CTO of'],\n",
       " ['CTO', 'Omniscien'],\n",
       " ['is', 'Customizing a High Quality Machine Translation Engine'],\n",
       " ['is', 'employee of Omniscien']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"graph_dict\"][\"Dion Wiggins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    n_gpu_layers = 32 \n",
    "    n_batch = 512  \n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/home/sira/sira_project/meta-Llama2/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        callback_manager=callback_manager,\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        verbose=True,\n",
    "        n_ctx = 4096, \n",
    "        temperature = 0.1, \n",
    "        max_tokens = 4096\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/sira/sira_project/meta-Llama2/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 7354.73 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "Exception ignored in: <function Llama.__del__ at 0x7f97fd37b940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sira/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_cpp/llama.py\", line 1507, in __del__\n",
      "    if self.model is not None:\n",
      "AttributeError: 'Llama' object has no attribute 'model'\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=LangChainLLM(llm = load_llm()))\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs = {'device': 'cpu'}))\n",
    "service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor, \n",
    "        chunk_size=1000, \n",
    "        embed_model = embed_model)\n",
    "#Graph Vector\n",
    "storage_context_graph = StorageContext.from_defaults(persist_dir=\"./llama7b_graph_index_removeHTML_MaxTriplets10\")\n",
    "kg_index = load_index_from_storage(storage_context = storage_context_graph, service_context = service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_keyword_query_engine = kg_index.as_query_engine(\n",
    "    # setting to false uses the raw triplets instead of adding the text from the corresponding nodes\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Dion Wiggins is a 35-year-old man from Texas who was arrested on January 10, 2023, for allegedly robbing a convenience store in Houston. He has been charged with aggravated robbery and is currently being held at the Harris County Jail.\n",
      "\n",
      "KEYWORDS: Dion Wiggins, Texas, arrest, robbery, convenience store, aggravated robbery, Harris County Jail"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23032.39 ms\n",
      "llama_print_timings:      sample time =    61.56 ms /   107 runs   (    0.58 ms per token,  1738.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23262.62 ms /    83 tokens (  280.27 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time = 33469.07 ms /   106 runs   (  315.75 ms per token,     3.17 tokens per second)\n",
      "llama_print_timings:       total time = 57148.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Subject: Dion Wiggins\n",
      "Predicate: has_job\n",
      "Object: software engineer at Google\n",
      "Predicate next hop: works_at\n",
      "Object next hop: Google\n",
      "\n",
      "Therefore, the answer to the question \"Who is Dion Wiggins?\" is: Dion Wiggins is a software engineer at Google."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23032.39 ms\n",
      "llama_print_timings:      sample time =    40.58 ms /    73 runs   (    0.56 ms per token,  1798.74 tokens per second)\n",
      "llama_print_timings: prompt eval time = 20448.92 ms /    73 tokens (  280.12 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time = 22333.94 ms /    72 runs   (  310.19 ms per token,     3.22 tokens per second)\n",
      "llama_print_timings:       total time = 43058.45 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>---------------------\n",
       "Subject: Dion Wiggins\n",
       "Predicate: has_job\n",
       "Object: software engineer at Google\n",
       "Predicate next hop: works_at\n",
       "Object next hop: Google\n",
       "\n",
       "Therefore, the answer to the question \"Who is Dion Wiggins?\" is: Dion Wiggins is a software engineer at Google.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = kg_keyword_query_engine.query(\"Who is Dion Wiggins\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bound method Response.get_formatted_sources of Response(response=\\'\\\\n\\', source_nodes=[NodeWithScore(node=TextNode(id_=\\'aa6339d4-9cf3-4fc9-962b-68b4745371a1\\', embedding=None, metadata={\\'kg_rel_texts\\': [\"text [\\'is\\', \\'content\\']\", \"text [\\'can\\', \\'be translated\\']\", \"text [\\'is\\', \\'data\\']\"], \\'kg_rel_map\\': {\\'text\\': [[\\'is\\', \\'content\\'], [\\'can\\', \\'be translated\\'], [\\'is\\', \\'data\\']], \\'keywords\\': [], \\'10\\': [], \\'extracted\\': [], \\'Where\\': [], \\'list\\': [], \\'Where <keywords> is a list of up to 10 keywords extracted from the text.\\': []}}, excluded_embed_metadata_keys=[\\'kg_rel_map\\', \\'kg_rel_texts\\'], excluded_llm_metadata_keys=[\\'kg_rel_map\\', \\'kg_rel_texts\\'], relationships={}, hash=\\'b8f2b120a2b9a0e41f82b838823accf6aee7b68be43304547664d8690f6e701e\\', text=\"The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\\\\ntext [\\'is\\', \\'content\\']\\\\ntext [\\'can\\', \\'be translated\\']\\\\ntext [\\'is\\', \\'data\\']\", start_char_idx=None, end_char_idx=None, text_template=\\'{metadata_str}\\\\n\\\\n{content}\\', metadata_template=\\'{key}: {value}\\', metadata_seperator=\\'\\\\n\\'), score=1000.0)], metadata={\\'aa6339d4-9cf3-4fc9-962b-68b4745371a1\\': {\\'kg_rel_texts\\': [\"text [\\'is\\', \\'content\\']\", \"text [\\'can\\', \\'be translated\\']\", \"text [\\'is\\', \\'data\\']\"], \\'kg_rel_map\\': {\\'text\\': [[\\'is\\', \\'content\\'], [\\'can\\', \\'be translated\\'], [\\'is\\', \\'data\\']], \\'keywords\\': [], \\'10\\': [], \\'extracted\\': [], \\'Where\\': [], \\'list\\': [], \\'Where <keywords> is a list of up to 10 keywords extracted from the text.\\': []}}})>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response.get_formatted_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'service_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Graph Vector\u001b[39;00m\n\u001b[1;32m      2\u001b[0m storage_context_graph \u001b[39m=\u001b[39m StorageContext\u001b[39m.\u001b[39mfrom_defaults(persist_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./llama7b_graph_index_removeHTML_MaxTriplets10\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m kg_index \u001b[39m=\u001b[39m load_index_from_storage(storage_context \u001b[39m=\u001b[39m storage_context_graph, service_context \u001b[39m=\u001b[39m service_context)\n\u001b[1;32m      5\u001b[0m storage_context_vector \u001b[39m=\u001b[39m StorageContext\u001b[39m.\u001b[39mfrom_defaults(persist_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./llama7b_vector_index_removeHTML\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m vector_index \u001b[39m=\u001b[39m load_index_from_storage(storage_context \u001b[39m=\u001b[39m storage_context_vector, service_context\u001b[39m=\u001b[39mservice_context)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'service_context' is not defined"
     ]
    }
   ],
   "source": [
    "#Graph Vector\n",
    "storage_context_graph = StorageContext.from_defaults(persist_dir=\"./llama7b_graph_index_removeHTML_MaxTriplets10\")\n",
    "kg_index = load_index_from_storage(storage_context = storage_context_graph, service_context = service_context)\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir=\"./llama7b_vector_index_removeHTML\")\n",
    "vector_index = load_index_from_storage(storage_context = storage_context_vector, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My answer:\n",
      "KEYWORDS: Dion Wiggins, events, biography, life story, history, personality, achievements, awards, career, net worth."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2611.63 ms\n",
      "llama_print_timings:      sample time =    23.70 ms /    39 runs   (    0.61 ms per token,  1645.43 tokens per second)\n",
      "llama_print_timings: prompt eval time = 24835.43 ms /    86 tokens (  288.78 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:        eval time = 11331.39 ms /    38 runs   (  298.19 ms per token,     3.35 tokens per second)\n",
      "llama_print_timings:       total time = 36334.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Speaks at Symposium as Chief Scientist of Omniscien Technologies\n",
      "2. Co-Founder and CT0 of Omniscien Technologies\n",
      "3. Has a picture associated with their name\n",
      "4. Is a member of the Media & Entertainment Industry Alliance (MESA)\n",
      "5. Has a video replay available for download"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2611.63 ms\n",
      "llama_print_timings:      sample time =    48.74 ms /    77 runs   (    0.63 ms per token,  1579.94 tokens per second)\n",
      "llama_print_timings: prompt eval time = 643877.33 ms /  2120 tokens (  303.72 ms per token,     3.29 tokens per second)\n",
      "llama_print_timings:        eval time = 28787.50 ms /    77 runs   (  373.86 ms per token,     2.67 tokens per second)\n",
      "llama_print_timings:       total time = 673732.13 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "1. Speaks at Symposium as Chief Scientist of Omniscien Technologies\n",
       "2. Co-Founder and CT0 of Omniscien Technologies\n",
       "3. Has a picture associated with their name\n",
       "4. Is a member of the Media & Entertainment Industry Alliance (MESA)\n",
       "5. Has a video replay available for download</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = custom_query_engine.query(\"Tell me events about Dion Wiggins\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query with Embbeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = kg_index.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, if you were asked how many books are written by J."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mHow many book are writing by Philipp Koehn.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 23\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_index/query_engine/retriever_query_engine.py:157\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    152\u001b[0m query_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    153\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    156\u001b[0m retrieve_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_start(CBEventType\u001b[39m.\u001b[39mRETRIEVE)\n\u001b[0;32m--> 157\u001b[0m nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_end(\n\u001b[1;32m    159\u001b[0m     CBEventType\u001b[39m.\u001b[39mRETRIEVE,\n\u001b[1;32m    160\u001b[0m     payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: nodes},\n\u001b[1;32m    161\u001b[0m     event_id\u001b[39m=\u001b[39mretrieve_id,\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_synthesizer\u001b[39m.\u001b[39msynthesize(\n\u001b[1;32m    165\u001b[0m     query\u001b[39m=\u001b[39mquery_bundle,\n\u001b[1;32m    166\u001b[0m     nodes\u001b[39m=\u001b[39mnodes,\n\u001b[1;32m    167\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_index/query_engine/retriever_query_engine.py:115\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mretrieve\u001b[39m(\u001b[39mself\u001b[39m, query_bundle: QueryBundle) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m--> 115\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retriever\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[1;32m    116\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_node_postprocessors(nodes)\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_index/indices/base_retriever.py:22\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     21\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 22\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retrieve(str_or_query_bundle)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_index/indices/knowledge_graph/retriever.py:131\u001b[0m, in \u001b[0;36mKGTableRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    129\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m> Starting query: \u001b[39m\u001b[39m{\u001b[39;00mquery_bundle\u001b[39m.\u001b[39mquery_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m node_visited \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m--> 131\u001b[0m keywords \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_keywords(query_bundle\u001b[39m.\u001b[39;49mquery_str)\n\u001b[1;32m    132\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m> Query keywords: \u001b[39m\u001b[39m{\u001b[39;00mkeywords\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m rel_texts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_index/indices/knowledge_graph/retriever.py:105\u001b[0m, in \u001b[0;36mKGTableRetriever._get_keywords\u001b[0;34m(self, query_str)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_keywords\u001b[39m(\u001b[39mself\u001b[39m, query_str: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Extract keywords.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_service_context\u001b[39m.\u001b[39;49mllm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    106\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_keyword_extract_template,\n\u001b[1;32m    107\u001b[0m         max_keywords\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_keywords_per_query,\n\u001b[1;32m    108\u001b[0m         question\u001b[39m=\u001b[39;49mquery_str,\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m     keywords \u001b[39m=\u001b[39m extract_keywords_given_response(\n\u001b[1;32m    111\u001b[0m         response, start_token\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKEYWORDS:\u001b[39m\u001b[39m\"\u001b[39m, lowercase\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(keywords)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_index/llm_predictor/base.py:123\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     formatted_prompt \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[0;32m--> 123\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mcomplete(formatted_prompt)\n\u001b[1;32m    124\u001b[0m     output \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[1;32m    126\u001b[0m logger\u001b[39m.\u001b[39mdebug(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_index/llms/langchain.py:46\u001b[0m, in \u001b[0;36mLangChainLLM.complete\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomplete\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponse:\n\u001b[0;32m---> 46\u001b[0m     output_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mpredict(prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m CompletionResponse(text\u001b[39m=\u001b[39moutput_str)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/llms/base.py:469\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 469\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(text, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/llms/base.py:429\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    424\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m     )\n\u001b[1;32m    428\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    430\u001b[0m         [prompt],\n\u001b[1;32m    431\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    434\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    435\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    438\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    439\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/llms/base.py:281\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    276\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m         )\n\u001b[1;32m    278\u001b[0m     run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    279\u001b[0m         dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 281\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    282\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/llms/base.py:225\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    224\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 225\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    226\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    227\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/llms/base.py:212\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    204\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    210\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 212\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    213\u001b[0m                 prompts,\n\u001b[1;32m    214\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    215\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    216\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    217\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    218\u001b[0m             )\n\u001b[1;32m    219\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    220\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    223\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/llms/base.py:604\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    603\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 604\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    605\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    606\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    608\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/llms/llamacpp.py:229\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     combined_text_output \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream(prompt\u001b[39m=\u001b[39mprompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager):\n\u001b[1;32m    230\u001b[0m         combined_text_output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m token[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/langchain/llms/llamacpp.py:279\u001b[0m, in \u001b[0;36mLlamaCpp.stream\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m    277\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_parameters(stop)\n\u001b[1;32m    278\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient(prompt\u001b[39m=\u001b[39mprompt, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 279\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m result:\n\u001b[1;32m    280\u001b[0m     token \u001b[39m=\u001b[39m chunk[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    281\u001b[0m     log_probs \u001b[39m=\u001b[39m chunk[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_cpp/llama.py:899\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor)\u001b[0m\n\u001b[1;32m    897\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    898\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 899\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    900\u001b[0m     prompt_tokens,\n\u001b[1;32m    901\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    902\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    903\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    904\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m    905\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m    906\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m    907\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m    908\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    909\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m    910\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    911\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m    912\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    913\u001b[0m ):\n\u001b[1;32m    914\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_eos:\n\u001b[1;32m    915\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_cpp/llama.py:721\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    720\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    722\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    723\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    724\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    733\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    734\u001b[0m     )\n\u001b[1;32m    735\u001b[0m     \u001b[39mif\u001b[39;00m stopping_criteria \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    736\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids\u001b[39m.\u001b[39mtolist(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    737\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang-lama/lib/python3.9/site-packages/llama_cpp/llama.py:480\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    474\u001b[0m cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_vocab\n\u001b[1;32m    475\u001b[0m offset \u001b[39m=\u001b[39m (\n\u001b[1;32m    476\u001b[0m     \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mlogits_all \u001b[39melse\u001b[39;00m n_tokens \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    477\u001b[0m )  \u001b[39m# NOTE: Only save the last token logits if logits_all is False\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscores[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens \u001b[39m+\u001b[39m offset : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens \u001b[39m+\u001b[39m n_tokens, :]\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    479\u001b[0m     \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 480\u001b[0m )[:] \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_get_logits(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx)[: rows \u001b[39m*\u001b[39;49m cols]\n\u001b[1;32m    481\u001b[0m \u001b[39m# Update n_tokens\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m n_tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How many book are writing by Philipp Koehn.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
