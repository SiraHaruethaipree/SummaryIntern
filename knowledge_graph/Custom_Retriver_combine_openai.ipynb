{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index import(\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader,\n",
    "    LangchainEmbedding,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    "    load_graph_from_storage,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    KnowledgeGraphIndex,\n",
    "    LLMPredictor,\n",
    "    )\n",
    "\n",
    "# upload model\n",
    "from llama_index.llms import LangChainLLM\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index import (KnowledgeGraphIndex)\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=[\"2023 in science\"], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\", streaming=True))\n",
    "embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
    "#embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\",model_kwargs = {'device': 'cpu'}))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_name = \"llamaindex\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\n",
    "    \"relationship\"]  # default, could be omit if create from an empty kg\n",
    "tags = [\"entity\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sira/anaconda3/envs/lang-lama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing documents into nodes: 100%|██████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:01<00:00,  6.51it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:02<00:00,  4.22it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  9.36it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 11.86it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:01<00:00,  9.43it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:01<00:00,  8.26it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 10.89it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:02<00:00,  5.49it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:01<00:00,  6.13it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:01<00:00,  6.69it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 11.06it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 12.35it/s]\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:02<00:00,  5.15it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:01<00:00,  7.71it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:01<00:00,  6.97it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 12.30it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:01<00:00,  6.74it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:00<00:00, 10.54it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 12.62it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:01<00:00,  8.79it/s]\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:01<00:00,  9.15it/s]\n",
      "Processing nodes: 100%|██████████| 21/21 [02:10<00:00,  6.19s/it]\n"
     ]
    }
   ],
   "source": [
    "graph_store = SimpleGraphStore(space_name=space_name,edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context = storage_context,\n",
    "    service_context = service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(documents, service_context= service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "kg_keyword_query_engine = kg_index.as_query_engine(\n",
    "    # setting to false uses the raw triplets instead of adding the text from the corresponding nodes\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "NASA has been involved in many events, including the first human spaceflight in 1961, the first moon landing in 1969, the launch of the Hubble Space Telescope in 1990, and the launch of the Mars Curiosity Rover in 2011.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = kg_keyword_query_engine.query(\"Tell me events about NASA\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "NASA is expected to receive increased budgets in 2023, which will be used to fund various research topics and agencies. On 12 July, astronomers reported considerable success of the James Webb Space Telescope (JWST) after its first year of operations. On 14 July, the Indian Space Research Organisation (ISRO) successfully launched its Chandrayaan-3 spacecraft towards the Moon. On 19 July, astronomers reported the discovery of a bizarre 'two-faced' star, with one side made up of hydrogen and the other consisting of helium. On 25 July, a study published in Nature found that a collapse of the Atlantic meridional overturning circulation (AMOC) is highly likely this century, and may occur as early as 2025. On 26 July, DARPA, in collaboration with NASA, began work on the first in-orbit demonstration of a nuclear thermal rocket engine.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"Tell me events about NASA\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "NASA is scheduled to launch a Venus probe in October 2023, which will partly search for signs of life on Venus. Additionally, NASA has been provided with an increased budget for various fields, research topics, and agencies, including the new Advanced Research Projects Agency for Health (ARPA-H).</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = custom_query_engine.query(\"Tell me events about NASA\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023_Science_Wikipedia_KnowledgeGraph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"2023_Science_Wikipedia_KnowledgeGraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe2e0b93b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "g = kg_index.get_networkx_graph(200)\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "net.show(\"2023_Science_Wikipedia_KnowledgeGraph.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-lama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
