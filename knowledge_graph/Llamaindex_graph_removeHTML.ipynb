{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import(\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader,\n",
    "    LangchainEmbedding,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    "    load_graph_from_storage,\n",
    "    LLMPredictor,\n",
    "    PromptHelper\n",
    "    )\n",
    "\n",
    "# upload model\n",
    "from llama_index.llms import LangChainLLM\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index import (KnowledgeGraphIndex)\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from IPython.display import Markdown, display\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/sira/sira_project/meta-Llama2/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 7354.73 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "def load_llm():\n",
    "    n_gpu_layers = 32 \n",
    "    n_batch = 512  \n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/home/sira/sira_project/meta-Llama2/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        callback_manager=callback_manager,\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        verbose=True,\n",
    "        n_ctx = 4096, \n",
    "        temperature = 0.1, \n",
    "        max_tokens = 4096\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=LangChainLLM(llm = load_llm()))\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs = {'device': 'cpu'}))\n",
    "service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor, \n",
    "        chunk_size=1000, \n",
    "        embed_model = embed_model)\n",
    "\n",
    "#Graph Vector\n",
    "storage_context_graph = StorageContext.from_defaults(persist_dir=\"./llama7b_graph_index_removeHTML\")\n",
    "graph_index = load_index_from_storage(storage_context = storage_context_graph, service_context=service_context)\n",
    "#Index vector\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir=\"./llama7b_vector_index_removeHTML\")\n",
    "vector_index = load_index_from_storage(storage_context = storage_context_vector, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k = 5)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=graph_index, retriever_mode=\"keyword\", include_text=True, similarity_top_k = 5\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "kg_keyword_query_engine = graph_index.as_query_engine(\n",
    "    # setting to false uses the raw triplets instead of adding the text from the corresponding nodes\n",
    "    include_text=True,\n",
    "    retriever_mode=\"hybrid\",\n",
    "    response_mode=\"tree_summarize\",\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My answer:\n",
      "KEYWORDS: Dion, Wiggins, person, biography"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23650.94 ms\n",
      "llama_print_timings:      sample time =    12.40 ms /    21 runs   (    0.59 ms per token,  1693.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23439.40 ms /    83 tokens (  282.40 ms per token,     3.54 tokens per second)\n",
      "llama_print_timings:        eval time =  6253.69 ms /    20 runs   (  312.68 ms per token,     3.20 tokens per second)\n",
      "llama_print_timings:       total time = 29778.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Subject: Dion Wiggins\n",
      "Predicate: has_job\n",
      "Object: software engineer at Google\n",
      "Predicate next hop: works_at\n",
      "Object next hop: Google\n",
      "\n",
      "Subject: Dion Wiggins\n",
      "Predicate: has_education\n",
      "Object: Bachelor of Science in Computer Science from University of California, Los Angeles (UCLA)\n",
      "Predicate next hop: graduated_from\n",
      "Object next hop: University of California, Los Angeles (UCLA)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23650.94 ms\n",
      "llama_print_timings:      sample time =    56.68 ms /   102 runs   (    0.56 ms per token,  1799.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19676.23 ms /    73 tokens (  269.54 ms per token,     3.71 tokens per second)\n",
      "llama_print_timings:        eval time = 31389.22 ms /   101 runs   (  310.78 ms per token,     3.22 tokens per second)\n",
      "llama_print_timings:       total time = 51463.02 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "Subject: Dion Wiggins\n",
       "Predicate: has_job\n",
       "Object: software engineer at Google\n",
       "Predicate next hop: works_at\n",
       "Object next hop: Google\n",
       "\n",
       "Subject: Dion Wiggins\n",
       "Predicate: has_education\n",
       "Object: Bachelor of Science in Computer Science from University of California, Los Angeles (UCLA)\n",
       "Predicate next hop: graduated_from\n",
       "Object next hop: University of California, Los Angeles (UCLA)</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = kg_keyword_query_engine.query(\"Who is Dion Wiggins\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Response.get_formatted_sources of Response(response='\\n\\nSubject: Dion Wiggins\\nPredicate: has_job\\nObject: software engineer at Google\\nPredicate next hop: works_at\\nObject next hop: Google\\n\\nSubject: Dion Wiggins\\nPredicate: has_education\\nObject: Bachelor of Science in Computer Science from University of California, Los Angeles (UCLA)\\nPredicate next hop: graduated_from\\nObject next hop: University of California, Los Angeles (UCLA)', source_nodes=[NodeWithScore(node=TextNode(id_='d3b9a7cd-763e-4137-946e-219010aa2b34', embedding=None, metadata={'kg_rel_texts': [], 'kg_rel_map': {'My': [], 'KEYWORDS': [], 'answer': [], 'person': [], 'Wiggins': [], 'My answer:\\nKEYWORDS: Dion': [], 'biography': [], 'Dion': []}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_texts'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_texts'], relationships={}, hash='881dae6d82e1071e7c4737eb6d5f9fb2bb7776f2d51672bf75412a3aa6f4668f', text='The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0)], metadata={'d3b9a7cd-763e-4137-946e-219010aa2b34': {'kg_rel_texts': [], 'kg_rel_map': {'My': [], 'KEYWORDS': [], 'answer': [], 'person': [], 'Wiggins': [], 'My answer:\\nKEYWORDS: Dion': [], 'biography': [], 'Dion': []}}})>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and Philipp Koehn that are relevant to the webinar.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23650.94 ms\n",
      "llama_print_timings:      sample time =    10.16 ms /    17 runs   (    0.60 ms per token,  1673.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 1039753.10 ms /  3707 tokens (  280.48 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time =  7103.06 ms /    16 runs   (  443.94 ms per token,     2.25 tokens per second)\n",
      "llama_print_timings:       total time = 1046939.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23650.94 ms\n",
      "llama_print_timings:      sample time =     2.97 ms /     5 runs   (    0.59 ms per token,  1684.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 482379.35 ms /  1872 tokens (  257.68 ms per token,     3.88 tokens per second)\n",
      "llama_print_timings:        eval time =  1411.45 ms /     4 runs   (  352.86 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time = 483811.54 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "\n",
       "\n",
       "</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"Tell me events about Dion Wiggins\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Response.get_formatted_sources of Response(response='\\n\\n\\n\\n', source_nodes=[NodeWithScore(node=TextNode(id_='720a1017-8d9a-40fc-bc92-a6ddc931cac6', embedding=None, metadata={'file_name': 'omniscien.com/resources/webinars.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='773551c9-d229-40c0-b40d-41b4567870a4', node_type=None, metadata={'file_name': 'omniscien.com/resources/webinars.html'}, hash='33eba177902eb2a9c79ad4313cce9d9817cd115b14725d0266b8fb241a9ab4cd'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='995c40f4-62b7-440e-91f4-2f489d346293', node_type=None, metadata={'file_name': 'omniscien.com/resources/webinars.html'}, hash='4576a6856c60aec9bf65b6b5665afdc32ea61cfab0dea41eb0b57be89490bd16'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b5b93aa1-6e8b-4ba5-b819-3fe9a1f742e6', node_type=None, metadata={'file_name': 'omniscien.com/resources/webinars.html'}, hash='1982a4dc5a09b9de78763c41f5dfb391fe4c5ebda5ba487a7932646c87076012')}, hash='34c58405e8ae8b27276f254fe10f5355cf8871d3f1e6d11b7c5357312d369b18', text='Recognition** , **Data Synthesis** , **Natural\\nLanguage Processing** , **Machine Learning** , **Data Mining** , **Text\\nAnalytics** , and more.\\n\\nSpeakers|  \\n---|---  \\n![Philipp\\nKoehn](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Philipp\\nKoehn](https://docs.omniscien.com/wp-\\ncontent/uploads/2022/07/Koehn-1721-150x1501-1.jpg)| ![Dion\\nWiggins](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Dion\\nWiggins](https://docs.omniscien.com/wp-\\ncontent/uploads/2022/07/Dion_1501-150x1501-1.jpg)  \\nProfessor Philipp Koehn  \\nChief Scientist,  \\nOmniscien Technologies| Dion Wiggins  \\nCTO, Co-Founder,  \\nOmniscien Technologies  \\n  \\nLarge Language Models (such as ChatGPT, Machine Translation (MT) and Automated\\nSpeech Recognition (ASR) are three of the many different Artificial\\nIntelligence (AI) technologies today that use Transformer models, so their\\ndomain adaptation (aka customization) processes have a lot in common. Domain\\nAdaptation (also known as customization) is the adaptation of AI models used\\nin MT and ASR to a specific domain or purpose such as automotive legal\\ndocumentation, medical discussions, or investment portfolio sales.\\n\\nThis webinar will demonstrate how Omniscien’s team gathers and synthesizes\\nbillions of sentences of training data that is used to teach and adapt AI\\nsystems. We will explore best practices and explain the limitations of\\ntraditional approaches. We will show how to use AI to create many millions of\\nsentences of training data that can be used as part of domain adaptation.\\nEssentially, this means using one form of AI to teach another AI form new\\nknowledge.\\n\\nModern AI systems are built on foundation models containing billions of\\nparameters and need modern approaches to creating training data. New\\napproaches are needed. The legacy historical approaches of simply uploading a\\nfew thousand sentences of data are insufficient. Some simple examples are\\nlisted below:10K-500K sentences that an organization can typically provide are\\ninsufficient to have any notable impact against billions of parameters used in\\nmodern foundation models.\\n\\n  * Translation Management Systems (TMS) such as memoQ, Trados Studio, XTM, etc., do not store translation memories in optimal formats for training modern AI algorithms. Translation memories are not in sequence of the original document and are de-duplicated making them unsuitable for training document level translation systems where entire bilingual documents in sequence are needed to obtain the correct context.\\n  * Language Service Providers (LSPs) and translation departments may keep translation memories classified by organisation or customer, but seldom by writing style or purpose. Content from sales, marketing, engineering, legal and other areas are all mixed together.\\n  * Glossaries alone are absolute and result in over-fitting in the wrong context. Many MT systems using glossaries simply search and replace rather than use the glossary in context and the use of the glossary is not 100% reliable.\\n\\nWe will also explore how the advantages that LSPs and large organizations have\\nhad in the past by having “larger amounts of training data” on file are very\\nquickly becoming irrelevant in the new redefined approach to domain adaptation\\nwhere tens or evens hundreds of millions of sentences are needed to have any\\nmeaningful effect. Fortunately, there are already solutions that address this\\nissue by enabling any organization to create this data quickly and cost\\neffectively.\\n\\n[Watch the', start_char_idx=17539, end_char_idx=21078, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4105482468091183), NodeWithScore(node=TextNode(id_='2751878d-0947-4314-8f51-7a16720a82ee', embedding=None, metadata={'file_name': 'omniscien.com/symposium/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0ec212a1-c7d3-4832-8508-2ded9861cb68', node_type=None, metadata={'file_name': 'omniscien.com/symposium/index.html'}, hash='20460cb18897fad7376b9fabf3446ea5ccc42862490fbb6e40340d0bcbe88af4'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8c05a83d-d942-4e0f-9907-5c07955f0687', node_type=None, metadata={'file_name': 'omniscien.com/symposium/index.html'}, hash='1e46e227ea685489b06b80b99092956244c556c30708b4e342edc82bb786b49b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='562c09b9-34ed-475d-9583-8bd380b58bda', node_type=None, metadata={'file_name': 'omniscien.com/symposium/index.html'}, hash='208275b45798a61f3201aa249e508b9b7acb35a7d88b943a551d21fdba9bea2d')}, hash='2bdee9dfc38b66b6c9e91e4ce7243d6eba3650474f919556c33854837c957386', text='are now available.\\n\\n### Speakers From:\\n\\n![Symposium Speaker Company Logos](http://symposium.omniscien.com/wp-\\ncontent/uploads/2020/06/SymposiumBrands.jpg)\\n\\n### Day 1 - Wednesday 24 June 2020\\n\\nKEYNOTE  \\nW1 - Found in Translation - Language Meets Technology  \\n**Dion Wiggins** - Omniscien Technologies![View session overview, speaker bio,\\nvideo replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nW2 - Leadership Effectiveness Enabled by Artificial Intelligence  \\n**Viktor Morovic** - KeenCorp![View session overview, speaker bio, video\\nreplay and download presentation slides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nW3 - On-Premises Secure Translation and Workflow Automation  \\n**Martine Massiera** - Omniscien Technologies![View session overview, speaker\\nbio, video replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nW4 - Research in Translation - What Is Exciting and Shows Promise Ahead?  \\n**Philipp Koehn** - Omniscien Technologies / Johns Hopkins University![View\\nsession overview, speaker bio, video replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nEXPERT PANEL  \\nW5 - The Evolving Requirements of the Localisation Industry  \\nMedia & Entertainment Industry Alliance (MESA) Members  \\n**Hosted by MESA** and joined by **Scott Rose** - SDI Media,  \\n**Greg Taieb** - Deluxe Entertainment and **Julian Day** - Zoo Digital\\n\\n![View session overview, speaker bio, video replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio and Video Replay\\n\\nW6 - AI Meets Media Processing and Workflows - Media Studio 2.0 Preview  \\n**Dion Wiggins** - Omniscien Technologies  \\n **Alphie Larrieu** - Astro (Case Study)![View session overview, speaker bio,\\nvideo replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\n### Day 2 - Thursday 25 June 2020\\n\\nKEYNOTE  \\nT1 - The AI-Fueled Organisation  \\n**Bob Hayward** - Deloitte Development LLC![View session overview, speaker\\nbio, video replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nT2 - Secrets to Customizing a High Quality Machine Translation Engine  \\n**Dion Wiggins** - Omniscien Technologies, **Philipp Koehn** - Omniscien\\nTechnologies / Johns Hopkins University![View session overview, speaker bio,\\nvideo replay and download', start_char_idx=5947, end_char_idx=9178, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.2879989261588695), NodeWithScore(node=TextNode(id_='3cacd607-a0ee-46cf-893f-3866b8d0dc27', embedding=None, metadata={'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8dfee464-df01-416a-b675-c027d6329721', node_type=None, metadata={'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, hash='c692c8910d097f91ff4faa1a25c0463e83749937baee26abd24fd650cb4e516d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b4c92cea-31be-4981-8832-fb2a8dce08ea', node_type=None, metadata={'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, hash='5a76abd3369b018fda4ba1ac1060ceb9d2f5178b9ab30650acbe382804d74e5a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8e20e25b-2af3-417a-81c2-80fbae4646b6', node_type=None, metadata={'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, hash='399cd43c29778fbaebd5bc9d9d52c87655916654b98b64b85ffb6ca53549deb0')}, hash='66ea77544b5d8f561b87d75fc7818985b84546f3d32c2cc13dc705a1fc27f90f', text='   * [Supported Document Formats](https://omniscien.com/machine-translation/supported-document-formats/)\\n    * [Ways to Translate](https://omniscien.com/machine-translation/ways-to-translate/)\\n    * [Data Security and Privacy](https://omniscien.com/machine-translation/data-security-and-privacy/)\\n\\n__ Search\\n\\n# Thailand Joint Foreign Chambers of Commerce - AI and Machine Learning -\\nTrends and Caveats\\n\\nMay 11, 2023\\n\\nOmniscien\\'s CTO and Co-Founder Dion Wiggins will be speaking at the Thailand\\n**Joint Foreign Chambers of Commerce** \\'s event **\" AI and Machine Learning\\n—Trends and Caveats\"** on May 25.\\n\\nPlease follow this link for more information and registration.\\n<https://www.jfcct.org/ai-and-machine-learning-trends-and-caveats-2023-25th-\\nmay/>\\n\\n[![](https://www.jfcct.org/wp-content/uploads/2023/05/AI-and-Machine-Learning-\\nTrends-and-Caveats-5-2048x1448-1.png)](https://www.jfcct.org/wp-\\ncontent/uploads/2023/05/AI-and-Machine-Learning-Trends-and-\\nCaveats-5-2048x1448-1.png)\\n\\nArtificial Intelligence has been with us for decades. Its uses have become\\nwidely embedded to make assessments, predictions, various decisions and has\\nother functions.\\n\\nBut the way AI is programmed has become controversial. Ethical AI or similarly\\nnamed approaches have become Guidelines and even legislation. (Ethical AI is\\nartificial intelligence which adheres to well-defined ethical guidelines\\nregarding fundamental values, including such things as individual rights,\\nprivacy, non-discrimination, and non-manipulation).\\n\\nGenerative AI (eg OpenAI’s ChatGPT) became popularly known only in 2022. It is\\nable to generate original looking content such as writing a term paper, a\\nspeech, doing a translation etc. However there is a dark side. False\\ninformation (fake news) and other bad outcomes have occurred resulting in\\ndefamation suits and other remedies. In response, some companies have devised\\npreventions and filters. A big question in academe is to ensure originality in\\nstudent work.\\n\\nMachine Learning is understanding and building methods that let machines\\n\"learn\" – that is, methods that leverage data to improve computer performance\\non some set of tasks. Training data teaches the machine. It can be seen as a\\nsubset of AI.\\n\\nJoin the Spanish-Thai Chamber and JFCCT’s Digital Economy/ICT group for a dive\\ninto understanding trends, issues and developments. What are the main business\\nand personal uses? How, broadly does the technology work? What are the ethical\\nrequirements and issues? What should businesses do to benefit and operate\\nethically?\\n\\nSpeakers and Moderator: AI including Generative AI; Ethical AI; Machine\\nLearning  \\n• [Pedro\\nUria](https://drive.google.com/file/d/1HHsMP7hzINz6EKgujNUvtY1Girr2sKyj/view)\\n– head of data sciences for True  \\n• [Dr Pin Pin Tea-Makon](https://sasin.edu/profile/pinnaree-tea-makorn-ph-d),\\nSasin School of Management, lectures in AI and in Business.  \\n• [Dion Wiggins](https://omniscien.com/about-us/company/) – co-founder and CTO\\nof Omniscien Technologies (Bangkok-based) (<https://omniscien.com/>)  \\nOmniscien covers generative', start_char_idx=7661, end_char_idx=10726, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.25884758538512587), NodeWithScore(node=TextNode(id_='09b0df30-4141-4580-bbe2-d91f32ba7be4', embedding=None, metadata={'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7f0ec76f-1b77-49eb-b1b9-6458595c795e', node_type=None, metadata={'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, hash='33eba177902eb2a9c79ad4313cce9d9817cd115b14725d0266b8fb241a9ab4cd'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c3972496-ad1c-4789-9933-2f7f8b979ef0', node_type=None, metadata={'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, hash='847fec9f75472d8f5ab725c55470c6d9bb826ebabc7083c5694aff9b9d98f2b3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c09a6fb8-3a88-4112-9714-b846e7dd6b7d', node_type=None, metadata={'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, hash='b3c4bd618327192154ed3c3d1cd37356bb03c2e2b2abc906285e8a069b7edf4a')}, hash='db52f00835fe2690a85be517a3e9685347bd11dad925fe889a0df5bf5aa55556', text='tools and technologies to arm you with the essential knowledge on how to\\nperform domain adaptation on modern AI systems.\\n\\nWe will look at trends in **Artificial Intelligence, Neural Machine\\nTranslation** , **Speech Recognition** , **Data Synthesis** , **Natural\\nLanguage Processing** , **Machine Learning** , **Data Mining** , **Text\\nAnalytics** , and more.\\n\\nSpeakers|  \\n---|---  \\n![Philipp\\nKoehn](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Philipp\\nKoehn](https://docs.omniscien.com/wp-\\ncontent/uploads/2022/07/Koehn-1721-150x1501-1.jpg)| ![Dion\\nWiggins](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Dion\\nWiggins](https://docs.omniscien.com/wp-\\ncontent/uploads/2022/07/Dion_1501-150x1501-1.jpg)  \\nProfessor Philipp Koehn  \\nChief Scientist,  \\nOmniscien Technologies| Dion Wiggins  \\nCTO, Co-Founder,  \\nOmniscien Technologies  \\n  \\nLarge Language Models (such as ChatGPT, Machine Translation (MT) and Automated\\nSpeech Recognition (ASR) are three of the many different Artificial\\nIntelligence (AI) technologies today that use Transformer models, so their\\ndomain adaptation (aka customization) processes have a lot in common. Domain\\nAdaptation (also known as customization) is the adaptation of AI models used\\nin MT and ASR to a specific domain or purpose such as automotive legal\\ndocumentation, medical discussions, or investment portfolio sales.\\n\\nThis webinar will demonstrate how Omniscien’s team gathers and synthesizes\\nbillions of sentences of training data that is used to teach and adapt AI\\nsystems. We will explore best practices and explain the limitations of\\ntraditional approaches. We will show how to use AI to create many millions of\\nsentences of training data that can be used as part of domain adaptation.\\nEssentially, this means using one form of AI to teach another AI form new\\nknowledge.\\n\\nModern AI systems are built on foundation models containing billions of\\nparameters and need modern approaches to creating training data. New\\napproaches are needed. The legacy historical approaches of simply uploading a\\nfew thousand sentences of data are insufficient. Some simple examples are\\nlisted below:10K-500K sentences that an organization can typically provide are\\ninsufficient to have any notable impact against billions of parameters used in\\nmodern foundation models.\\n\\n  * Translation Management Systems (TMS) such as memoQ, Trados Studio, XTM, etc., do not store translation memories in optimal formats for training modern AI algorithms. Translation memories are not in sequence of the original document and are de-duplicated making them unsuitable for training document level translation systems where entire bilingual documents in sequence are needed to obtain the correct context.\\n  * Language Service Providers (LSPs) and translation departments may keep translation memories classified by organisation or customer, but seldom by writing style or purpose. Content from sales, marketing, engineering, legal and other areas are all mixed together.\\n  * Glossaries alone are absolute and result in over-fitting in the wrong context. Many MT systems using glossaries simply search and replace rather than use the glossary in context and the use of the glossary is not 100% reliable.\\n\\nWe will also explore how the advantages that LSPs and large organizations have\\nhad in the past by having “larger amounts of training data” on file are very\\nquickly becoming irrelevant in the new redefined approach to domain adaptation\\nwhere tens or', start_char_idx=17319, end_char_idx=20835, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.24037372890428002), NodeWithScore(node=TextNode(id_='f6dd6ac9-f226-4268-aa6d-41aba2f41609', embedding=None, metadata={'file_name': 'omniscien.com/about-us/news/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e9e3c801-e6c1-498b-8fab-e5fbd9aeb664', node_type=None, metadata={'file_name': 'omniscien.com/about-us/news/index.html'}, hash='6381de11daa4c805db5cf2a774d1490cfbd91d33faae3d5d55c5f59e3a7f0e85'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='71c073c9-c233-4c97-836b-b42f3ed7eed7', node_type=None, metadata={'file_name': 'omniscien.com/about-us/news/index.html'}, hash='44f0d9c8d32de0b35c2b2665c66072362009caee55aa8d99e87bd5297d233b97'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b39f3036-6615-4678-8c60-cede2fc8abb2', node_type=None, metadata={'file_name': 'omniscien.com/about-us/news/index.html'}, hash='b18be91ee388732c79484171dc4c008e2bfa82f9e069654f0f85f9faf3851b60')}, hash='d7cde0cca03917eae464a95694d9fbd3bf9ccee493edd244913a12b351772c3a', text='   * [Data Security and Privacy](https://omniscien.com/machine-translation/data-security-and-privacy/)\\n\\n__ Search\\n\\n[Omniscien](https://omniscien.com/) » [About Us](https://omniscien.com/about-\\nus/) » News\\n\\n#  News , Announcements, and Media Coverage\\n\\n## [NAB Show 2019: The 13 Innovative Media Companies to\\nWatch](https://omniscien.com/news/nab-show-2019-the-13-innovative-media-\\ncompanies-to-watch/)\\n\\nMay 3, 2019\\n\\nForbes: NAB Show: The 13 Innovative Media Companies to WatchOmniscien was\\nlisted in Forbes as one of 13 innovative media companies to watch while\\nattending NAB Show in Las Vegas Read more here:...\\n\\n[read more](https://omniscien.com/news/nab-show-2019-the-13-innovative-media-\\ncompanies-to-watch/)\\n\\n## [Omniscien Technologies Releases Next-Generation, Machine Learning Based\\nDeep Neural Machine Translation (Deep\\nNMT)](https://omniscien.com/news/omniscien-technologies-releases-next-\\ngeneration-machine-learning-based-deep-neural-machine-translation-deep-nmt/)\\n\\nOct 18, 2017\\n\\nOmniscien Technologies Releases Next-Generation, Machine Learning Based Deep\\nNeural Machine Translation (Deep NMT) SINGAPORE – October 19, 2017 – Omniscien\\nTechnologies releases Next-Generation, Machine Learning Based Deep Neural\\nMachine Translation (Deep NMT). With...\\n\\n[read more](https://omniscien.com/news/omniscien-technologies-releases-next-\\ngeneration-machine-learning-based-deep-neural-machine-translation-deep-nmt/)\\n\\n## [Omniscien Technologies Releases Next-Generation REST API and Language\\nScripting Toolkit for Language Studio\\nCloud](https://omniscien.com/news/omniscien-technologies-releases-next-\\ngeneration-rest-api-and-language-scripting-toolkit-for-language-studio-cloud/)\\n\\nJul 18, 2017\\n\\nOmniscien Technologies Releases Next-Generation REST API and Language\\nScripting Toolkit for Language Studio Cloud SINGAPORE – July 18, 2017 –\\nOmniscien Technologies (formerly Asia Online) today announces the release of\\nits next-generation REST Application Programming...\\n\\n[read more](https://omniscien.com/news/omniscien-technologies-releases-next-\\ngeneration-rest-api-and-language-scripting-toolkit-for-language-studio-cloud/)\\n\\n## [Omniscien Technologies Announces Partnership with LexisNexis to Deploy\\nLanguage Studio Neural Machine\\nTranslation](https://omniscien.com/news/omniscien-technologies-announces-\\npartnership-with-lexisnexis-to-deploy-language-studio-neural-machine-\\ntranslation/)\\n\\nApr 27, 2017\\n\\nOmniscien Technologies Announces Partnership with LexisNexis to Deploy\\nLanguage Studio Neural Machine Translation SINGAPORE – April 27, 2017 –\\nOmniscien Technologies (formerly Asia Online) today announced a partnership\\nwith LexisNexis to deploy Language Studio Neural...\\n\\n[read more](https://omniscien.com/news/omniscien-technologies-announces-\\npartnership-with-lexisnexis-to-deploy-language-studio-neural-machine-\\ntranslation/)\\n\\n## [Omniscien Technologies Announces Release of Language Studio with Next-\\nGeneration Neural Machine', start_char_idx=7886, end_char_idx=10818, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.21720125208652655)], metadata={'720a1017-8d9a-40fc-bc92-a6ddc931cac6': {'file_name': 'omniscien.com/resources/webinars.html'}, '2751878d-0947-4314-8f51-7a16720a82ee': {'file_name': 'omniscien.com/symposium/index.html'}, '3cacd607-a0ee-46cf-893f-3866b8d0dc27': {'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, '09b0df30-4141-4580-bbe2-d91f32ba7be4': {'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, 'f6dd6ac9-f226-4268-aa6d-41aba2f41609': {'file_name': 'omniscien.com/about-us/news/index.html'}})>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\n\\n\\n\\n', source_nodes=[NodeWithScore(node=TextNode(id_='720a1017-8d9a-40fc-bc92-a6ddc931cac6', embedding=None, metadata={'file_name': 'omniscien.com/resources/webinars.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='773551c9-d229-40c0-b40d-41b4567870a4', node_type=None, metadata={'file_name': 'omniscien.com/resources/webinars.html'}, hash='33eba177902eb2a9c79ad4313cce9d9817cd115b14725d0266b8fb241a9ab4cd'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='995c40f4-62b7-440e-91f4-2f489d346293', node_type=None, metadata={'file_name': 'omniscien.com/resources/webinars.html'}, hash='4576a6856c60aec9bf65b6b5665afdc32ea61cfab0dea41eb0b57be89490bd16'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b5b93aa1-6e8b-4ba5-b819-3fe9a1f742e6', node_type=None, metadata={'file_name': 'omniscien.com/resources/webinars.html'}, hash='1982a4dc5a09b9de78763c41f5dfb391fe4c5ebda5ba487a7932646c87076012')}, hash='34c58405e8ae8b27276f254fe10f5355cf8871d3f1e6d11b7c5357312d369b18', text='Recognition** , **Data Synthesis** , **Natural\\nLanguage Processing** , **Machine Learning** , **Data Mining** , **Text\\nAnalytics** , and more.\\n\\nSpeakers|  \\n---|---  \\n![Philipp\\nKoehn](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Philipp\\nKoehn](https://docs.omniscien.com/wp-\\ncontent/uploads/2022/07/Koehn-1721-150x1501-1.jpg)| ![Dion\\nWiggins](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Dion\\nWiggins](https://docs.omniscien.com/wp-\\ncontent/uploads/2022/07/Dion_1501-150x1501-1.jpg)  \\nProfessor Philipp Koehn  \\nChief Scientist,  \\nOmniscien Technologies| Dion Wiggins  \\nCTO, Co-Founder,  \\nOmniscien Technologies  \\n  \\nLarge Language Models (such as ChatGPT, Machine Translation (MT) and Automated\\nSpeech Recognition (ASR) are three of the many different Artificial\\nIntelligence (AI) technologies today that use Transformer models, so their\\ndomain adaptation (aka customization) processes have a lot in common. Domain\\nAdaptation (also known as customization) is the adaptation of AI models used\\nin MT and ASR to a specific domain or purpose such as automotive legal\\ndocumentation, medical discussions, or investment portfolio sales.\\n\\nThis webinar will demonstrate how Omniscien’s team gathers and synthesizes\\nbillions of sentences of training data that is used to teach and adapt AI\\nsystems. We will explore best practices and explain the limitations of\\ntraditional approaches. We will show how to use AI to create many millions of\\nsentences of training data that can be used as part of domain adaptation.\\nEssentially, this means using one form of AI to teach another AI form new\\nknowledge.\\n\\nModern AI systems are built on foundation models containing billions of\\nparameters and need modern approaches to creating training data. New\\napproaches are needed. The legacy historical approaches of simply uploading a\\nfew thousand sentences of data are insufficient. Some simple examples are\\nlisted below:10K-500K sentences that an organization can typically provide are\\ninsufficient to have any notable impact against billions of parameters used in\\nmodern foundation models.\\n\\n  * Translation Management Systems (TMS) such as memoQ, Trados Studio, XTM, etc., do not store translation memories in optimal formats for training modern AI algorithms. Translation memories are not in sequence of the original document and are de-duplicated making them unsuitable for training document level translation systems where entire bilingual documents in sequence are needed to obtain the correct context.\\n  * Language Service Providers (LSPs) and translation departments may keep translation memories classified by organisation or customer, but seldom by writing style or purpose. Content from sales, marketing, engineering, legal and other areas are all mixed together.\\n  * Glossaries alone are absolute and result in over-fitting in the wrong context. Many MT systems using glossaries simply search and replace rather than use the glossary in context and the use of the glossary is not 100% reliable.\\n\\nWe will also explore how the advantages that LSPs and large organizations have\\nhad in the past by having “larger amounts of training data” on file are very\\nquickly becoming irrelevant in the new redefined approach to domain adaptation\\nwhere tens or evens hundreds of millions of sentences are needed to have any\\nmeaningful effect. Fortunately, there are already solutions that address this\\nissue by enabling any organization to create this data quickly and cost\\neffectively.\\n\\n[Watch the', start_char_idx=17539, end_char_idx=21078, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4105482468091183), NodeWithScore(node=TextNode(id_='2751878d-0947-4314-8f51-7a16720a82ee', embedding=None, metadata={'file_name': 'omniscien.com/symposium/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0ec212a1-c7d3-4832-8508-2ded9861cb68', node_type=None, metadata={'file_name': 'omniscien.com/symposium/index.html'}, hash='20460cb18897fad7376b9fabf3446ea5ccc42862490fbb6e40340d0bcbe88af4'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8c05a83d-d942-4e0f-9907-5c07955f0687', node_type=None, metadata={'file_name': 'omniscien.com/symposium/index.html'}, hash='1e46e227ea685489b06b80b99092956244c556c30708b4e342edc82bb786b49b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='562c09b9-34ed-475d-9583-8bd380b58bda', node_type=None, metadata={'file_name': 'omniscien.com/symposium/index.html'}, hash='208275b45798a61f3201aa249e508b9b7acb35a7d88b943a551d21fdba9bea2d')}, hash='2bdee9dfc38b66b6c9e91e4ce7243d6eba3650474f919556c33854837c957386', text='are now available.\\n\\n### Speakers From:\\n\\n![Symposium Speaker Company Logos](http://symposium.omniscien.com/wp-\\ncontent/uploads/2020/06/SymposiumBrands.jpg)\\n\\n### Day 1 - Wednesday 24 June 2020\\n\\nKEYNOTE  \\nW1 - Found in Translation - Language Meets Technology  \\n**Dion Wiggins** - Omniscien Technologies![View session overview, speaker bio,\\nvideo replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nW2 - Leadership Effectiveness Enabled by Artificial Intelligence  \\n**Viktor Morovic** - KeenCorp![View session overview, speaker bio, video\\nreplay and download presentation slides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nW3 - On-Premises Secure Translation and Workflow Automation  \\n**Martine Massiera** - Omniscien Technologies![View session overview, speaker\\nbio, video replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nW4 - Research in Translation - What Is Exciting and Shows Promise Ahead?  \\n**Philipp Koehn** - Omniscien Technologies / Johns Hopkins University![View\\nsession overview, speaker bio, video replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nEXPERT PANEL  \\nW5 - The Evolving Requirements of the Localisation Industry  \\nMedia & Entertainment Industry Alliance (MESA) Members  \\n**Hosted by MESA** and joined by **Scott Rose** - SDI Media,  \\n**Greg Taieb** - Deluxe Entertainment and **Julian Day** - Zoo Digital\\n\\n![View session overview, speaker bio, video replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio and Video Replay\\n\\nW6 - AI Meets Media Processing and Workflows - Media Studio 2.0 Preview  \\n**Dion Wiggins** - Omniscien Technologies  \\n **Alphie Larrieu** - Astro (Case Study)![View session overview, speaker bio,\\nvideo replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\n### Day 2 - Thursday 25 June 2020\\n\\nKEYNOTE  \\nT1 - The AI-Fueled Organisation  \\n**Bob Hayward** - Deloitte Development LLC![View session overview, speaker\\nbio, video replay and download presentation\\nslides.](http://symposium.omniscien.com/wp-\\ncontent/uploads/2019/03/DownloadPresentation.png)\\n\\n#### View Session Overview, Speaker Bio, Video Replay and Download\\nPresentation Slides\\n\\nT2 - Secrets to Customizing a High Quality Machine Translation Engine  \\n**Dion Wiggins** - Omniscien Technologies, **Philipp Koehn** - Omniscien\\nTechnologies / Johns Hopkins University![View session overview, speaker bio,\\nvideo replay and download', start_char_idx=5947, end_char_idx=9178, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.2879989261588695), NodeWithScore(node=TextNode(id_='3cacd607-a0ee-46cf-893f-3866b8d0dc27', embedding=None, metadata={'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8dfee464-df01-416a-b675-c027d6329721', node_type=None, metadata={'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, hash='c692c8910d097f91ff4faa1a25c0463e83749937baee26abd24fd650cb4e516d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b4c92cea-31be-4981-8832-fb2a8dce08ea', node_type=None, metadata={'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, hash='5a76abd3369b018fda4ba1ac1060ceb9d2f5178b9ab30650acbe382804d74e5a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8e20e25b-2af3-417a-81c2-80fbae4646b6', node_type=None, metadata={'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, hash='399cd43c29778fbaebd5bc9d9d52c87655916654b98b64b85ffb6ca53549deb0')}, hash='66ea77544b5d8f561b87d75fc7818985b84546f3d32c2cc13dc705a1fc27f90f', text='   * [Supported Document Formats](https://omniscien.com/machine-translation/supported-document-formats/)\\n    * [Ways to Translate](https://omniscien.com/machine-translation/ways-to-translate/)\\n    * [Data Security and Privacy](https://omniscien.com/machine-translation/data-security-and-privacy/)\\n\\n__ Search\\n\\n# Thailand Joint Foreign Chambers of Commerce - AI and Machine Learning -\\nTrends and Caveats\\n\\nMay 11, 2023\\n\\nOmniscien\\'s CTO and Co-Founder Dion Wiggins will be speaking at the Thailand\\n**Joint Foreign Chambers of Commerce** \\'s event **\" AI and Machine Learning\\n—Trends and Caveats\"** on May 25.\\n\\nPlease follow this link for more information and registration.\\n<https://www.jfcct.org/ai-and-machine-learning-trends-and-caveats-2023-25th-\\nmay/>\\n\\n[![](https://www.jfcct.org/wp-content/uploads/2023/05/AI-and-Machine-Learning-\\nTrends-and-Caveats-5-2048x1448-1.png)](https://www.jfcct.org/wp-\\ncontent/uploads/2023/05/AI-and-Machine-Learning-Trends-and-\\nCaveats-5-2048x1448-1.png)\\n\\nArtificial Intelligence has been with us for decades. Its uses have become\\nwidely embedded to make assessments, predictions, various decisions and has\\nother functions.\\n\\nBut the way AI is programmed has become controversial. Ethical AI or similarly\\nnamed approaches have become Guidelines and even legislation. (Ethical AI is\\nartificial intelligence which adheres to well-defined ethical guidelines\\nregarding fundamental values, including such things as individual rights,\\nprivacy, non-discrimination, and non-manipulation).\\n\\nGenerative AI (eg OpenAI’s ChatGPT) became popularly known only in 2022. It is\\nable to generate original looking content such as writing a term paper, a\\nspeech, doing a translation etc. However there is a dark side. False\\ninformation (fake news) and other bad outcomes have occurred resulting in\\ndefamation suits and other remedies. In response, some companies have devised\\npreventions and filters. A big question in academe is to ensure originality in\\nstudent work.\\n\\nMachine Learning is understanding and building methods that let machines\\n\"learn\" – that is, methods that leverage data to improve computer performance\\non some set of tasks. Training data teaches the machine. It can be seen as a\\nsubset of AI.\\n\\nJoin the Spanish-Thai Chamber and JFCCT’s Digital Economy/ICT group for a dive\\ninto understanding trends, issues and developments. What are the main business\\nand personal uses? How, broadly does the technology work? What are the ethical\\nrequirements and issues? What should businesses do to benefit and operate\\nethically?\\n\\nSpeakers and Moderator: AI including Generative AI; Ethical AI; Machine\\nLearning  \\n• [Pedro\\nUria](https://drive.google.com/file/d/1HHsMP7hzINz6EKgujNUvtY1Girr2sKyj/view)\\n– head of data sciences for True  \\n• [Dr Pin Pin Tea-Makon](https://sasin.edu/profile/pinnaree-tea-makorn-ph-d),\\nSasin School of Management, lectures in AI and in Business.  \\n• [Dion Wiggins](https://omniscien.com/about-us/company/) – co-founder and CTO\\nof Omniscien Technologies (Bangkok-based) (<https://omniscien.com/>)  \\nOmniscien covers generative', start_char_idx=7661, end_char_idx=10726, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.25884758538512587), NodeWithScore(node=TextNode(id_='09b0df30-4141-4580-bbe2-d91f32ba7be4', embedding=None, metadata={'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7f0ec76f-1b77-49eb-b1b9-6458595c795e', node_type=None, metadata={'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, hash='33eba177902eb2a9c79ad4313cce9d9817cd115b14725d0266b8fb241a9ab4cd'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c3972496-ad1c-4789-9933-2f7f8b979ef0', node_type=None, metadata={'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, hash='847fec9f75472d8f5ab725c55470c6d9bb826ebabc7083c5694aff9b9d98f2b3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c09a6fb8-3a88-4112-9714-b846e7dd6b7d', node_type=None, metadata={'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, hash='b3c4bd618327192154ed3c3d1cd37356bb03c2e2b2abc906285e8a069b7edf4a')}, hash='db52f00835fe2690a85be517a3e9685347bd11dad925fe889a0df5bf5aa55556', text='tools and technologies to arm you with the essential knowledge on how to\\nperform domain adaptation on modern AI systems.\\n\\nWe will look at trends in **Artificial Intelligence, Neural Machine\\nTranslation** , **Speech Recognition** , **Data Synthesis** , **Natural\\nLanguage Processing** , **Machine Learning** , **Data Mining** , **Text\\nAnalytics** , and more.\\n\\nSpeakers|  \\n---|---  \\n![Philipp\\nKoehn](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Philipp\\nKoehn](https://docs.omniscien.com/wp-\\ncontent/uploads/2022/07/Koehn-1721-150x1501-1.jpg)| ![Dion\\nWiggins](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Dion\\nWiggins](https://docs.omniscien.com/wp-\\ncontent/uploads/2022/07/Dion_1501-150x1501-1.jpg)  \\nProfessor Philipp Koehn  \\nChief Scientist,  \\nOmniscien Technologies| Dion Wiggins  \\nCTO, Co-Founder,  \\nOmniscien Technologies  \\n  \\nLarge Language Models (such as ChatGPT, Machine Translation (MT) and Automated\\nSpeech Recognition (ASR) are three of the many different Artificial\\nIntelligence (AI) technologies today that use Transformer models, so their\\ndomain adaptation (aka customization) processes have a lot in common. Domain\\nAdaptation (also known as customization) is the adaptation of AI models used\\nin MT and ASR to a specific domain or purpose such as automotive legal\\ndocumentation, medical discussions, or investment portfolio sales.\\n\\nThis webinar will demonstrate how Omniscien’s team gathers and synthesizes\\nbillions of sentences of training data that is used to teach and adapt AI\\nsystems. We will explore best practices and explain the limitations of\\ntraditional approaches. We will show how to use AI to create many millions of\\nsentences of training data that can be used as part of domain adaptation.\\nEssentially, this means using one form of AI to teach another AI form new\\nknowledge.\\n\\nModern AI systems are built on foundation models containing billions of\\nparameters and need modern approaches to creating training data. New\\napproaches are needed. The legacy historical approaches of simply uploading a\\nfew thousand sentences of data are insufficient. Some simple examples are\\nlisted below:10K-500K sentences that an organization can typically provide are\\ninsufficient to have any notable impact against billions of parameters used in\\nmodern foundation models.\\n\\n  * Translation Management Systems (TMS) such as memoQ, Trados Studio, XTM, etc., do not store translation memories in optimal formats for training modern AI algorithms. Translation memories are not in sequence of the original document and are de-duplicated making them unsuitable for training document level translation systems where entire bilingual documents in sequence are needed to obtain the correct context.\\n  * Language Service Providers (LSPs) and translation departments may keep translation memories classified by organisation or customer, but seldom by writing style or purpose. Content from sales, marketing, engineering, legal and other areas are all mixed together.\\n  * Glossaries alone are absolute and result in over-fitting in the wrong context. Many MT systems using glossaries simply search and replace rather than use the glossary in context and the use of the glossary is not 100% reliable.\\n\\nWe will also explore how the advantages that LSPs and large organizations have\\nhad in the past by having “larger amounts of training data” on file are very\\nquickly becoming irrelevant in the new redefined approach to domain adaptation\\nwhere tens or', start_char_idx=17319, end_char_idx=20835, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.24037372890428002), NodeWithScore(node=TextNode(id_='f6dd6ac9-f226-4268-aa6d-41aba2f41609', embedding=None, metadata={'file_name': 'omniscien.com/about-us/news/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e9e3c801-e6c1-498b-8fab-e5fbd9aeb664', node_type=None, metadata={'file_name': 'omniscien.com/about-us/news/index.html'}, hash='6381de11daa4c805db5cf2a774d1490cfbd91d33faae3d5d55c5f59e3a7f0e85'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='71c073c9-c233-4c97-836b-b42f3ed7eed7', node_type=None, metadata={'file_name': 'omniscien.com/about-us/news/index.html'}, hash='44f0d9c8d32de0b35c2b2665c66072362009caee55aa8d99e87bd5297d233b97'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b39f3036-6615-4678-8c60-cede2fc8abb2', node_type=None, metadata={'file_name': 'omniscien.com/about-us/news/index.html'}, hash='b18be91ee388732c79484171dc4c008e2bfa82f9e069654f0f85f9faf3851b60')}, hash='d7cde0cca03917eae464a95694d9fbd3bf9ccee493edd244913a12b351772c3a', text='   * [Data Security and Privacy](https://omniscien.com/machine-translation/data-security-and-privacy/)\\n\\n__ Search\\n\\n[Omniscien](https://omniscien.com/) » [About Us](https://omniscien.com/about-\\nus/) » News\\n\\n#  News , Announcements, and Media Coverage\\n\\n## [NAB Show 2019: The 13 Innovative Media Companies to\\nWatch](https://omniscien.com/news/nab-show-2019-the-13-innovative-media-\\ncompanies-to-watch/)\\n\\nMay 3, 2019\\n\\nForbes: NAB Show: The 13 Innovative Media Companies to WatchOmniscien was\\nlisted in Forbes as one of 13 innovative media companies to watch while\\nattending NAB Show in Las Vegas Read more here:...\\n\\n[read more](https://omniscien.com/news/nab-show-2019-the-13-innovative-media-\\ncompanies-to-watch/)\\n\\n## [Omniscien Technologies Releases Next-Generation, Machine Learning Based\\nDeep Neural Machine Translation (Deep\\nNMT)](https://omniscien.com/news/omniscien-technologies-releases-next-\\ngeneration-machine-learning-based-deep-neural-machine-translation-deep-nmt/)\\n\\nOct 18, 2017\\n\\nOmniscien Technologies Releases Next-Generation, Machine Learning Based Deep\\nNeural Machine Translation (Deep NMT) SINGAPORE – October 19, 2017 – Omniscien\\nTechnologies releases Next-Generation, Machine Learning Based Deep Neural\\nMachine Translation (Deep NMT). With...\\n\\n[read more](https://omniscien.com/news/omniscien-technologies-releases-next-\\ngeneration-machine-learning-based-deep-neural-machine-translation-deep-nmt/)\\n\\n## [Omniscien Technologies Releases Next-Generation REST API and Language\\nScripting Toolkit for Language Studio\\nCloud](https://omniscien.com/news/omniscien-technologies-releases-next-\\ngeneration-rest-api-and-language-scripting-toolkit-for-language-studio-cloud/)\\n\\nJul 18, 2017\\n\\nOmniscien Technologies Releases Next-Generation REST API and Language\\nScripting Toolkit for Language Studio Cloud SINGAPORE – July 18, 2017 –\\nOmniscien Technologies (formerly Asia Online) today announces the release of\\nits next-generation REST Application Programming...\\n\\n[read more](https://omniscien.com/news/omniscien-technologies-releases-next-\\ngeneration-rest-api-and-language-scripting-toolkit-for-language-studio-cloud/)\\n\\n## [Omniscien Technologies Announces Partnership with LexisNexis to Deploy\\nLanguage Studio Neural Machine\\nTranslation](https://omniscien.com/news/omniscien-technologies-announces-\\npartnership-with-lexisnexis-to-deploy-language-studio-neural-machine-\\ntranslation/)\\n\\nApr 27, 2017\\n\\nOmniscien Technologies Announces Partnership with LexisNexis to Deploy\\nLanguage Studio Neural Machine Translation SINGAPORE – April 27, 2017 –\\nOmniscien Technologies (formerly Asia Online) today announced a partnership\\nwith LexisNexis to deploy Language Studio Neural...\\n\\n[read more](https://omniscien.com/news/omniscien-technologies-announces-\\npartnership-with-lexisnexis-to-deploy-language-studio-neural-machine-\\ntranslation/)\\n\\n## [Omniscien Technologies Announces Release of Language Studio with Next-\\nGeneration Neural Machine', start_char_idx=7886, end_char_idx=10818, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.21720125208652655)], metadata={'720a1017-8d9a-40fc-bc92-a6ddc931cac6': {'file_name': 'omniscien.com/resources/webinars.html'}, '2751878d-0947-4314-8f51-7a16720a82ee': {'file_name': 'omniscien.com/symposium/index.html'}, '3cacd607-a0ee-46cf-893f-3866b8d0dc27': {'file_name': 'omniscien.com/blog/thailand-joint-foreign-chambers-of-commerce-ai-and-machine-learning-trends-and-caveats/index.html'}, '09b0df30-4141-4580-bbe2-d91f32ba7be4': {'file_name': 'omniscien.com/resources/events-and-conferences/index.html'}, 'f6dd6ac9-f226-4268-aa6d-41aba2f41609': {'file_name': 'omniscien.com/about-us/news/index.html'}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My answer:\n",
      "KEYWORDS: Dion Wiggins, events, biography, life story, history, achievements, awards, career, personal life."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23650.94 ms\n",
      "llama_print_timings:      sample time =    24.05 ms /    36 runs   (    0.67 ms per token,  1497.13 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46701.33 ms /    86 tokens (  543.04 ms per token,     1.84 tokens per second)\n",
      "llama_print_timings:        eval time = 35147.42 ms /    35 runs   ( 1004.21 ms per token,     1.00 tokens per second)\n",
      "llama_print_timings:       total time = 82037.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23650.94 ms\n",
      "llama_print_timings:      sample time =   184.05 ms /   257 runs   (    0.72 ms per token,  1396.38 tokens per second)\n",
      "llama_print_timings: prompt eval time = 1683674.91 ms /  3838 tokens (  438.69 ms per token,     2.28 tokens per second)\n",
      "llama_print_timings:        eval time = 115811.36 ms /   256 runs   (  452.39 ms per token,     2.21 tokens per second)\n",
      "llama_print_timings:       total time = 1800574.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. What is Dion Wiggins's role at Omniscien Technologies?\n",
      "2. What are some of the technologies that use Transformer models?\n",
      "3. How does domain adaptation work in MT and ASR?\n",
      "4. What are some limitations of traditional approaches to creating training data for AI systems?\n",
      "5. How can AI be used to create many millions of sentences of training data for domain adaptation?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23650.94 ms\n",
      "llama_print_timings:      sample time =    61.06 ms /    95 runs   (    0.64 ms per token,  1555.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 440161.19 ms /  1672 tokens (  263.25 ms per token,     3.80 tokens per second)\n",
      "llama_print_timings:        eval time = 33748.30 ms /    94 runs   (  359.02 ms per token,     2.79 tokens per second)\n",
      "llama_print_timings:       total time = 474263.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "1. Dion Wiggins is a research scientist at Omniscien Technologies.\n",
      "2. Some technologies that use Transformer models include chatbots, language translation systems, and text summarization tools.\n",
      "3. In MT and ASR, domain adaptation involves adapting machine learning models to new domains or languages by using data from the target domain or language to fine-tune the model.\n",
      "4. Traditional approaches to creating training data for AI systems can be limited by the quality and quantity of available data, as well as the cost and time required to collect and label it.\n",
      "5. AI can be used to create many millions of sentences of training data for domain adaptation by using natural language processing techniques to generate synthetic data that mimics real-world language use. This can help to augment existing training data and improve the performance of machine learning models in new domains or languages."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23650.94 ms\n",
      "llama_print_timings:      sample time =   114.61 ms /   194 runs   (    0.59 ms per token,  1692.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 31036.96 ms /   120 tokens (  258.64 ms per token,     3.87 tokens per second)\n",
      "llama_print_timings:        eval time = 59415.47 ms /   193 runs   (  307.85 ms per token,     3.25 tokens per second)\n",
      "llama_print_timings:       total time = 91127.95 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Answer: \n",
       "1. Dion Wiggins is a research scientist at Omniscien Technologies.\n",
       "2. Some technologies that use Transformer models include chatbots, language translation systems, and text summarization tools.\n",
       "3. In MT and ASR, domain adaptation involves adapting machine learning models to new domains or languages by using data from the target domain or language to fine-tune the model.\n",
       "4. Traditional approaches to creating training data for AI systems can be limited by the quality and quantity of available data, as well as the cost and time required to collect and label it.\n",
       "5. AI can be used to create many millions of sentences of training data for domain adaptation by using natural language processing techniques to generate synthetic data that mimics real-world language use. This can help to augment existing training data and improve the performance of machine learning models in new domains or languages.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = custom_query_engine.query(\"Tell me events about Dion Wiggins\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My answer:\n",
      "KEYWORDS: Dion Wiggins, bio, information, personal details, career, achievements, awards, interests, hobbies, social media."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23491.94 ms\n",
      "llama_print_timings:      sample time =    23.18 ms /    39 runs   (    0.59 ms per token,  1682.78 tokens per second)\n",
      "llama_print_timings: prompt eval time = 23491.88 ms /    87 tokens (  270.02 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:        eval time = 11838.98 ms /    38 runs   (  311.55 ms per token,     3.21 tokens per second)\n",
      "llama_print_timings:       total time = 35496.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Dion Wiggins is a researcher at Omniscien Technologies. His work focuses on developing and applying machine learning techniques to various applications, including natural language processing, computer vision, and recommender systems. He has published several papers in top-tier conferences and journals and has received awards for his contributions to the field.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 23491.94 ms\n",
      "llama_print_timings:      sample time =    51.09 ms /    80 runs   (    0.64 ms per token,  1565.83 tokens per second)\n",
      "llama_print_timings: prompt eval time = 476897.76 ms /  1645 tokens (  289.91 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:        eval time = 30066.48 ms /    79 runs   (  380.59 ms per token,     2.63 tokens per second)\n",
      "llama_print_timings:       total time = 507347.48 ms\n"
     ]
    }
   ],
   "source": [
    "query_engine = index_load.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"Tell me more about Dion Wiggins\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>---------------------\n",
       "Dion Wiggins is a researcher at Omniscien Technologies. His work focuses on developing and applying machine learning techniques to various applications, including natural language processing, computer vision, and recommender systems. He has published several papers in top-tier conferences and journals and has received awards for his contributions to the field.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='---------------------\\nDion Wiggins is a researcher at Omniscien Technologies. His work focuses on developing and applying machine learning techniques to various applications, including natural language processing, computer vision, and recommender systems. He has published several papers in top-tier conferences and journals and has received awards for his contributions to the field.\\n\\n\\n\\n\\n', source_nodes=[NodeWithScore(node=TextNode(id_='7e5d38ce-de8a-44fc-9986-484850f1a22b', embedding=None, metadata={'file_name': 'omniscien.com/faq/what-is-real-time-captioning/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ba307834-171e-47fd-b2a5-d2c90f8148f6', node_type=None, metadata={'file_name': 'omniscien.com/faq/what-is-real-time-captioning/index.html'}, hash='6891b8ae60fb547d4d1fae9169fc0206f91488d417ec2da3a0b39f52b92b107d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='df0763ea-2e06-42be-9110-a0fe722f03c8', node_type=None, metadata={'file_name': 'omniscien.com/faq/what-is-real-time-captioning/index.html'}, hash='3cf7f12c389edd96f9a0c20924e43d26255bece4d581217d549ae4b9e1147689')}, hash='b0d336b2d9abfc704adde57b88ba8d64430d0aefadbdde635c93f04c04935ecf', text=' * [Office Locations](/about-us/office-locations/)\\n  * [Legal](/about-us/legal/)\\n\\nConnect\\n\\n  * [Follow](https://www.facebook.com/omniscien \"Follow on Facebook\")\\n  * [Follow](https://twitter.com/omniscientech \"Follow on Twitter\")\\n  * [Follow](https://www.linkedin.com/company/omniscien \"Follow on LinkedIn\")\\n\\n  * [Contact Us](/about-us/contact-us/)\\n  * [Request a Demo](/resources/request-a-demo/)\\n  * [Subscribe to Mailing List](/resources/subscribe-to-omniscien-mailing-list/)\\n\\nCopyright © 2023 Omniscien Technologies. All Rights Reserved.\\n[Legal](/about-us/legal/)\\n\\nFREE WEBINAR: The Future of Language Related AI for Enterprises: Local Agents\\nand Fine-Tuned Large Language Models (LLMs)[Watch the\\nReplay](https://omniscien.com/resources/webinars)\\n\\n+ __\\n\\n[ -> Index ](javascript:void\\\\(0\\\\))', start_char_idx=17581, end_char_idx=18372, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='4f03afac-7e69-444d-95ec-d6ee609acb97', embedding=None, metadata={'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='082434e6-1b30-4036-811c-8706e39d399b', node_type=None, metadata={'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, hash='6bd68f806e2d360039ecc69e1475cbbaf825788834c188599038918dd92b8399'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e8b2ff61-ea79-42a4-87e4-0c80adc39458', node_type=None, metadata={'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, hash='113e955c02692a9237bb624e554dda35ec453b55b616da536440bf560b8a6aa9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5462f00b-72cd-4911-ae6d-9632b82043ef', node_type=None, metadata={'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, hash='9399e75a27a45569673d63e52ca01580f06e77d54a5ad1fa4c24c28cc30abb9e')}, hash='6006c2b876a18eb086e2ace014a87a5bd73576a96c000c53b16728bb282954c4', text='or classifications.\\n\\nOne common use case for End-to-End models is in speech recognition, where the\\nmodel is trained directly on audio recordings of speech and produces the\\ncorresponding text transcription. This eliminates the need for intermediate\\nsteps such as phoneme recognition and language modeling, which were previously\\nrequired in traditional speech recognition systems.\\n\\nWhile End-to-End models offer many advantages, they also have some\\nlimitations. For example, they may require a large amount of training data and\\ncomputing resources to train effectively. Additionally, they may not be as\\ninterpretable as traditional machine learning models, making it difficult to\\nunderstand how they are making predictions.\\n\\n### End-To-End Speech Recognition\\n\\nEnd-to-End Speech Recognition refers to a technique in automatic speech\\nrecognition (ASR) that allows the entire speech recognition process to be done\\nin one step, directly mapping the acoustic waveform to the recognized text\\nwithout relying on intermediate representations, such as phonemes or words.\\nThis approach differs from traditional ASR systems, which involve multiple\\nstages of processing, including feature extraction, acoustic modeling,\\npronunciation modeling, and language modeling.\\n\\nIn end-to-end speech recognition, a neural network is trained to directly map\\nthe acoustic waveform to the corresponding text, without relying on any pre-\\ndefined linguistic knowledge. This approach has the potential to simplify the\\nspeech recognition pipeline, improve recognition accuracy, and reduce the need\\nfor hand-crafted features and complex modeling.\\n\\nEnd-to-end speech recognition has been successfully applied in a number of\\napplications, including virtual assistants, speech-to-text transcription, and\\ndictation systems. It is particularly useful in scenarios where the amount of\\nlabeled training data is limited, as it allows the use of large amounts of\\nunlabeled data to improve the performance of the system. However, end-to-end\\nspeech recognition is still an active area of research, and there are ongoing\\nefforts to improve its performance, scalability, and robustness.\\n\\n### Entity\\n\\nIn natural language processing, an entity refers to a specific piece of\\ninformation within a user\\'s utterance that can be extracted and used to inform\\nthe system\\'s response. Entities are often associated with a particular intent,\\nwhich is the overall goal or objective of the user\\'s statement.\\n\\nFor example, in the statement \"Book me a flight to Boston,\" the intent is to\\nbook a flight, while the entities are \"flight\" and \"Boston.\" The entity\\n\"flight\" refers to the mode of transportation that the user wants to book,\\nwhile the entity \"Boston\" refers to the destination of the flight.\\n\\nEntities are important for enabling more advanced natural language processing\\ncapabilities, such as slot filling and entity recognition. By identifying and\\nextracting entities from user input, systems can better understand the user\\'s\\nintent and provide more accurate and relevant responses.\\n\\nOverall, entities play a critical role in natural language processing,\\nenabling more sophisticated and effective interactions between users and\\nmachines. As the technology continues to evolve, we can expect to see even\\nmore advanced entity recognition and extraction capabilities that enable even\\nmore natural and intuitive conversations.\\n\\n### Equal Error Rate (ERR)\\n\\nEqual Error Rate (ERR) is a performance metric used to evaluate the accuracy\\nof a biometric system, such as voice recognition or facial recognition. The\\nEqual Error Rate measures the point where the False Acceptance Rate (FAR) and\\nFalse Rejection Rate (FRR) are equal. In other words, ERR is the rate at which\\nthe system incorrectly identifies a non-matching biometric sample as a\\nmatching sample, and also incorrectly identifies a matching sample as a non-\\nmatching sample.\\n\\nA system with a low ERR indicates that it has a good balance between the false\\nrejection', start_char_idx=194429, end_char_idx=198394, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='ec175b13-e16b-4f9e-bdb0-e0a50fbd4e59', embedding=None, metadata={'file_name': 'omniscien.com/machine-translation/ways-to-translate/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='54be8363-9157-4748-b3c5-1c0081169faa', node_type=None, metadata={'file_name': 'omniscien.com/machine-translation/ways-to-translate/index.html'}, hash='1e94ee8cb479f025294e0f811e2ce975144f1291523058bcfba52df28615dee3'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='34cf8e38-364f-43ec-8ea9-cfd49401fcfa', node_type=None, metadata={'file_name': 'omniscien.com/machine-translation/ways-to-translate/index.html'}, hash='7fe51bb82aaaa9ec395bcfe5bc6432bb798258916b6a220a87e1bc97931ce9d3')}, hash='339f17629b38937914069fa6eb65a9f8f9c6fac8350f4bd8b5d49672a8ab8170', text='Locations](../../about-us/office-locations/index.html)\\n  * [Legal](../../about-us/legal/index.html)\\n\\nConnect\\n\\n  * [Follow](https://www.facebook.com/omniscien \"Follow on Facebook\")\\n  * [Follow](https://twitter.com/omniscientech \"Follow on Twitter\")\\n  * [Follow](https://www.linkedin.com/company/omniscien \"Follow on LinkedIn\")\\n\\n  * [Contact Us](../../about-us/contact-us/index.html)\\n  * [Request a Demo](../../resources/request-a-demo/index.html)\\n  * [Subscribe to Mailing List](../../resources/subscribe-to-omniscien-mailing-list/index.html)\\n\\nCopyright © 2023 Omniscien Technologies. All Rights Reserved.\\n[Legal](../../about-us/legal/index.html)\\n\\nFREE WEBINAR: The Future of Language Related AI for Enterprises: Local Agents\\nand Fine-Tuned Large Language Models (LLMs)[Watch the\\nReplay](../../resources/webinars.html)\\n\\n+ __', start_char_idx=17646, end_char_idx=18469, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='554cae49-310a-4c3b-90d2-e6dc7e9b929f', embedding=None, metadata={'kg_rel_texts': [], 'kg_rel_map': {'My': [], 'My answer:\\nKEYWORDS: Dion Wiggins': [], 'awards': [], 'details': [], 'social': [], 'answer': [], 'hobbies': [], 'career': [], 'achievements': [], 'social media.': [], 'KEYWORDS': [], 'information': [], 'personal details': [], 'bio': [], 'Dion': [], 'interests': [], 'media': [], 'Wiggins': [], 'personal': []}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_texts'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_texts'], relationships={}, hash='b4cf2f3c44c829309286f79681a4fbe07317171f8dd49330061b4df01f0e51a9', text='The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0)], metadata={'7e5d38ce-de8a-44fc-9986-484850f1a22b': {'file_name': 'omniscien.com/faq/what-is-real-time-captioning/index.html'}, '4f03afac-7e69-444d-95ec-d6ee609acb97': {'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, 'ec175b13-e16b-4f9e-bdb0-e0a50fbd4e59': {'file_name': 'omniscien.com/machine-translation/ways-to-translate/index.html'}, '554cae49-310a-4c3b-90d2-e6dc7e9b929f': {'kg_rel_texts': [], 'kg_rel_map': {'My': [], 'My answer:\\nKEYWORDS: Dion Wiggins': [], 'awards': [], 'details': [], 'social': [], 'answer': [], 'hobbies': [], 'career': [], 'achievements': [], 'social media.': [], 'KEYWORDS': [], 'information': [], 'personal details': [], 'bio': [], 'Dion': [], 'interests': [], 'media': [], 'Wiggins': [], 'personal': []}}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(source_list):\n",
    "    res = []\n",
    "    for i in source_list:\n",
    "        if i not in res:\n",
    "            res.append(i)\n",
    "    return res\n",
    "\n",
    "def convert_to_website_format(urls):\n",
    "    convert_urls = []\n",
    "    for url in urls:\n",
    "        # Remove any '.html' at the end of the URL\n",
    "        url = re.sub(r'\\.html$', '', url)\n",
    "        # Check if the URL starts with 'www.' or 'http://'\n",
    "        if not re.match(r'(www\\.|http://)', url):\n",
    "            url = 'www.' + url\n",
    "        if '/index' in url:\n",
    "            url = url.split('/index')[0] \n",
    "        convert_urls.append(url)\n",
    "    return convert_urls\n",
    "\n",
    "def regex_source(answer):\n",
    "    pattern = r\"'file_name': '(.*?)'\"\n",
    "    matchs = re.findall(pattern, str(answer))\n",
    "    convert_urls = convert_to_website_format(matchs)\n",
    "    res_urls = check_duplicate(source_list=convert_urls)\n",
    "    return res_urls\n",
    "\n",
    "urls = regex_source(response.get_formatted_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.omniscien.com/faq/what-is-real-time-captioning',\n",
       " 'www.omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g',\n",
       " 'www.omniscien.com/machine-translation/ways-to-translate']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Response.get_formatted_sources of Response(response='---------------------\\nDion Wiggins is a researcher at Omniscien Technologies. His work focuses on developing and applying machine learning techniques to various applications, including natural language processing, computer vision, and recommender systems. He has published several papers in top-tier conferences and journals and has received awards for his contributions to the field.\\n\\n\\n\\n\\n', source_nodes=[NodeWithScore(node=TextNode(id_='7e5d38ce-de8a-44fc-9986-484850f1a22b', embedding=None, metadata={'file_name': 'omniscien.com/faq/what-is-real-time-captioning/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ba307834-171e-47fd-b2a5-d2c90f8148f6', node_type=None, metadata={'file_name': 'omniscien.com/faq/what-is-real-time-captioning/index.html'}, hash='6891b8ae60fb547d4d1fae9169fc0206f91488d417ec2da3a0b39f52b92b107d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='df0763ea-2e06-42be-9110-a0fe722f03c8', node_type=None, metadata={'file_name': 'omniscien.com/faq/what-is-real-time-captioning/index.html'}, hash='3cf7f12c389edd96f9a0c20924e43d26255bece4d581217d549ae4b9e1147689')}, hash='b0d336b2d9abfc704adde57b88ba8d64430d0aefadbdde635c93f04c04935ecf', text=' * [Office Locations](/about-us/office-locations/)\\n  * [Legal](/about-us/legal/)\\n\\nConnect\\n\\n  * [Follow](https://www.facebook.com/omniscien \"Follow on Facebook\")\\n  * [Follow](https://twitter.com/omniscientech \"Follow on Twitter\")\\n  * [Follow](https://www.linkedin.com/company/omniscien \"Follow on LinkedIn\")\\n\\n  * [Contact Us](/about-us/contact-us/)\\n  * [Request a Demo](/resources/request-a-demo/)\\n  * [Subscribe to Mailing List](/resources/subscribe-to-omniscien-mailing-list/)\\n\\nCopyright © 2023 Omniscien Technologies. All Rights Reserved.\\n[Legal](/about-us/legal/)\\n\\nFREE WEBINAR: The Future of Language Related AI for Enterprises: Local Agents\\nand Fine-Tuned Large Language Models (LLMs)[Watch the\\nReplay](https://omniscien.com/resources/webinars)\\n\\n+ __\\n\\n[ -> Index ](javascript:void\\\\(0\\\\))', start_char_idx=17581, end_char_idx=18372, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='4f03afac-7e69-444d-95ec-d6ee609acb97', embedding=None, metadata={'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='082434e6-1b30-4036-811c-8706e39d399b', node_type=None, metadata={'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, hash='6bd68f806e2d360039ecc69e1475cbbaf825788834c188599038918dd92b8399'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e8b2ff61-ea79-42a4-87e4-0c80adc39458', node_type=None, metadata={'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, hash='113e955c02692a9237bb624e554dda35ec453b55b616da536440bf560b8a6aa9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5462f00b-72cd-4911-ae6d-9632b82043ef', node_type=None, metadata={'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, hash='9399e75a27a45569673d63e52ca01580f06e77d54a5ad1fa4c24c28cc30abb9e')}, hash='6006c2b876a18eb086e2ace014a87a5bd73576a96c000c53b16728bb282954c4', text='or classifications.\\n\\nOne common use case for End-to-End models is in speech recognition, where the\\nmodel is trained directly on audio recordings of speech and produces the\\ncorresponding text transcription. This eliminates the need for intermediate\\nsteps such as phoneme recognition and language modeling, which were previously\\nrequired in traditional speech recognition systems.\\n\\nWhile End-to-End models offer many advantages, they also have some\\nlimitations. For example, they may require a large amount of training data and\\ncomputing resources to train effectively. Additionally, they may not be as\\ninterpretable as traditional machine learning models, making it difficult to\\nunderstand how they are making predictions.\\n\\n### End-To-End Speech Recognition\\n\\nEnd-to-End Speech Recognition refers to a technique in automatic speech\\nrecognition (ASR) that allows the entire speech recognition process to be done\\nin one step, directly mapping the acoustic waveform to the recognized text\\nwithout relying on intermediate representations, such as phonemes or words.\\nThis approach differs from traditional ASR systems, which involve multiple\\nstages of processing, including feature extraction, acoustic modeling,\\npronunciation modeling, and language modeling.\\n\\nIn end-to-end speech recognition, a neural network is trained to directly map\\nthe acoustic waveform to the corresponding text, without relying on any pre-\\ndefined linguistic knowledge. This approach has the potential to simplify the\\nspeech recognition pipeline, improve recognition accuracy, and reduce the need\\nfor hand-crafted features and complex modeling.\\n\\nEnd-to-end speech recognition has been successfully applied in a number of\\napplications, including virtual assistants, speech-to-text transcription, and\\ndictation systems. It is particularly useful in scenarios where the amount of\\nlabeled training data is limited, as it allows the use of large amounts of\\nunlabeled data to improve the performance of the system. However, end-to-end\\nspeech recognition is still an active area of research, and there are ongoing\\nefforts to improve its performance, scalability, and robustness.\\n\\n### Entity\\n\\nIn natural language processing, an entity refers to a specific piece of\\ninformation within a user\\'s utterance that can be extracted and used to inform\\nthe system\\'s response. Entities are often associated with a particular intent,\\nwhich is the overall goal or objective of the user\\'s statement.\\n\\nFor example, in the statement \"Book me a flight to Boston,\" the intent is to\\nbook a flight, while the entities are \"flight\" and \"Boston.\" The entity\\n\"flight\" refers to the mode of transportation that the user wants to book,\\nwhile the entity \"Boston\" refers to the destination of the flight.\\n\\nEntities are important for enabling more advanced natural language processing\\ncapabilities, such as slot filling and entity recognition. By identifying and\\nextracting entities from user input, systems can better understand the user\\'s\\nintent and provide more accurate and relevant responses.\\n\\nOverall, entities play a critical role in natural language processing,\\nenabling more sophisticated and effective interactions between users and\\nmachines. As the technology continues to evolve, we can expect to see even\\nmore advanced entity recognition and extraction capabilities that enable even\\nmore natural and intuitive conversations.\\n\\n### Equal Error Rate (ERR)\\n\\nEqual Error Rate (ERR) is a performance metric used to evaluate the accuracy\\nof a biometric system, such as voice recognition or facial recognition. The\\nEqual Error Rate measures the point where the False Acceptance Rate (FAR) and\\nFalse Rejection Rate (FRR) are equal. In other words, ERR is the rate at which\\nthe system incorrectly identifies a non-matching biometric sample as a\\nmatching sample, and also incorrectly identifies a matching sample as a non-\\nmatching sample.\\n\\nA system with a low ERR indicates that it has a good balance between the false\\nrejection', start_char_idx=194429, end_char_idx=198394, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='ec175b13-e16b-4f9e-bdb0-e0a50fbd4e59', embedding=None, metadata={'file_name': 'omniscien.com/machine-translation/ways-to-translate/index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='54be8363-9157-4748-b3c5-1c0081169faa', node_type=None, metadata={'file_name': 'omniscien.com/machine-translation/ways-to-translate/index.html'}, hash='1e94ee8cb479f025294e0f811e2ce975144f1291523058bcfba52df28615dee3'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='34cf8e38-364f-43ec-8ea9-cfd49401fcfa', node_type=None, metadata={'file_name': 'omniscien.com/machine-translation/ways-to-translate/index.html'}, hash='7fe51bb82aaaa9ec395bcfe5bc6432bb798258916b6a220a87e1bc97931ce9d3')}, hash='339f17629b38937914069fa6eb65a9f8f9c6fac8350f4bd8b5d49672a8ab8170', text='Locations](../../about-us/office-locations/index.html)\\n  * [Legal](../../about-us/legal/index.html)\\n\\nConnect\\n\\n  * [Follow](https://www.facebook.com/omniscien \"Follow on Facebook\")\\n  * [Follow](https://twitter.com/omniscientech \"Follow on Twitter\")\\n  * [Follow](https://www.linkedin.com/company/omniscien \"Follow on LinkedIn\")\\n\\n  * [Contact Us](../../about-us/contact-us/index.html)\\n  * [Request a Demo](../../resources/request-a-demo/index.html)\\n  * [Subscribe to Mailing List](../../resources/subscribe-to-omniscien-mailing-list/index.html)\\n\\nCopyright © 2023 Omniscien Technologies. All Rights Reserved.\\n[Legal](../../about-us/legal/index.html)\\n\\nFREE WEBINAR: The Future of Language Related AI for Enterprises: Local Agents\\nand Fine-Tuned Large Language Models (LLMs)[Watch the\\nReplay](../../resources/webinars.html)\\n\\n+ __', start_char_idx=17646, end_char_idx=18469, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0), NodeWithScore(node=TextNode(id_='554cae49-310a-4c3b-90d2-e6dc7e9b929f', embedding=None, metadata={'kg_rel_texts': [], 'kg_rel_map': {'My': [], 'My answer:\\nKEYWORDS: Dion Wiggins': [], 'awards': [], 'details': [], 'social': [], 'answer': [], 'hobbies': [], 'career': [], 'achievements': [], 'social media.': [], 'KEYWORDS': [], 'information': [], 'personal details': [], 'bio': [], 'Dion': [], 'interests': [], 'media': [], 'Wiggins': [], 'personal': []}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_texts'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_texts'], relationships={}, hash='b4cf2f3c44c829309286f79681a4fbe07317171f8dd49330061b4df01f0e51a9', text='The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1000.0)], metadata={'7e5d38ce-de8a-44fc-9986-484850f1a22b': {'file_name': 'omniscien.com/faq/what-is-real-time-captioning/index.html'}, '4f03afac-7e69-444d-95ec-d6ee609acb97': {'file_name': 'omniscien.com/blog/speech-recognition-speech-synthesis-glossary-a-g/index.html'}, 'ec175b13-e16b-4f9e-bdb0-e0a50fbd4e59': {'file_name': 'omniscien.com/machine-translation/ways-to-translate/index.html'}, '554cae49-310a-4c3b-90d2-e6dc7e9b929f': {'kg_rel_texts': [], 'kg_rel_map': {'My': [], 'My answer:\\nKEYWORDS: Dion Wiggins': [], 'awards': [], 'details': [], 'social': [], 'answer': [], 'hobbies': [], 'career': [], 'achievements': [], 'social media.': [], 'KEYWORDS': [], 'information': [], 'personal details': [], 'bio': [], 'Dion': [], 'interests': [], 'media': [], 'Wiggins': [], 'personal': []}}})>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-lama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
